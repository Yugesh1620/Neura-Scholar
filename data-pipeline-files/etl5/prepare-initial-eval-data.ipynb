{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70804ccc-fe73-443f-9ad1-796e567b6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import shutil\n",
    "import re\n",
    "import unicodedata\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.dialects.postgresql import ARRAY, TEXT\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import psycopg2\n",
    "from sqlalchemy import inspect\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1327621-079a-4d9b-aabe-d9fa0c3ff1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\n",
    "    'postgresql+psycopg2://rg5073:rg5073pass@129.114.27.112:5432/cleaned_meta_data_db',\n",
    "    pool_size=12,\n",
    "    max_overflow=0,\n",
    "    pool_timeout=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd603dd-e14b-4bc7-b14b-0bee6f427286",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS arxiv_metadata_training_data (\n",
    "    paper_id TEXT,\n",
    "    chunk_no INT,\n",
    "    chunk_id TEXT,\n",
    "    txt_filename TEXT,\n",
    "    query TEXT,\n",
    "    chunk_data TEXT,\n",
    "    query_phrases TEXT\n",
    ");\n",
    "\"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(create_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2083cd4d-aba9-49d8-a8fc-3764772517a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_csv_path = os.path.join('/home/jovyan/work', 'arxiv_chunks_training_4_phrases1.csv')\n",
    "all_data_csv_path = os.path.join('/home/jovyan/work', 'arxiv_cleaned_v5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e18bea89-bbaf-4b8b-a9af-65ca8127b245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training data contains:  6043  records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>chunk_no</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>txt_filename</th>\n",
       "      <th>query</th>\n",
       "      <th>chunk_data</th>\n",
       "      <th>query_phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0107v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0704.0107v1_5</td>\n",
       "      <td>0704.0107v1.txt</td>\n",
       "      <td>[\"What is the Parzen's estimator formula used ...</td>\n",
       "      <td>lim N 1 N N X x qi, , which we consider as the...</td>\n",
       "      <td>[\"parzen estimator formula\", \"redundancy numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0107v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0704.0107v1_6</td>\n",
       "      <td>0704.0107v1.txt</td>\n",
       "      <td>[\"How to adapt the model Eq. 19 to experimenta...</td>\n",
       "      <td>model relative to experimentally estimated fT ...</td>\n",
       "      <td>[\"adapt model eq\", \"growing pruning methods\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0076v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0704.0076v2_12</td>\n",
       "      <td>0704.0076v2.txt</td>\n",
       "      <td>[\"CP asymmetry\", \"Amplitude C and T\", \"SU rela...</td>\n",
       "      <td>the CP asymmetry sum rule predicts ACP B0 K0 0...</td>\n",
       "      <td>[\"cp asymmetry\", \"amplitude\", \"su relation\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0107v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0704.0107v1_7</td>\n",
       "      <td>0704.0107v1.txt</td>\n",
       "      <td>[\"What is the effect of increasing T on the mo...</td>\n",
       "      <td>a new term with the parameters xT , , x g x xT...</td>\n",
       "      <td>[\"pdf new experimental\", \"annihilation process...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0674v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0704.0674v2_1</td>\n",
       "      <td>0704.0674v2.txt</td>\n",
       "      <td>[\"Galaxy alignment types\", \"Galaxy group catal...</td>\n",
       "      <td>arXiv 0704.0674v2 astro ph 8 Jun 2007 Draft ve...</td>\n",
       "      <td>[\"galaxy alignment types\", \"galaxy group catal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      paper_id  chunk_no        chunk_id     txt_filename  \\\n",
       "0  0704.0107v1       NaN   0704.0107v1_5  0704.0107v1.txt   \n",
       "1  0704.0107v1       NaN   0704.0107v1_6  0704.0107v1.txt   \n",
       "2  0704.0076v2       NaN  0704.0076v2_12  0704.0076v2.txt   \n",
       "3  0704.0107v1       NaN   0704.0107v1_7  0704.0107v1.txt   \n",
       "4  0704.0674v2       NaN   0704.0674v2_1  0704.0674v2.txt   \n",
       "\n",
       "                                               query  \\\n",
       "0  [\"What is the Parzen's estimator formula used ...   \n",
       "1  [\"How to adapt the model Eq. 19 to experimenta...   \n",
       "2  [\"CP asymmetry\", \"Amplitude C and T\", \"SU rela...   \n",
       "3  [\"What is the effect of increasing T on the mo...   \n",
       "4  [\"Galaxy alignment types\", \"Galaxy group catal...   \n",
       "\n",
       "                                          chunk_data  \\\n",
       "0  lim N 1 N N X x qi, , which we consider as the...   \n",
       "1  model relative to experimentally estimated fT ...   \n",
       "2  the CP asymmetry sum rule predicts ACP B0 K0 0...   \n",
       "3  a new term with the parameters xT , , x g x xT...   \n",
       "4  arXiv 0704.0674v2 astro ph 8 Jun 2007 Draft ve...   \n",
       "\n",
       "                                       query_phrases  \n",
       "0  [\"parzen estimator formula\", \"redundancy numbe...  \n",
       "1  [\"adapt model eq\", \"growing pruning methods\", ...  \n",
       "2       [\"cp asymmetry\", \"amplitude\", \"su relation\"]  \n",
       "3  [\"pdf new experimental\", \"annihilation process...  \n",
       "4  [\"galaxy alignment types\", \"galaxy group catal...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data = pd.read_csv(training_data_csv_path)\n",
    "print(\"Our training data contains: \",len(df_training_data), \" records\")\n",
    "df_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39eca8f7-f3d5-45ff-98d1-c7607aaf195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_conn = engine.raw_connection()\n",
    "# cur = raw_conn.cursor()\n",
    "\n",
    "# with open(training_data_csv_path, 'r') as f:\n",
    "#     cur.copy_expert(f\"\"\"\n",
    "#         COPY arxiv_metadata_training_data\n",
    "#         FROM STDIN\n",
    "#         WITH CSV HEADER\n",
    "#     \"\"\", f)\n",
    "\n",
    "# raw_conn.commit()\n",
    "# cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e5e7d53-271d-405d-a32f-6e9f95cfc175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our all contains:  281931  records\n"
     ]
    }
   ],
   "source": [
    "df_all_data = pd.read_csv(all_data_csv_path)\n",
    "print(\"Our all contains: \",len(df_all_data), \" records\")\n",
    "# df_all_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b1c15d-21d5-4f31-a8f1-387c2a99d387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281931 unique paper IDs:\n",
      "['0704.0001v2', '0704.0002v2', '0704.0003v3', '0704.0004v1', '0704.0005v1']\n"
     ]
    }
   ],
   "source": [
    "# Drop NaNs and get unique filenames\n",
    "unique_filenames = df_all_data[\"txt_filename\"].dropna().unique()\n",
    "\n",
    "# Remove the '.txt' extension and convert to a Python list\n",
    "unique_paper_ids_all = [os.path.splitext(pid)[0] for pid in unique_filenames]\n",
    "\n",
    "print(f\"{len(unique_paper_ids_all)} unique paper IDs:\")\n",
    "print(unique_paper_ids_all[:5])  # Show first 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8e67ce-2d7c-46a4-ae61-43cce0a071ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812 unique paper IDs:\n",
      "['0704.0107v1', '0704.0076v2', '0704.0674v2', '0704.0360v1', '0704.0717v2']\n"
     ]
    }
   ],
   "source": [
    "# Drop NaNs and get unique filenames\n",
    "unique_filenames_training = df_training_data[\"txt_filename\"].dropna().unique()\n",
    "\n",
    "# Remove the '.txt' extension and convert to a Python list\n",
    "unique_paper_ids_training = [os.path.splitext(pid)[0] for pid in unique_filenames_training]\n",
    "\n",
    "print(f\"{len(unique_paper_ids_training)} unique paper IDs:\")\n",
    "print(unique_paper_ids_training[:5])  # Show first 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2053d47a-0356-4161-9a87-6a690bdef3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812 unique paper IDs in training set:\n",
      "['0704.0002v2', '0704.0357v3', '0704.0726v2', '0704.0784v2', '0704.0018v2']\n",
      "281931 unique paper IDs in all set:\n",
      "['0808.2912v2', '1108.2604v2', '0712.1697v1', '0807.2824v1', '0809.5023v1']\n"
     ]
    }
   ],
   "source": [
    "unique_paper_ids_training_set = set(unique_paper_ids_training)\n",
    "unique_paper_ids_all_set = set(unique_paper_ids_all)\n",
    "\n",
    "# Print counts and preview\n",
    "print(f\"{len(unique_paper_ids_training_set)} unique paper IDs in training set:\")\n",
    "print(list(unique_paper_ids_training_set)[:5])\n",
    "\n",
    "print(f\"{len(unique_paper_ids_all_set)} unique paper IDs in all set:\")\n",
    "print(list(unique_paper_ids_all_set)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102e1aa6-42d3-4099-a471-202d86a3f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/jovyan/work/data/meta-data/internal-citations.json\") as f:\n",
    "    citation_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abfb64da-f016-4a44-89a1-dafc1998068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_version(paper_id):\n",
    "    return paper_id.split('v')[0]\n",
    "\n",
    "# --- Step 1: Create version-less sets and mapping ---\n",
    "training_ids_base = set()\n",
    "all_ids_base = set()\n",
    "base_to_full = defaultdict(list)\n",
    "\n",
    "# Build sets and mapping\n",
    "for pid in unique_paper_ids_training:\n",
    "    base = strip_version(pid)\n",
    "    training_ids_base.add(base)\n",
    "    base_to_full[base].append(pid)\n",
    "\n",
    "for pid in unique_paper_ids_all:\n",
    "    base = strip_version(pid)\n",
    "    all_ids_base.add(base)\n",
    "    base_to_full[base].append(pid)\n",
    "\n",
    "# --- Step 2: Filter citations using base IDs ---\n",
    "intermediate_filtered = {\n",
    "    strip_version(paper): [\n",
    "        strip_version(cited) for cited in cites\n",
    "        if strip_version(cited) in all_ids_base and strip_version(cited) not in training_ids_base\n",
    "    ]\n",
    "    for paper, cites in citation_dict.items()\n",
    "    if strip_version(paper) in all_ids_base and strip_version(paper) not in training_ids_base\n",
    "}\n",
    "\n",
    "# Drop papers with empty citation lists\n",
    "intermediate_filtered = {p: c for p, c in intermediate_filtered.items() if c}\n",
    "\n",
    "# --- Step 3: Map base IDs back to full versions (choose latest version) ---\n",
    "def get_latest_version(paper_list):\n",
    "    return sorted(paper_list, key=lambda x: int(x.split('v')[1]), reverse=True)[0]\n",
    "\n",
    "filtered = {}\n",
    "for base_paper, base_cites in intermediate_filtered.items():\n",
    "    full_paper = get_latest_version(base_to_full[base_paper])\n",
    "    full_cites = [get_latest_version(base_to_full[c]) for c in base_cites]\n",
    "    filtered[full_paper] = full_cites\n",
    "\n",
    "# --- Step 4: Convert both mappings to DataFrames ---\n",
    "df_base = pd.DataFrame(\n",
    "    [(base_paper, cited) for base_paper, cites in intermediate_filtered.items() for cited in cites],\n",
    "    columns=[\"base_paper_id\", \"cited_base_paper_id\"]\n",
    ")\n",
    "\n",
    "df_final = pd.DataFrame(\n",
    "    [(full_paper, cited) for full_paper, cites in filtered.items() for cited in cites],\n",
    "    columns=[\"full_paper_id\", \"cited_full_paper_id\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21eecbf4-e4e1-4fa6-950d-e8640f047e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered citation pairs: 261150\n",
      "\n",
      "0911.4482: ['0907.4750', '0911.4482']\n",
      "0911.4991: ['0805.1882', '0911.4991', '0911.4877', '0911.4845', '0911.4881']\n",
      "0911.5072: ['0910.3220', '0911.5072', '0812.4140', '0803.0982', '0910.0326', '0808.2243']\n",
      "0911.3671: ['0911.3671']\n",
      "0911.1402: ['0905.4267', '0911.1401', '0911.1402']\n",
      "0911.3198: ['0911.3198']\n",
      "0911.4124: ['0911.4124', '0905.1823', '0812.3471']\n",
      "0911.1560: ['0906.0302', '0812.1202', '0812.1368', '0809.1229', '0903.3598', '0901.2599', '0812.4265', '0710.5136', '0905.3947', '0801.1817', '0706.1726', '0804.0473', '0911.1560', '0906.4728']\n",
      "0911.2332: ['0911.2332']\n",
      "0911.1134: ['0903.1115', '0710.2486', '1001.3884', '0709.0980', '0911.1134', '0711.2741', '0907.1922', '0801.4554', '0908.0857', '0705.4387']\n",
      "0911.3141: ['0911.3141', '0807.0265']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list and print\n",
    "filtered_list_imm = list(intermediate_filtered.items())\n",
    "print(f\"\\nFiltered citation pairs: {len(intermediate_filtered)}\\n\")\n",
    "\n",
    "temp_count = 0\n",
    "for paper, cites in filtered_list_imm:\n",
    "    print(f\"{paper}: {cites}\")\n",
    "    temp_count += 1\n",
    "    if temp_count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fd9d8fa-7668-4463-ac38-92264f808e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered citation pairs: 261150\n",
      "\n",
      "0911.4482v2: ['0907.4750v1', '0911.4482v2']\n",
      "0911.4991v3: ['0805.1882v1', '0911.4991v3', '0911.4877v2', '0911.4845v2', '0911.4881v3']\n",
      "0911.5072v4: ['0910.3220v2', '0911.5072v4', '0812.4140v3', '0803.0982v1', '0910.0326v1', '0808.2243v2']\n",
      "0911.3671v1: ['0911.3671v1']\n",
      "0911.1402v2: ['0905.4267v2', '0911.1401v1', '0911.1402v2']\n",
      "0911.3198v3: ['0911.3198v3']\n",
      "0911.4124v1: ['0911.4124v1', '0905.1823v1', '0812.3471v3']\n",
      "0911.1560v1: ['0906.0302v2', '0812.1202v1', '0812.1368v1', '0809.1229v1', '0903.3598v2', '0901.2599v2', '0812.4265v1', '0710.5136v2', '0905.3947v3', '0801.1817v1', '0706.1726v2', '0804.0473v1', '0911.1560v1', '0906.4728v1']\n",
      "0911.2332v1: ['0911.2332v1']\n",
      "0911.1134v2: ['0903.1115v2', '0710.2486v1', '1001.3884v1', '0709.0980v1', '0911.1134v2', '0711.2741v2', '0907.1922v2', '0801.4554v2', '0908.0857v2', '0705.4387v2']\n",
      "0911.3141v1: ['0911.3141v1', '0807.0265v1']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list and print\n",
    "filtered_list = list(filtered.items())\n",
    "print(f\"\\nFiltered citation pairs: {len(filtered_list)}\\n\")\n",
    "\n",
    "temp_count = 0\n",
    "for paper, cites in filtered_list:\n",
    "    print(f\"{paper}: {cites}\")\n",
    "    temp_count += 1\n",
    "    if temp_count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ceeecaf5-4ec9-40c7-9282-f1536a50c3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files saved with correct PostgreSQL array format.\n"
     ]
    }
   ],
   "source": [
    "def to_pg_array(lst):\n",
    "    return \"{\" + \",\".join(f'\"{item}\"' for item in lst) + \"}\"\n",
    "\n",
    "# Create DataFrame for intermediate_filtered and save\n",
    "df_base = pd.DataFrame(\n",
    "    [(base_paper, to_pg_array(cited)) for base_paper, cited in filtered_list_imm],\n",
    "    columns=[\"base_paper_id\", \"cited_base_paper_id\"]\n",
    ")\n",
    "df_base.to_csv(\"citation_filtered_base.csv\", index=False)\n",
    "\n",
    "# Create DataFrame for filtered and save\n",
    "df_final = pd.DataFrame(\n",
    "    [(full_paper, to_pg_array(cites)) for full_paper, cites in filtered_list],\n",
    "    columns=[\"full_paper_id\", \"cited_full_paper_id\"]\n",
    ")\n",
    "df_final.to_csv(\"citation_filtered_final.csv\", index=False)\n",
    "\n",
    "print(\"CSV files saved with correct PostgreSQL array format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b9eb085f-a542-4be0-b525-bf7542f33c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_sql_base = \"DROP TABLE IF EXISTS citation_filtered_base;\"\n",
    "drop_sql_final = \"DROP TABLE IF EXISTS citation_filtered_final;\"\n",
    "\n",
    "create_sql_base = \"\"\"\n",
    "CREATE TABLE citation_filtered_base (\n",
    "    base_paper_id TEXT,\n",
    "    cited_base_paper_id TEXT []\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_sql_final = \"\"\"\n",
    "CREATE TABLE citation_filtered_final (\n",
    "    full_paper_id TEXT,\n",
    "    cited_full_paper_id TEXT []\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(drop_sql_base))\n",
    "    conn.execute(text(drop_sql_final))\n",
    "    conn.execute(text(create_sql_base))\n",
    "    conn.execute(text(create_sql_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f9f7af71-809d-440f-bbd0-6bc6f50b42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_conn = engine.raw_connection()\n",
    "cur = raw_conn.cursor()\n",
    "\n",
    "with open(\"citation_filtered_base.csv\", 'r') as f:\n",
    "    cur.copy_expert(\"\"\"\n",
    "        COPY citation_filtered_base(base_paper_id, cited_base_paper_id)\n",
    "        FROM STDIN\n",
    "        WITH CSV HEADER\n",
    "    \"\"\", f)\n",
    "\n",
    "with open(\"citation_filtered_final.csv\", 'r') as f:\n",
    "    cur.copy_expert(\"\"\"\n",
    "        COPY citation_filtered_final(full_paper_id, cited_full_paper_id)\n",
    "        FROM STDIN\n",
    "        WITH CSV HEADER\n",
    "    \"\"\", f)\n",
    "\n",
    "raw_conn.commit()\n",
    "cur.close()\n",
    "raw_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6822e-de7d-43b8-9845-4743d088e873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c2ae036-425c-49a9-b896-d12e7d7926be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final bucket size: 13629 (target window 14995–15005)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  assume `filtered_ok` already built as before\n",
    "# ------------------------------------------------------------\n",
    "TARGET     = 15_000                     # ideal size\n",
    "TOLERANCE  = 5\n",
    "MAX_SIZE   = TARGET + TOLERANCE        # 3 005\n",
    "MIN_SIZE   = TARGET - TOLERANCE        # 2 995\n",
    "\n",
    "# 1⃣  build Union-Find components (same as before) -------------\n",
    "parent = {}\n",
    "def find(x):\n",
    "    parent.setdefault(x, x)\n",
    "    if parent[x] != x:\n",
    "        parent[x] = find(parent[x])\n",
    "    return parent[x]\n",
    "\n",
    "def union(a, b):\n",
    "    pa, pb = find(a), find(b)\n",
    "    if pa != pb:\n",
    "        parent[pb] = pa\n",
    "\n",
    "for paper, refs in filtered_ok.items():\n",
    "    for r in refs:\n",
    "        union(paper, r)\n",
    "\n",
    "components = defaultdict(set)\n",
    "for node in list(filtered_ok.keys()) + [r for refs in filtered_ok.values() for r in refs]:\n",
    "    components[find(node)].add(node)\n",
    "\n",
    "comp_list = list(components.values())\n",
    "random.Random(42).shuffle(comp_list)    # deterministic shuffle for reproducibility\n",
    "\n",
    "# 2⃣  fill ONE bucket ------------------------------------------\n",
    "bucket = set()\n",
    "\n",
    "for comp in comp_list:\n",
    "    # quit if bucket already in window\n",
    "    if MIN_SIZE <= len(bucket) <= MAX_SIZE:\n",
    "        break\n",
    "\n",
    "    # add component only if it doesn't overshoot the max\n",
    "    if len(bucket) + len(comp) <= MAX_SIZE:\n",
    "        bucket.update(comp)\n",
    "    # if the component is too large or would overshoot, skip it\n",
    "\n",
    "print(f\"Final bucket size: {len(bucket)} (target window {MIN_SIZE}–{MAX_SIZE})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2d3a2ac-a416-430b-860f-72f5b26eb400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  wrote 13,629 IDs\n",
      "✅  wrote citations for 13,629 IDs\n",
      "✅  wrote combined bucket+citations\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "# where to save the files\n",
    "out_dir = \"/home/jovyan/work\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# 1️⃣  dump the raw ID list\n",
    "with open(\"bucket.json\", \"w\") as f:\n",
    "    json.dump(list(bucket), f, indent=2)\n",
    "print(f\"✅  wrote {len(bucket):,} IDs\")\n",
    "\n",
    "# 2️⃣  build & dump the citation map\n",
    "bucket_citations = {pid: filtered_ok.get(pid, []) for pid in bucket}\n",
    "cit_path = os.path.join(out_dir, \"bucket_citations.json\")\n",
    "with open(\"bucket_citations.json\", \"w\") as f:\n",
    "    json.dump(bucket_citations, f, indent=2)\n",
    "print(f\"✅  wrote citations for {len(bucket_citations):,} IDs\")\n",
    "\n",
    "combined_obj  = {\n",
    "    \"ids\": list(bucket),           # same content as bucket.json\n",
    "    \"citations\": bucket_citations  # same content as bucket_citations.json\n",
    "}\n",
    "\n",
    "with open(\"buckets-combined.json\", \"w\") as f:\n",
    "    json.dump(combined_obj, f, indent=2)\n",
    "\n",
    "print(f\"✅  wrote combined bucket+citations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "598e2c5c-e960-4cf8-b605-84434491d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table bucket_citations is ready\n",
      "Loaded 13,629 records\n"
     ]
    }
   ],
   "source": [
    "create_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bucket_citations (\n",
    "    paper_id   TEXT PRIMARY KEY,\n",
    "    citations  TEXT[] NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(create_sql))\n",
    "print(\"table bucket_citations is ready\")\n",
    "\n",
    "with open(\"bucket_citations.json\") as f:\n",
    "    bucket_citations = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(bucket_citations):,} records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1890d5d5-e148-46ec-b7c9-f90336d3ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all rows written to PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "insert_sql = \"\"\"\n",
    "INSERT INTO bucket_citations (paper_id, citations)\n",
    "VALUES (:paper_id, :citations)\n",
    "ON CONFLICT (paper_id)\n",
    "DO UPDATE SET citations = EXCLUDED.citations;\n",
    "\"\"\"\n",
    "\n",
    "payload = [\n",
    "    {\"paper_id\": pid, \"citations\": cites}\n",
    "    for pid, cites in bucket_citations.items()\n",
    "]\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(insert_sql), payload)\n",
    "\n",
    "print(\"all rows written to PostgreSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5d2229a-8801-4e62-8313-f7f527ec1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS arxiv_chunks_eval_data_v1 (\n",
    "    paper_id TEXT,\n",
    "    chunk_id TEXT,\n",
    "    txt_filename TEXT,\n",
    "    query TEXT,\n",
    "    chunk_data TEXT,\n",
    "    paper_cited TEXT[]\n",
    ");\n",
    "\"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(create_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efa8c6ac-a5e6-4a2a-91ce-959f0c582941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  rows inserted (only bucket papers)\n"
     ]
    }
   ],
   "source": [
    "insert_sql = \"\"\"\n",
    "INSERT INTO arxiv_chunks_eval_data_v2 (\n",
    "    paper_id, chunk_id, txt_filename, query, chunk_data, paper_cited\n",
    ")\n",
    "SELECT\n",
    "    ac.paper_id,\n",
    "    ac.chunk_id,\n",
    "    ac.txt_filename,\n",
    "    ac.query,\n",
    "    ac.chunk_data,\n",
    "    bc.citations                       -- TEXT[] from the bucket table\n",
    "FROM arxiv_chunks_backup     AS ac\n",
    "JOIN bucket_citations        AS bc   ON bc.paper_id = ac.paper_id;\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(insert_sql))\n",
    "\n",
    "print(\"✅  rows inserted (only bucket papers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9d411-2886-4688-80ee-7c2d6ab00b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20383eb1-b57b-4bd9-8f81-7a58552189f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c18434-b197-4a4b-9002-8b7419d2bfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed119ac6-a95a-4a02-a3e9-335303a9193d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a4b9d-5822-4913-b9a6-193450b8da19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0e350-9dcf-4641-aee3-db44df56f564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730378af-3e94-4b11-8064-1c9b0e1830fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d115b-4cee-4e9f-8bcd-f31278536aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1b197-698b-456f-b73e-65fcd773e0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d09550d-8103-4337-82af-e5ba8e740368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826c7a3-5870-494b-971e-05b54b586602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb323606-89a4-4840-b5cf-bf6998a72bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
