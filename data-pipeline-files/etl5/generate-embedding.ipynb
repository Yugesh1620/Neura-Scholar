{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Found existing installation: psycopg2-binary 2.9.10\n",
      "Uninstalling psycopg2-binary-2.9.10:\n",
      "  Successfully uninstalled psycopg2-binary-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.9.10-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y psycopg2-binary\n",
    "%pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import shutil\n",
    "import re\n",
    "import unicodedata\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.dialects.postgresql import ARRAY, TEXT\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from sqlalchemy import inspect\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import psycopg2\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerModel(\n",
       "  (embeddings): LongformerEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): LongformerEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): LongformerPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_DIR = \"/Users/riyagarg/Downloads/prepare_eval_data/longformer_model2/model.sentence_transformer\"\n",
    "\n",
    "# ── ❷ Load tokenizer & model from local files ────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_DIR, local_files_only=True)\n",
    "model     = AutoModel.from_pretrained(   HF_DIR, local_files_only=True)\n",
    "model.eval()  # disable dropout, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (2, 768)\n",
      "First vector (first 5 dims): [-0.02423628 -0.0608274   0.4972675   0.30362946  0.11610436]\n",
      "Second vector norm: 16.511986\n"
     ]
    }
   ],
   "source": [
    "#Testing the model\n",
    "\n",
    "# ── ❸ Prepare some texts ─────────────────────────────────────────\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Longformer can process documents up to 4096 tokens!\"\n",
    "]\n",
    "\n",
    "# ── ❹ Tokenize (returns PyTorch tensors) ───────────────────────────\n",
    "inputs = tokenizer(\n",
    "    texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=4096\n",
    ")\n",
    "\n",
    "# ── ❺ Forward pass to get last hidden states ───────────────────────\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden = outputs.last_hidden_state          \n",
    "\n",
    "# ── ❻ Mean‐pool over the token dimension, masking PADs ─────────────\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "masked_hidden  = last_hidden * attention_mask          \n",
    "sum_hidden     = masked_hidden.sum(dim=1)              \n",
    "counts         = attention_mask.sum(dim=1).clamp(min=1e-9)      \n",
    "embeddings     = (sum_hidden / counts).cpu().numpy()            \n",
    "\n",
    "# ── ❼ Inspect ───────────────────────────────────────────────────────\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"First vector (first 5 dims):\", embeddings[0][:5])\n",
    "print(\"Second vector norm:\", np.linalg.norm(embeddings[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS  = 4\n",
    "MODEL_DIR = \"/Users/riyagarg/Downloads/prepare_eval_data/longformer_model2/model.sentence_transformer\"\n",
    "DATABASE_URI = 'postgresql+psycopg2://rg5073:rg5073pass@129.114.27.112:5432/cleaned_meta_data_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(DATABASE_URI, pool_size=8, max_overflow=0)\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "      ALTER TABLE arxiv_chunks_eval_4\n",
    "      ADD COLUMN IF NOT EXISTS chunk_embedding_768 vector(768)\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerModel(\n",
       "  (embeddings): LongformerEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): LongformerEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): LongformerPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    total = conn.execute(text(\"SELECT COUNT(*) FROM arxiv_chunks_eval_4\")).scalar_one()\n",
    "\n",
    "# ───────────── LOAD MODEL & TOKENIZER ─────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "model     = AutoModel.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts):\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        last_hidden = model(**inputs).last_hidden_state  # (N, T, D)\n",
    "    mask   = inputs['attention_mask'].unsqueeze(-1)    # (N, T, 1)\n",
    "    summed = (last_hidden * mask).sum(dim=1)           # (N, D)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)           # (N, 1)\n",
    "    return (summed / counts).cpu().numpy()             # (N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(offset: int):\n",
    "    \"\"\"\n",
    "    Fetches one batch (by OFFSET), embeds, then bulk-updates.\n",
    "    Returns number of rows written.\n",
    "    \"\"\"\n",
    "    # 1) fetch batch\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(\n",
    "            text(\"\"\"\n",
    "              SELECT paper_id, chunk_id, chunk_data\n",
    "                FROM arxiv_chunks_eval_4\n",
    "               ORDER BY paper_id, chunk_id\n",
    "               LIMIT :limit OFFSET :offset\n",
    "            \"\"\"),\n",
    "            {\"limit\": BATCH_SIZE, \"offset\": offset}\n",
    "        ).fetchall()\n",
    "\n",
    "    if not rows:\n",
    "        return 0\n",
    "\n",
    "    # 2) compute embeddings\n",
    "    ids   = [(r.paper_id, r.chunk_id) for r in rows]\n",
    "    texts = [r.chunk_data for r in rows]\n",
    "    embs  = embed_texts(texts)  # shape (len(rows), 768)\n",
    "\n",
    "    # 3) bulk update\n",
    "    params = [\n",
    "        {\"pid\": pid, \"cid\": cid, \"vec\": vec.tolist()}\n",
    "        for (pid, cid), vec in zip(ids, embs)\n",
    "    ]\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(\n",
    "            text(\"\"\"\n",
    "              UPDATE arxiv_chunks_eval_4\n",
    "                 SET chunk_embedding_768 = :vec\n",
    "               WHERE paper_id = :pid\n",
    "                 AND chunk_id   = :cid\n",
    "            \"\"\"),\n",
    "            params\n",
    "        )\n",
    "\n",
    "    return len(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = math.ceil(total / BATCH_SIZE)\n",
    "offsets   = [i * BATCH_SIZE for i in range(n_batches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "… done 2704/52554 rows\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      3\u001b[0m futures \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(process_batch, off): off \u001b[38;5;28;01mfor\u001b[39;00m off \u001b[38;5;129;01min\u001b[39;00m offsets}\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fut \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[1;32m      5\u001b[0m     done \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow-env/lib/python3.8/concurrent/futures/_base.py:244\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 244\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow-env/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow-env/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m         done \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m      6\u001b[0m         processed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m done\n\u001b[0;32m----> 7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m… done \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Finished embedding & updating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows into chunk_embedding_768\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow-env/lib/python3.8/concurrent/futures/_base.py:644\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow-env/lib/python3.8/concurrent/futures/thread.py:236\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 236\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow-env/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow-env/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processed = 0\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = {executor.submit(process_batch, off): off for off in offsets}\n",
    "    for fut in as_completed(futures):\n",
    "        done = fut.result()\n",
    "        processed += done\n",
    "        print(f\"… done {processed}/{total} rows\", end=\"\\r\")\n",
    "\n",
    "print(f\"\\n✅ Finished embedding & updating {processed} rows into chunk_embedding_768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
