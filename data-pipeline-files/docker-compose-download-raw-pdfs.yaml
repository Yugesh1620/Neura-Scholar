version: '3.8'
name: extract-raw-pdfs

volumes:
  project-22-raw-pdfs:

services:
  extract-data:
    container_name: etl_extract_data
    image: python:3.11
    user: root
    volumes:
      - project-22-raw-pdfs:/data
    working_dir: /data
    command:
      - bash
      - -c
      - |
        set -e
        apt-get update && apt-get install -y curl unzip python3-openssl \
        && curl https://sdk.cloud.google.com | bash
        source /root/google-cloud-sdk/path.bash.inc
        echo "Reset & prepare workspace‚Ä¶"
        rm -rf project-22-pdf-data
        mkdir -p project-22-pdf-data
        cd project-22-pdf-data

        echo "Listing all PDF prefixes from GCS‚Ä¶"
        gsutil ls gs://arxiv-dataset/arxiv/arxiv/pdf > files_list.txt

        echo "Extract complete: see files_list.txt"
      
  process-data:
  #combines transform and load step for our kind of data
    container_name: etl_process_data
    image: python:3.11
    user: root
    depends_on:
      - extract-data
    environment:
      - RCLONE_CONTAINER=${RCLONE_CONTAINER}
    volumes:
      - project-22-raw-pdfs:/data
      - ~/.config/rclone/rclone.conf:/root/.config/rclone/rclone.conf:ro
    working_dir: /data/project-22-pdf-data
    entrypoint: bash
    command:
      - -c
      - |
        set -e

        apt-get update && apt-get install -y curl unzip python3-openssl \
        && curl https://sdk.cloud.google.com | bash
        source /root/google-cloud-sdk/path.bash.inc

        if [ -z "$RCLONE_CONTAINER" ]; then
          echo "ERROR: RCLONE_CONTAINER is not set"
        exit 1
        fi

        echo "Cleaning up existing contents of container..."
        rclone delete chi_tacc:$RCLONE_CONTAINER --rmdirs || true

        echo "Ensure remote folder exists: raw-pdf-data"
        rclone mkdir -p chi_tacc:"$RCLONE_CONTAINER/raw-pdf-data" || true

        echo "Starting per‚Äëprefix ETL‚Ä¶"
        # set threshold: 110¬†GiB in bytes
        max_bytes=$((110 * 1024**3))
        total_bytes=0

        while IFS= read -r prefix_uri; do
          echo " prefix-uri: $prefix_uri"

          name=$(basename "$(dirname "$prefix_uri")")
          echo "‚Ä¢ Processing $name‚Ä¶"

          # download PDFs
          echo " Processing $name‚Ä¶"
          echo " prefix-uri: $prefix_uri"
          echo "Container $RCLONE_CONTAINER"

          # now you can download, tar, upload, etc.
          mkdir -p "./$name"

          gsutil -m cp -r "${prefix_uri}" ./"$name"/

          # create the tarball
          tar czf "$name".tar.gz "$name"

          # measure its size (portable via wc)
          size=$(wc -c < "$name".tar.gz)

          # check if we‚Äôd go over the limit
          if (( total_bytes + size > max_bytes )); then
            echo "Reached 110¬†GiB limit (current total: $((total_bytes/1024**3))¬†GiB, next file: $((size/1024**3))¬†GiB)."
            # clean up this prefix and exit
            rm -rf "$name" "$name".tar.gz
            break
          fi

          # update running total
          total_bytes=$((total_bytes + size))
          echo " ‚Üí Uploading $name.tar.gz (size: $((size/1024**3))¬†GiB)‚Ä¶"

          # push and then delete locally
          rclone copy "$name".tar.gz chi_tacc:"$RCLONE_CONTAINER/raw-pdf-data" \
            --progress --transfers=16 --checkers=8 --fast-list
          rm -rf "$name" "$name".tar.gz

          echo " ‚úî Done $name (freed space). Total uploaded: $((total_bytes/1024**3))¬†GiB."
        done < files_list.txt

        echo "‚úî All prefixes processed!"

  process-data1:
    container_name: etl_process_data
    image: rclone/rclone:latest
    user: root
    depends_on:
      - extract-data
    environment:
      - RCLONE_CONTAINER=${RCLONE_CONTAINER}
    volumes:
      - project-22-raw-pdfs:/data
      - ~/.config/rclone/rclone.conf:/root/.config/rclone/rclone.conf:ro
    working_dir: /data/project-22-pdf-data
    entrypoint: ["/bin/sh", "-c"]
    command: |
      set -e

      # 1) Install gsutil (Cloud SDK)
      apt-get update \
        && apt-get install -y curl unzip python3-openssl \
        && curl -sSL https://sdk.cloud.google.com | bash \
        && source /root/google-cloud-sdk/path.bash.inc

      # 2) Validate env
      if [ -z "$RCLONE_CONTAINER" ]; then
        echo "ERROR: RCLONE_CONTAINER is not set" >&2
        exit 1
      fi

      # 3) Show the prefixes we‚Äôll process
      echo "=== files_list.txt ==="
      cat files_list.txt
      echo "======================"

      # 4) Loop with a 110¬†GiB cap
      max_bytes=$((110 * 1024**3))
      total_bytes=0

      while IFS= read -r prefix_uri; do
        # derive ‚Äú2107‚Äù from ‚Äú‚Ä¶/pdf/2107/‚Äù
        uri_no_slash=${prefix_uri%/}
        name=${uri_no_slash##*/}

        echo "‚Ä¢ Processing $name (uri=$prefix_uri)‚Ä¶"

        # download all PDFs in that prefix
        mkdir -p "./$name"
        gsutil -m cp "${prefix_uri}"*.pdf "./$name/"

        # pack into a plain .tar
        tar cf "$name".tar "$name"
        size=$(wc -c < "$name".tar)

        # check cap
        # if (( total_bytes + size > max_bytes )); then
        #   echo "‚ö†Ô∏è Hit 110¬†GiB limit (current $((total_bytes/1024**3))¬†GiB; next $((size/1024**3))¬†GiB)." >&2
        #   rm -rf "$name" "$name".tar
        #   break
        # fi

        # total_bytes=$((total_bytes + size))
        # echo "‚Üí Uploading $name.tar ($((size/1024**3))¬†GiB)‚Ä¶"

        # # upload and remove local tar
        # rclone copy "$name".tar \
        #   chi_tacc:"$RCLONE_CONTAINER/raw-pdf-data" \
        #   --progress --transfers=16 --checkers=8 --fast-list

        # rm -rf "$name" "$name".tar
        echo "‚úî Done $name. Total uploaded: $((total_bytes/1024**3))¬†GiB."
      done < files_list.txt

      echo "üéâ All prefixes processed!"
