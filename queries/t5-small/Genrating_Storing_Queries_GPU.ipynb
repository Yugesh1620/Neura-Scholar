{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c90396a-0535-46c3-9894-1f3734f5b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision tqdm numpy pandas sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b17c1b-21bf-431e-9e36-63b232a1190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import shutil\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75faedb0-d6aa-4a72-88f8-746066623e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ac6d4e6-aaeb-4a4f-850c-6181b94d1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\n",
    "    'postgresql://rg5073:rg5073pass@129.114.26.75:30002/cleaned_meta_data_db',\n",
    "    pool_size=10,\n",
    "    max_overflow=0,\n",
    "    pool_timeout=30,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac09fd-db48-4f05-b21c-d7dfdc33d53e",
   "metadata": {},
   "source": [
    "Showing data with schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9833db-184e-4e57-bb53-221ede571152",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_preview = \"SELECT * FROM arxiv_chunks_training_initial_1 LIMIT 5;\"\n",
    "preview = pd.read_sql(query_preview, engine)\n",
    "print(\" Data:\")\n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8dda3b-787c-44d8-a252-a02d1e82ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_preview = \"SELECT * FROM arxiv_chunks_training_initial LIMIT 5;\"\n",
    "preview = pd.read_sql(query_preview, engine)\n",
    "print(\" Data:\")\n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558b5d8-0b71-4d88-8b59-f78e87099a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "check_query = \"\"\"\n",
    "    SELECT paper_id, chunk_id, query\n",
    "    FROM arxiv_chunks_training_initial_1\n",
    "    WHERE query IS NOT NULL\n",
    "\"\"\"\n",
    "df = pd.read_sql(check_query, engine)\n",
    "\n",
    "df = df[df[\"query\"].str.strip().ne(\"\")]\n",
    "df[\"query_list\"] = df[\"query\"].apply(json.loads)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    print(f\"\\n Paper ID: {row['paper_id']}\")\n",
    "    print(f\" Chunk ID: {row['chunk_id']}\")\n",
    "    print(\"Queries:\")\n",
    "    for i, q in enumerate(row[\"query_list\"], 1):\n",
    "        print(f\"  {i}. {q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b0681-561a-4494-a2f6-e5ecca630e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sqlalchemy tqdm pandas torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612005f-39da-4a97-9e33-58b7a8e61508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "engine = create_engine(\n",
    "    'postgresql://rg5073:rg5073pass@129.114.26.75:30002/cleaned_meta_data_db',\n",
    "    pool_size=10, max_overflow=0, pool_timeout=30\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT paper_id, chunk_id, chunk_data\n",
    "    FROM arxiv_chunks_training_initial_1\n",
    "    ORDER BY chunk_id\n",
    "    LIMIT 20\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    chunk = row[\"chunk_data\"]\n",
    "    if len(chunk.split()) < 30:\n",
    "        continue\n",
    "    row[\"chunk_data\"] = chunk\n",
    "    records.append(row)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "for i in tqdm(range(0, len(records), batch_size)):\n",
    "    batch = records[i:i+batch_size]\n",
    "    prompts = [\n",
    "        f\"List 3 short search phrases (not questions) that are relevant for this scientific text:\\n\\n{r['chunk_data']}\"\n",
    "        for r in batch\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            num_return_sequences=3,\n",
    "            do_sample=True,\n",
    "            temperature=0.95,\n",
    "        )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    grouped_outputs = [decoded_outputs[j:j+3] for j in range(0, len(decoded_outputs), 3)]\n",
    "\n",
    "    with engine.begin() as connection:\n",
    "        for record, phrases in zip(batch, grouped_outputs):\n",
    "            print(f\"\\n Raw model outputs for chunk {record['chunk_id']} (Paper: {record['paper_id']}):\")\n",
    "            for idx, p in enumerate(phrases, 1):\n",
    "                print(f\"[Output {idx}]: {p}\")\n",
    "\n",
    "            cleaned = []\n",
    "            for phrase in phrases:\n",
    "                for line in phrase.split(\"\\n\"):\n",
    "                    line = line.strip()\n",
    "                    if any(c.isalpha() for c in line) and len(line) > 3:\n",
    "                        cleaned.append(line)\n",
    "                        break\n",
    "\n",
    "            if len(cleaned) == 3:\n",
    "                connection.execute(text(\"\"\"\n",
    "                    UPDATE arxiv_chunks_training_initial_1\n",
    "                    SET query = :query_data\n",
    "                    WHERE paper_id = :pid AND chunk_id = :cid\n",
    "                \"\"\"), {\n",
    "                    \"query_data\": json.dumps(cleaned),\n",
    "                    \"pid\": record[\"paper_id\"],\n",
    "                    \"cid\": record[\"chunk_id\"]\n",
    "                })\n",
    "                print(f\" Stored  3 clean phrases for chunk {record['chunk_id']}\")\n",
    "            else:\n",
    "                print(f\" Skiped {record['chunk_id']} — only {len(cleaned)} valid phrases\")\n",
    "\n",
    "print(f\"\\n{len(records)} valid chunks processed — stored 3 clean phrases each (if valid).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5df09-c8f4-4b17-a240-454acd5f634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "engine = create_engine(\n",
    "    'postgresql://rg5073:rg5073pass@129.114.26.75:30002/cleaned_meta_data_db',\n",
    "    pool_size=10, max_overflow=0, pool_timeout=30\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT paper_id, chunk_id, chunk_data\n",
    "    FROM arxiv_chunks_training_initial_1\n",
    "    ORDER BY chunk_id\n",
    "    LIMIT 2000\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    chunk = row[\"chunk_data\"]\n",
    "    if len(chunk.split()) < 30:\n",
    "        continue\n",
    "    row[\"chunk_data\"] = chunk\n",
    "    records.append(row)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "for i in tqdm(range(0, len(records), batch_size)):\n",
    "    batch = records[i:i+batch_size]\n",
    "    prompts = [\n",
    "        f\"List 3 short search phrases (not questions) that are relevant for this scientific text:\\n\\n{r['chunk_data']}\"\n",
    "        for r in batch\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            num_return_sequences=3,\n",
    "            do_sample=True,\n",
    "            temperature=0.95,\n",
    "        )\n",
    "\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    grouped_outputs = [decoded_outputs[j:j+3] for j in range(0, len(decoded_outputs), 3)]\n",
    "\n",
    "    with engine.begin() as connection:\n",
    "        for record, phrases in zip(batch, grouped_outputs):\n",
    "            print(f\"\\n Raw model outputs for chunk {record['chunk_id']} (Paper: {record['paper_id']}):\")\n",
    "            for idx, p in enumerate(phrases, 1):\n",
    "                print(f\"[Output {idx}]: {p}\")\n",
    "\n",
    "            cleaned = []\n",
    "            for phrase in phrases:\n",
    "                for line in phrase.split(\"\\n\"):\n",
    "                    line = line.strip()\n",
    "                    if any(c.isalpha() for c in line) and len(line) > 3:\n",
    "                        cleaned.append(line)\n",
    "                        break\n",
    "\n",
    "            if len(cleaned) == 3:\n",
    "                connection.execute(text(\"\"\"\n",
    "                    UPDATE arxiv_chunks_training_initial_1\n",
    "                    SET query = :query_data\n",
    "                    WHERE paper_id = :pid AND chunk_id = :cid\n",
    "                \"\"\"), {\n",
    "                    \"query_data\": json.dumps(cleaned),\n",
    "                    \"pid\": record[\"paper_id\"],\n",
    "                    \"cid\": record[\"chunk_id\"]\n",
    "                })\n",
    "                print(f\" Stored  3 clean phrases for chunk {record['chunk_id']}\")\n",
    "            else:\n",
    "                print(f\" Skiped {record['chunk_id']} — only {len(cleaned)} valid phrases\")\n",
    "\n",
    "print(f\"\\n{len(records)} valid chunks processed — stored 3 clean phrases each (if valid).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c41bb-dccb-45bc-8c89-f48e1d8ebc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
