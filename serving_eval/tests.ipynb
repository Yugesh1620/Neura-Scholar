{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01a7c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & config\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Your Postgres connection\n",
    "DB_URL     = \"postgresql+psycopg2://rg5073:rg5073pass@129.114.27.112:5432/cleaned_meta_data_db\"\n",
    "TABLE_NAME = \"arxiv_chunks_eval_5\"\n",
    "EVAL_DIR   = \"/home/pb/projects/course/sem2/mlops/project/mlops/Neura-Scholar/serving_eval/eval\"        # where heldout.jsonl etc live\n",
    "TOP_K      = 10            # how many papers to return & evaluate\n",
    "\n",
    "# The embedding columns you’ve stored, e.g.:\n",
    "MODEL_DETAILS = [\n",
    "    {\n",
    "        \"column\": \"chunk_embedding_768\",\n",
    "        \"model_path\": \"/home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert.onnx\",\n",
    "    },\n",
    "    {\n",
    "        \"column\": \"chunk_embedding_768_dyn\",\n",
    "        \"model_path\": \"/home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_dyn.onnx\",\n",
    "    },\n",
    "    {\n",
    "        \"column\": \"chunk_embedding_768_graph\",\n",
    "        \"model_path\": \"/home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_opt.onnx\",\n",
    "    },\n",
    "    # {\n",
    "    #     \"column\": \"chunk_embedding_768_static_h\",\n",
    "    #     \"model_path\": \"/home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_static_heavy.onnx\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"column\": \"chunk_embedding_768_static_m\",\n",
    "    #     \"model_path\": \"/home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_static_moderate.onnx\",\n",
    "    # },\n",
    "]\n",
    "\n",
    "engine = create_engine(DB_URL, pool_timeout=30, max_overflow=0)\n",
    "os.makedirs(\"indexes\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8feb36d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load your eval test records once\n",
    "heldout = [json.loads(l) for l in open(f\"{EVAL_DIR}/heldout.jsonl\")]\n",
    "slices  = [json.loads(l) for l in open(f\"{EVAL_DIR}/slices.jsonl\")]\n",
    "perturbs= [json.loads(l) for l in open(f\"{EVAL_DIR}/perturbations.jsonl\")]\n",
    "failures=[json.loads(l) for l in open(f\"{EVAL_DIR}/failures.jsonl\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aa2e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Helper to compute Recall@K and MRR@K\n",
    "def recall_at_k(gt, pred, k=TOP_K):\n",
    "    return int(any(p in gt for p in pred[:k]))\n",
    "\n",
    "def mrr_at_k(gt, pred, k=TOP_K):\n",
    "    for rank, p in enumerate(pred[:k], start=1):\n",
    "        if p in gt:\n",
    "            return 1.0/rank\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87165860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:43:59 INFO === Evaluating `chunk_embedding_768` with ONNX [/home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert.onnx] ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:43:59 INFO Loaded index `indexes/chunk_embedding_768.index` in 0.03s\n",
      "\u001b[0;93m2025-05-12 15:44:00.120345087 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 6 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-12 15:44:00.121458846 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-12 15:44:00.121467727 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "15:44:00 INFO Loaded ONNX session + tokenizer in 0.54s\n",
      "15:44:01 WARNING Embedding mismatch for `chunk_embedding_768` (cos = 0.684) – skipping this model.\n",
      "15:44:01 INFO Embedding sanity‑check passed (cos = 0.684)\n",
      "15:44:19 INFO → Held‑out Recall@10 (raw|adj|chunk) = 0.1096 | 0.2525 | 0.8267 ,  MRR@10 = 0.0915\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Cell 4 — Evaluation loop   (40 % noise in ground‑truth handled)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import ast, os, time, math\n",
    "from collections import defaultdict\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ---------- helpers --------------------------------------------------------\n",
    "def recall_at_k(gt: list[str], retrieved: list[str], k:int = TOP_K) -> float:\n",
    "    if not gt:\n",
    "        return 0.0\n",
    "    hit = sum(1 for p in retrieved[:k] if p in gt)\n",
    "    return hit / len(gt)\n",
    "\n",
    "def adj_recall_at_k(gt: list[str], retrieved: list[str], k:int = TOP_K) -> float:\n",
    "    if not gt:\n",
    "        return 0.0\n",
    "    effective_rel = max(1, math.ceil(0.4 * len(gt)))          # assume 40 % relev.\n",
    "    hit = sum(1 for p in retrieved[:k] if p in gt)\n",
    "    return hit / effective_rel\n",
    "\n",
    "# ---------- main loop ------------------------------------------------------\n",
    "results        = []\n",
    "EMBED_TOKENIZER = \"distilbert/distilbert-base-uncased\"\n",
    "\n",
    "for mdl in MODEL_DETAILS:\n",
    "    col       = mdl[\"column\"]\n",
    "    onnx_path = mdl[\"model_path\"]\n",
    "    meta_file = f\"indexes/{col}_meta.jsonl\"\n",
    "    idx_file  = f\"indexes/{col}.index\"\n",
    "\n",
    "    logger.info(f\"=== Evaluating `{col}` with ONNX [{onnx_path}] ===\")\n",
    "\n",
    "    # ── 1.  Load / build FAISS index ───────────────────────────────────────\n",
    "    if os.path.exists(idx_file):\n",
    "        t0 = time.perf_counter()\n",
    "        index = faiss.read_index(idx_file)\n",
    "        logger.info(\"Loaded index `%s` in %.2fs\", idx_file, time.perf_counter()-t0)\n",
    "\n",
    "        if os.path.exists(meta_file):\n",
    "            df_emb = pd.read_json(meta_file, lines=True)\n",
    "        else:\n",
    "            df_emb = pd.read_sql(\n",
    "                f\"SELECT chunk_id, paper_cited FROM {TABLE_NAME}\",\n",
    "                con=engine\n",
    "            )\n",
    "            df_emb[\"paper_list\"] = (\n",
    "                df_emb[\"paper_cited\"].str.strip(\"{}\").str.split(\",\")\n",
    "            )\n",
    "            logger.info(\"Meta file missing – reloaded chunk_ids+paper_lists from Postgres\")\n",
    "\n",
    "    else:\n",
    "        t0  = time.perf_counter()\n",
    "        sql = f\"SELECT chunk_id, paper_cited, chunk_data, {col} FROM {TABLE_NAME}\"\n",
    "        df_emb = pd.read_sql(sql, con=engine)\n",
    "        df_emb[\"paper_list\"] = (\n",
    "            df_emb[\"paper_cited\"].str.strip(\"{}\").str.split(\",\")\n",
    "        )\n",
    "\n",
    "        df_emb[\"emb_list\"] = df_emb[col].apply(\n",
    "            lambda v: v if isinstance(v, list) else ast.literal_eval(v)\n",
    "        )\n",
    "        embs = np.array(df_emb[\"emb_list\"].tolist(), dtype=\"float32\")\n",
    "        faiss.normalize_L2(embs)\n",
    "\n",
    "        index = faiss.IndexFlatIP(embs.shape[1])\n",
    "        index.add(embs)\n",
    "\n",
    "        os.makedirs(\"indexes\", exist_ok=True)\n",
    "        faiss.write_index(index, idx_file)\n",
    "        df_emb[[\"chunk_id\", \"paper_list\"]].to_json(\n",
    "            meta_file, orient=\"records\", lines=True\n",
    "        )\n",
    "        logger.info(\"Built & saved index `%s` in %.2fs\", col, time.perf_counter()-t0)\n",
    "\n",
    "    chunk_ids   = df_emb[\"chunk_id\"].tolist()\n",
    "    paper_lists = df_emb[\"paper_list\"].tolist()\n",
    "\n",
    "    # ── 2.  ONNX session + tokenizer  +  sanity‑check───────────────────────\n",
    "    t1       = time.perf_counter()\n",
    "    ort_sess = ort.InferenceSession(\n",
    "        onnx_path, providers=[\"CUDAExecutionProvider\"]\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(EMBED_TOKENIZER)\n",
    "    logger.info(\"Loaded ONNX session + tokenizer in %.2fs\", time.perf_counter()-t1)\n",
    "\n",
    "    # --- 2a.  encode_query helper -----------------------------------------\n",
    "    def encode_query(text: str):\n",
    "        toks = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"np\",\n",
    "            max_length=300,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        out = ort_sess.run(\n",
    "            None,\n",
    "            {\n",
    "                ort_sess.get_inputs()[0].name: toks[\"input_ids\"].astype(\"int64\"),\n",
    "                ort_sess.get_inputs()[1].name: toks[\"attention_mask\"].astype(\"int64\"),\n",
    "            },\n",
    "        )[0]\n",
    "        mask = np.expand_dims(toks[\"attention_mask\"], -1).astype(\"float32\")\n",
    "        emb  = (out * mask).sum(1) / np.clip(mask.sum(1), 1e-9, None)\n",
    "        faiss.normalize_L2(emb)\n",
    "        return emb.astype(\"float32\")\n",
    "\n",
    "    # --- 2b.  *sanity‑check* embedding path -------------------------------\n",
    "    sample = pd.read_sql(\n",
    "        f\"SELECT chunk_data, {col} FROM {TABLE_NAME} LIMIT 1\",\n",
    "        con=engine,\n",
    "    ).iloc[0]\n",
    "\n",
    "    offline_vec = np.array(\n",
    "        sample[col] if isinstance(sample[col], list) else ast.literal_eval(sample[col]),\n",
    "        dtype=\"float32\",\n",
    "    )\n",
    "    offline_vec = offline_vec / np.linalg.norm(offline_vec)  # ensure unit\n",
    "    online_vec  = encode_query(sample[\"chunk_data\"])[0]\n",
    "\n",
    "    cosine_sim  = float(np.dot(offline_vec, online_vec))\n",
    "    if cosine_sim < 0.90:\n",
    "        logger.warning(\n",
    "            \"Embedding mismatch for `%s` (cos = %.3f).\",\n",
    "            col,\n",
    "            cosine_sim,\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Embedding sanity‑check passed (cos = %.3f)\", cosine_sim)\n",
    "\n",
    "    # --- retrieval helper --------------------------------------------------\n",
    "    def retrieve_papers(q_emb, top_n=TOP_K):\n",
    "        D, I = index.search(q_emb, top_n * 20)        # bigger over‑fetch\n",
    "        paper2score = {}\n",
    "        for scores, idxs in zip(D, I):\n",
    "            for sc, idx in zip(scores, idxs):\n",
    "                for pid in paper_lists[idx]:\n",
    "                    paper2score[pid] = max(paper2score.get(pid, -1e9), sc)\n",
    "        return [p for p, _ in sorted(paper2score.items(), key=lambda x: -x[1])][:top_n], I\n",
    "\n",
    "    # ── 3.  Held‑out evaluation (paper + chunk recall) ─────────────────────\n",
    "    raw_rec   = []\n",
    "    adj_rec   = []\n",
    "    chunk_rec = []          # NEW\n",
    "    mrrs      = []\n",
    "\n",
    "    for rec in heldout:\n",
    "        q_emb          = encode_query(rec[\"query\"])\n",
    "        top_papers, I  = retrieve_papers(q_emb)\n",
    "        raw_rec.append(recall_at_k(rec[\"ground_truth\"], top_papers))\n",
    "        adj_rec.append(adj_recall_at_k(rec[\"ground_truth\"], top_papers))\n",
    "        mrrs.append(mrr_at_k(rec[\"ground_truth\"], top_papers))\n",
    "\n",
    "        # chunk‑level recall ⟶ any retrieved *chunk* from a GT paper?\n",
    "        top_chunk_hit = sum(\n",
    "            1\n",
    "            for idx in I[0][: TOP_K * 20]\n",
    "            if any(pid in rec[\"ground_truth\"] for pid in paper_lists[idx])\n",
    "        )\n",
    "        chunk_rec.append(top_chunk_hit / max(1, len(rec[\"ground_truth\"])))\n",
    "\n",
    "    logger.info(\n",
    "        \"→ Held‑out Recall@%d (raw|adj|chunk) = %.4f | %.4f | %.4f ,  MRR@%d = %.4f\",\n",
    "        TOP_K,\n",
    "        np.mean(raw_rec),\n",
    "        np.mean(adj_rec),\n",
    "        np.mean(chunk_rec),\n",
    "        TOP_K,\n",
    "        np.mean(mrrs),\n",
    "    )\n",
    "\n",
    "    # ── 4.  Slice evaluation (adjusted recall) ─────────────────────────────\n",
    "    slice_scores = defaultdict(list)\n",
    "    for rec in slices:\n",
    "        tops, _ = retrieve_papers(encode_query(rec[\"query\"]))\n",
    "        slice_scores[rec[\"slice\"]].append(\n",
    "            adj_recall_at_k(rec[\"ground_truth\"], tops)\n",
    "        )\n",
    "    logger.info(\"→ Slice count: %d\", len(slice_scores))\n",
    "\n",
    "    # ── 5.  Perturbation / failure‑mode checks (unchanged) ─────────────────\n",
    "    perturb_ok = [\n",
    "        retrieve_papers(encode_query(r[\"perturbed\"]), 1)[0][0] in r[\"expected_papers\"]\n",
    "        for r in perturbs\n",
    "    ]\n",
    "    logger.info(\"→ Perturbation pass rate %.4f\", np.mean(perturb_ok))\n",
    "\n",
    "    failure_ok = [\n",
    "        retrieve_papers(encode_query(r[\"query\"]), 1)[0][0] in r[\"correct_papers\"]\n",
    "        for r in failures\n",
    "    ]\n",
    "    logger.info(\"→ Failure‑mode pass rate %.4f\", np.mean(failure_ok))\n",
    "\n",
    "    # ── 6.  Collect results  ───────────────────────────────────────────────\n",
    "    results.append(\n",
    "        {\n",
    "            \"model\":           col,\n",
    "            \"recall@10_raw\":   np.mean(raw_rec),\n",
    "            \"recall@10_adj\":   np.mean(adj_rec),\n",
    "            \"chunk_recall\":    np.mean(chunk_rec),\n",
    "            \"MRR@10\":          np.mean(mrrs),\n",
    "            \"slice_recalls\":   {s: np.mean(v) for s, v in slice_scores.items()},\n",
    "            \"perturb_acc\":     np.mean(perturb_ok),\n",
    "            \"failure_acc\":     np.mean(failure_ok),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ---------- save & show ----------------------------------------------------\n",
    "df_res = pd.DataFrame(results)\n",
    "os.makedirs(\"eval\", exist_ok=True)\n",
    "df_res.to_json(\"eval/model_comparison.jsonl\", orient=\"records\", lines=True)\n",
    "logger.info(\"Done! Results written to eval/model_comparison.jsonl\")\n",
    "df_res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
