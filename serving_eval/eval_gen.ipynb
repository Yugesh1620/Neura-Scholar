{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8085f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total query–chunk rows: 18525\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports, Config & Data Pull + Parsing\n",
    "import os, json, random, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# ─── Configuration ──────────────────────────────────────────────────────────────\n",
    "DB_URL       = \"postgresql+psycopg2://local:password@localhost:5433/mlops_local\"\n",
    "TABLE_NAME   = \"arxiv_chunks_with_metadata\"\n",
    "EVAL_DIR     = \"eval\"\n",
    "HOLDOUT_PCT  = 0.10    # 10% held-out\n",
    "DEV_PCT      = 0.10    # 10% dev/slice\n",
    "PERTURB_BASE = 200     # # base records to perturb\n",
    "DRIFT_N      = 2000    # # of chunks for drift reference\n",
    "random.seed(42)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "os.makedirs(EVAL_DIR, exist_ok=True)\n",
    "engine = create_engine(DB_URL, pool_timeout=30, max_overflow=0)\n",
    "\n",
    "# Pull only needed columns\n",
    "df = pd.read_sql(f\"\"\"\n",
    "    SELECT chunk_id,\n",
    "           paper_cited,\n",
    "           query,\n",
    "           chunk_data,\n",
    "           categories,\n",
    "           created_yymm\n",
    "    FROM {TABLE_NAME}\n",
    "\"\"\", con=engine)\n",
    "\n",
    "# Parse string columns into Python lists\n",
    "def parse_queries(qstr):\n",
    "    try:\n",
    "        return ast.literal_eval(qstr)\n",
    "    except:\n",
    "        return [qstr]\n",
    "\n",
    "def parse_papers(pstr):\n",
    "    return pstr.strip(\"{}\").split(\",\") if pstr.startswith(\"{\") else [pstr]\n",
    "\n",
    "def parse_categories(cstr):\n",
    "    try:\n",
    "        lst = ast.literal_eval(cstr)      # e.g. [\"math.DS math.CV math.PR\"]\n",
    "        return lst[0].split()             # → [\"math.DS\",\"math.CV\",\"math.PR\"]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "df[\"query_list\"]    = df[\"query\"].apply(parse_queries)\n",
    "df[\"paper_list\"]    = df[\"paper_cited\"].apply(parse_papers)\n",
    "df[\"category_list\"] = df[\"categories\"].apply(parse_categories)\n",
    "\n",
    "# Flatten so each row = one (query, chunk_id, paper_list, chunk_data, category_list, created_yymm)\n",
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    for q in r[\"query_list\"]:\n",
    "        rows.append({\n",
    "            \"chunk_id\":      r[\"chunk_id\"],\n",
    "            \"query\":         q,\n",
    "            \"paper_list\":    r[\"paper_list\"],\n",
    "            \"chunk_data\":    r[\"chunk_data\"],\n",
    "            \"category_list\": r[\"category_list\"],\n",
    "            \"created_yymm\":  r[\"created_yymm\"]\n",
    "        })\n",
    "df2 = pd.DataFrame(rows).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "n = len(df2)\n",
    "print(f\"Total query-chunk rows: {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb3201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1852 records to eval/heldout.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Held-Out Test Set (~10%)\n",
    "n_hold = int(HOLDOUT_PCT * n)\n",
    "hold   = df2.iloc[:n_hold]\n",
    "\n",
    "with open(f\"{EVAL_DIR}/heldout.jsonl\", \"w\") as f:\n",
    "    for _, r in hold.iterrows():\n",
    "        rec = {\n",
    "            \"query\":        r[\"query\"],\n",
    "            \"ground_truth\": r[\"paper_list\"]\n",
    "        }\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(hold)} records to {EVAL_DIR}/heldout.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "260d89ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1852 records to eval/slices.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dev Set + Slice Tags (~10%)\n",
    "n_dev = int(DEV_PCT * n)\n",
    "dev   = df2.iloc[n_hold : n_hold + n_dev].copy()\n",
    "\n",
    "def make_slice(r):\n",
    "    cats = r[\"category_list\"]\n",
    "    first = cats[0] if cats else \"other\"\n",
    "    year  = r[\"created_yymm\"][:4] if pd.notna(r[\"created_yymm\"]) else \"unk\"\n",
    "    return f\"{first}_{year}\"\n",
    "\n",
    "dev[\"slice\"] = dev.apply(make_slice, axis=1)\n",
    "\n",
    "with open(f\"{EVAL_DIR}/slices.jsonl\", \"w\") as f:\n",
    "    for _, r in dev.iterrows():\n",
    "        rec = {\n",
    "            \"query\":        r[\"query\"],\n",
    "            \"ground_truth\": r[\"paper_list\"],\n",
    "            \"slice\":        r[\"slice\"]\n",
    "        }\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(dev)} records to {EVAL_DIR}/slices.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2399747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 400 records to eval/perturbations.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Perturbation Stability (~PERTURB_BASE bases × 2 variants)\n",
    "base = dev.sample(min(PERTURB_BASE, len(dev)), random_state=42)\n",
    "perturbs = []\n",
    "for _, r in base.iterrows():\n",
    "    q, papers = r[\"query\"], r[\"paper_list\"]\n",
    "    perturbs += [\n",
    "        {\"base_query\": q,                   \"perturbed\": q.upper(),                  \"expected_papers\": papers},\n",
    "        {\"base_query\": q, \"perturbed\": q.replace(\" \", \"   \"), \"expected_papers\": papers},\n",
    "    ]\n",
    "\n",
    "with open(f\"{EVAL_DIR}/perturbations.jsonl\", \"w\") as f:\n",
    "    for rec in perturbs:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(perturbs)} records to {EVAL_DIR}/perturbations.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b88802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 31 records to eval/failures.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Known Failure-Mode Set (~50 cases matching a keyword)\n",
    "hard = dev[dev[\"query\"].str.contains(\"quantum\", case=False)].head(50)\n",
    "failures = [\n",
    "    {\"query\": r[\"query\"], \"correct_papers\": r[\"paper_list\"]}\n",
    "    for _, r in hard.iterrows()\n",
    "]\n",
    "\n",
    "with open(f\"{EVAL_DIR}/failures.jsonl\", \"w\") as f:\n",
    "    for rec in failures:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(failures)} records to {EVAL_DIR}/failures.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92cda2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2000 records to eval/drift_reference.npz\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Drift Reference (sample DRIFT_N from remaining pool)\n",
    "train_pool = df2.iloc[n_hold + n_dev :]\n",
    "sampled    = train_pool.sample(min(DRIFT_N, len(train_pool)), random_state=42)\n",
    "\n",
    "# Save chunk_ids + raw texts; embeddings happen later\n",
    "np.savez(\n",
    "    f\"{EVAL_DIR}/drift_reference.npz\",\n",
    "    chunk_ids = sampled[\"chunk_id\"].values,\n",
    "    texts     = sampled[\"chunk_data\"].values\n",
    ")\n",
    "\n",
    "print(f\"Wrote {len(sampled)} records to {EVAL_DIR}/drift_reference.npz\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
