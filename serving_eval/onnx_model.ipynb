{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea8b06f",
   "metadata": {},
   "source": [
    "# Export HuggingFace Models to ONNX\n",
    "\n",
    "This notebook shows how to export:\n",
    "- A distilbert embedding model (encoder-only)  \n",
    "- A BART summarization model (seq2seq-LM)\n",
    "\n",
    "…to ONNX format, ready for GPU inference (e.g. with TensorRT).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313b1489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: onnxruntime in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (1.21.1)\n",
      "Requirement already satisfied: filelock in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (80.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: coloredlogs in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from onnxruntime) (4.25.7)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch onnxruntime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba0a37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from itertools import chain\n",
    "from transformers import (\n",
    "    DistilBertModel, DistilBertTokenizer,\n",
    "    AutoModelForSeq2SeqLM, AutoTokenizer\n",
    ")\n",
    "from transformers.onnx import FeaturesManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21f808dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from torch.jit._trace import TracerWarning\n",
    "import warnings\n",
    "\n",
    "# --- suppress tracer warnings if you like ---\n",
    "warnings.filterwarnings(\"ignore\", category=TracerWarning)\n",
    "\n",
    "# --- basic logging setup ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9be50f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Export DistilBERT → ONNX (no changes here)\n",
    "def export_embed_model(\n",
    "    model_dir: str,\n",
    "    onnx_output_path: str,\n",
    "    opset: int = 17\n",
    "):\n",
    "    t0 = time.time()\n",
    "    logger.info(f\"⏳ Loading DistilBERT model from `{model_dir}`\")\n",
    "    model = DistilBertModel.from_pretrained(model_dir)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
    "    model.eval()\n",
    "    logger.info(f\"✅ Loaded model & tokenizer in {time.time() - t0:.1f}s\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    logger.info(\"⏳ Building ONNX config for encoder-only export\")\n",
    "    _, onnx_config_cls = FeaturesManager.check_supported_model_or_raise(model, feature=\"default\")\n",
    "    onnx_config = onnx_config_cls(model.config)\n",
    "    dummy_inputs = onnx_config.generate_dummy_inputs(tokenizer, framework=\"pt\")\n",
    "    logger.info(f\"✅ Prepared ONNX config & dummy inputs in {time.time() - t1:.1f}s\")\n",
    "\n",
    "    input_names  = list(onnx_config.inputs.keys())\n",
    "    output_names = list(onnx_config.outputs.keys())\n",
    "    dynamic_axes = {**onnx_config.inputs, **onnx_config.outputs}\n",
    "    example_inputs = tuple(dummy_inputs[n] for n in input_names)\n",
    "\n",
    "    t2 = time.time()\n",
    "    logger.info(f\"⏳ Exporting to ONNX (opset {opset}) → {onnx_output_path}\")\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        example_inputs,\n",
    "        onnx_output_path,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=opset,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    logger.info(f\"✅ Exported ONNX in {time.time() - t2:.1f}s\")\n",
    "    logger.info(f\"🏁 Total embed export time: {time.time() - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43894922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_summarization_model(\n",
    "    hf_name_or_dir: str,\n",
    "    onnx_output_path: str,\n",
    "    opset: int = 17\n",
    "):\n",
    "    t0 = time.time()\n",
    "    logger.info(f\"⏳ Loading seq2seq model from `{hf_name_or_dir}`\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(hf_name_or_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_name_or_dir)\n",
    "    model.eval()\n",
    "    logger.info(f\"✅ Loaded model & tokenizer in {time.time() - t0:.1f}s\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    logger.info(\"⏳ Building ONNX config for seq2seq-LM export\")\n",
    "    _, onnx_config_class = FeaturesManager.check_supported_model_or_raise(\n",
    "        model, feature=\"seq2seq-lm\"\n",
    "    )\n",
    "    onnx_config = onnx_config_class(model.config)\n",
    "    dummy_inputs = onnx_config.generate_dummy_inputs(tokenizer, framework=\"pt\")\n",
    "    logger.info(f\"✅ Prepared ONNX config & dummy inputs in {time.time() - t1:.1f}s\")\n",
    "\n",
    "    # Prepare names & axes\n",
    "    input_names = list(onnx_config.inputs.keys())\n",
    "    output_names = list(onnx_config.outputs.keys())\n",
    "    dynamic_axes = {**onnx_config.inputs, **onnx_config.outputs}\n",
    "    example_inputs = tuple(dummy_inputs[n] for n in input_names)\n",
    "\n",
    "    t2 = time.time()\n",
    "    logger.info(f\"⏳ Exporting seq2seq model to ONNX (opset {opset}) → {onnx_output_path}\")\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        example_inputs,\n",
    "        onnx_output_path,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=opset,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    logger.info(f\"✅ Exported ONNX in {time.time() - t2:.1f}s\")\n",
    "    logger.info(f\"🏁 Total summarization export time: {time.time() - t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a10abfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:27:04 INFO ⏳ Loading DistilBERT model from `/home/pb/projects/course/sem2/mlops/project/mlops/models/artifacts/model/model.sentence_transformer`\n",
      "22:27:04 INFO ✅ Loaded model & tokenizer in 0.1s\n",
      "22:27:04 INFO ⏳ Building ONNX config for encoder-only export\n",
      "22:27:04 INFO ✅ Prepared ONNX config & dummy inputs in 0.0s\n",
      "22:27:04 INFO ⏳ Exporting to ONNX (opset 17) → /home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert.onnx\n",
      "/tmp/ipykernel_143424/3589074905.py:28: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.8, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "22:27:05 INFO ✅ Exported ONNX in 1.3s\n",
      "22:27:05 INFO 🏁 Total embed export time: 1.4s\n",
      "22:27:05 INFO ⏳ Loading seq2seq model from `facebook/bart-large`\n",
      "22:27:07 INFO ✅ Loaded model & tokenizer in 2.2s\n",
      "22:27:07 INFO ⏳ Building ONNX config for seq2seq-LM export\n",
      "22:27:07 INFO ✅ Prepared ONNX config & dummy inputs in 0.0s\n",
      "22:27:07 INFO ⏳ Exporting seq2seq model to ONNX (opset 17) → /home/pb/projects/course/sem2/mlops/project/mlops/models/bart_summarize.onnx\n",
      "/tmp/ipykernel_143424/1886556888.py:30: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.8, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "22:27:15 INFO ✅ Exported ONNX in 7.6s\n",
      "22:27:15 INFO 🏁 Total summarization export time: 9.8s\n"
     ]
    }
   ],
   "source": [
    "# export into your folder\n",
    "ONNX_DIR = Path(\"/home/pb/projects/course/sem2/mlops/project/mlops/models\")\n",
    "ONNX_DIR.mkdir(exist_ok=True)\n",
    "export_embed_model(\n",
    "    model_dir=\"/home/pb/projects/course/sem2/mlops/project/mlops/models/artifacts/model/model.sentence_transformer\",\n",
    "    onnx_output_path=str(ONNX_DIR/\"distilbert.onnx\"),\n",
    ")\n",
    "export_summarization_model(\n",
    "    hf_name_or_dir=\"facebook/bart-large\",\n",
    "    onnx_output_path=str(ONNX_DIR/\"bart_summarize.onnx\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0362f937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source URI: mlflow-artifacts:/7/72bdeff2ad79435fac87e63bd17da8cd/artifacts/model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2359ae52f0934dda8869f13ce0135cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flavors available: dict_keys(['python_function', 'transformers'])\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models import Model\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Fetch metadata for version 1 of your model\n",
    "mv = client.get_model_version(name=\"facebook-bart-large\", version=\"1\")\n",
    "print(\"Source URI:\", mv.source)\n",
    "\n",
    "# Load the MLmodel metadata and list its flavors\n",
    "model_conf = Model.load(mv.source)\n",
    "print(\"Flavors available:\", model_conf.flavors.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c94f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Register ONNX models in MLflow\n",
    "\n",
    "import os\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://129.114.27.112:8000\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"admin\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"password\"\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import mlflow.onnx\n",
    "import onnx\n",
    "\n",
    "# point MLflow at your tracking server (or rely on env vars you’ve already set)\n",
    "mlflow.set_experiment(\"onnx-model-registration\")\n",
    "\n",
    "def make_input_example(model_cls, model_dir_or_name, feature):\n",
    "    # load HF model & tokenizer to construct dummy inputs\n",
    "    if feature == \"default\":\n",
    "        model = model_cls.from_pretrained(model_dir_or_name)\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_dir_or_name)\n",
    "    else:  # \"seq2seq-lm\"\n",
    "        model = model_cls.from_pretrained(model_dir_or_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir_or_name)\n",
    "    model.eval()\n",
    "    _, config_cls = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "    onnx_config = config_cls(model.config)\n",
    "    dummy = onnx_config.generate_dummy_inputs(tokenizer, framework=\"pt\")\n",
    "    # convert to numpy for MLflow\n",
    "    return {k: v.cpu().numpy() for k, v in dummy.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09662bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run distilbert-base-onnx at: http://129.114.27.112:8000/#/experiments/8/runs/20c4fab9168d471995e1f53b4ee2f77a\n",
      "🧪 View experiment at: http://129.114.27.112:8000/#/experiments/8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      8\u001b[39m example_inputs = make_input_example(\n\u001b[32m      9\u001b[39m     DistilBertModel,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/home/pb/projects/course/sem2/mlops/project/mlops/models/artifacts/model/model.sentence_transformer\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run(run_name=\u001b[33m\"\u001b[39m\u001b[33mdistilbert-base-onnx\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43monnx_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdistilbert-embedding-onnx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# already a dict of NumPy arrays\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Registered base DistilBERT ONNX\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/mlflow/onnx/__init__.py:521\u001b[39m, in \u001b[36mlog_model\u001b[39m\u001b[34m(onnx_model, artifact_path, conda_env, code_paths, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, onnx_execution_providers, onnx_session_options, metadata, save_as_external_data)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;129m@format_docstring\u001b[39m(LOG_MODEL_PARAM_DOCS.format(package_name=FLAVOR_NAME))\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_model\u001b[39m(\n\u001b[32m    450\u001b[39m     onnx_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    463\u001b[39m     save_as_external_data=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    464\u001b[39m ):\n\u001b[32m    465\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[33;03m    Log an ONNX model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[32m    467\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m \u001b[33;03m        metadata of the logged model.\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43monnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconda_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconda_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m        \u001b[49m\u001b[43monnx_execution_providers\u001b[49m\u001b[43m=\u001b[49m\u001b[43monnx_execution_providers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m        \u001b[49m\u001b[43monnx_session_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43monnx_session_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_as_external_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_as_external_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/mlflow/models/model.py:921\u001b[39m, in \u001b[36mModel.log\u001b[39m\u001b[34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, **kwargs)\u001b[39m\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m    919\u001b[39m         client.log_prompt(run_id, prompt)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtracking\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfluent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlflow_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# if the model_config kwarg is passed in, then log the model config as an params\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_config := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mmodel_config\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/mlflow/tracking/fluent.py:1219\u001b[39m, in \u001b[36mlog_artifacts\u001b[39m\u001b[34m(local_dir, artifact_path, run_id)\u001b[39m\n\u001b[32m   1185\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1186\u001b[39m \u001b[33;03mLog all the contents of a local directory as artifacts of the run. If no run is active,\u001b[39;00m\n\u001b[32m   1187\u001b[39m \u001b[33;03mthis method will create a new active run.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1216\u001b[39m \u001b[33;03m            mlflow.log_artifacts(tmp_dir, artifact_path=\"states\")\u001b[39;00m\n\u001b[32m   1217\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1218\u001b[39m run_id = run_id \u001b[38;5;129;01mor\u001b[39;00m _get_or_start_run().info.run_id\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/mlflow/tracking/client.py:2428\u001b[39m, in \u001b[36mMlflowClient.log_artifacts\u001b[39m\u001b[34m(self, run_id, local_dir, artifact_path)\u001b[39m\n\u001b[32m   2381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_artifacts\u001b[39m(\n\u001b[32m   2382\u001b[39m     \u001b[38;5;28mself\u001b[39m, run_id: \u001b[38;5;28mstr\u001b[39m, local_dir: \u001b[38;5;28mstr\u001b[39m, artifact_path: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Write a directory of files to the remote ``artifact_uri``.\u001b[39;00m\n\u001b[32m   2385\u001b[39m \n\u001b[32m   2386\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2426\u001b[39m \n\u001b[32m   2427\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2428\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tracking_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/client.py:964\u001b[39m, in \u001b[36mTrackingServiceClient.log_artifacts\u001b[39m\u001b[34m(self, run_id, local_dir, artifact_path)\u001b[39m\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_artifacts\u001b[39m(\u001b[38;5;28mself\u001b[39m, run_id, local_dir, artifact_path=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    956\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Write a directory of files to the remote ``artifact_uri``.\u001b[39;00m\n\u001b[32m    957\u001b[39m \n\u001b[32m    958\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m \n\u001b[32m    963\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_artifact_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/mlflow/store/artifact/http_artifact_repo.py:80\u001b[39m, in \u001b[36mHttpArtifactRepository.log_artifacts\u001b[39m\u001b[34m(self, local_dir, artifact_path)\u001b[39m\n\u001b[32m     76\u001b[39m     artifact_dir = (\n\u001b[32m     77\u001b[39m         posixpath.join(artifact_path, rel_path) \u001b[38;5;28;01mif\u001b[39;00m artifact_path \u001b[38;5;28;01melse\u001b[39;00m rel_path\n\u001b[32m     78\u001b[39m     )\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/mlflow/store/artifact/http_artifact_repo.py:63\u001b[39m, in \u001b[36mHttpArtifactRepository.log_artifact\u001b[39m\u001b[34m(self, local_file, artifact_path)\u001b[39m\n\u001b[32m     61\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: mime_type}\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(local_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     resp = \u001b[43mhttp_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_host_creds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPUT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     augmented_raise_for_status(resp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:184\u001b[39m, in \u001b[36mhttp_request\u001b[39m\u001b[34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, retry_timeout_seconds, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mauth\u001b[39m\u001b[33m\"\u001b[39m] = fetch_auth(host_creds.auth)\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_http_response_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackoff_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackoff_jitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraise_on_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost_creds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.Timeout \u001b[38;5;28;01mas\u001b[39;00m to:\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m    200\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed with timeout exception \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m To increase the timeout, set the environment variable \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMLFLOW_HTTP_REQUEST_TIMEOUT\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m to a larger value.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mto\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/mlflow/utils/request_utils.py:237\u001b[39m, in \u001b[36m_get_http_response_with_retries\u001b[39m\u001b[34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[39m\n\u001b[32m    234\u001b[39m env_value = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mMLFLOW_ALLOW_HTTP_REDIRECTS\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m).lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    235\u001b[39m allow_redirects = env_value \u001b[38;5;28;01mif\u001b[39;00m allow_redirects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m allow_redirects\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/urllib3/connectionpool.py:493\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages/urllib3/connection.py:459\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    457\u001b[39m             \u001b[38;5;28mself\u001b[39m.send(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%x\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33mb\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m % (\u001b[38;5;28mlen\u001b[39m(chunk), chunk))\n\u001b[32m    458\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[38;5;66;03m# Regardless of whether we have a body or not, if we're in\u001b[39;00m\n\u001b[32m    462\u001b[39m \u001b[38;5;66;03m# chunked mode we want to send an explicit empty chunk.\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunked:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/lib/python3.12/http/client.py:1057\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1055\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.send\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, data)\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1059\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections.abc.Iterable):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell: Register base ONNX in MLflow (fixed)\n",
    "import onnx\n",
    "\n",
    "# load the base ONNX file\n",
    "onnx_base = onnx.load(str(ONNX_DIR/\"distilbert.onnx\"))\n",
    "\n",
    "# build the input_example (already NumPy arrays)\n",
    "example_inputs = make_input_example(\n",
    "    DistilBertModel,\n",
    "    \"/home/pb/projects/course/sem2/mlops/project/mlops/models/artifacts/model/model.sentence_transformer\",\n",
    "    \"default\"\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name=\"distilbert-base-onnx\"):\n",
    "    mlflow.onnx.log_model(\n",
    "        onnx_model=onnx_base,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"distilbert-embedding-onnx\",\n",
    "        input_example=example_inputs    # already a dict of NumPy arrays\n",
    "    )\n",
    "    print(\"✅ Registered base DistilBERT ONNX\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90ffd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graph-optimized model written to /home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_opt.onnx\n"
     ]
    }
   ],
   "source": [
    "# Cell: Graph-optimize\n",
    "import onnxruntime as ort\n",
    "\n",
    "base_path = str(ONNX_DIR/\"distilbert.onnx\")\n",
    "opt_path  = str(ONNX_DIR/\"distilbert_opt.onnx\")\n",
    "\n",
    "sess_opts = ort.SessionOptions()\n",
    "sess_opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "sess_opts.optimized_model_filepath = opt_path\n",
    "\n",
    "# this will write out the optimized graph\n",
    "_ = ort.InferenceSession(base_path, sess_options=sess_opts, providers=[\"CPUExecutionProvider\"])\n",
    "print(f\"✅ Graph-optimized model written to {opt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1e819fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:05:09 WARNING Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "21:05:09 INFO Quantization parameters for tensor:\"/embeddings/LayerNorm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.0/attention/MatMul]\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.0/attention/MatMul_1]\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.0/attention/Reshape_3_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.0/sa_layer_norm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.0/ffn/activation/Mul_1_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.0/output_layer_norm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.1/attention/MatMul]\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.1/attention/MatMul_1]\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.1/attention/Reshape_3_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.1/sa_layer_norm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.1/ffn/activation/Mul_1_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.1/output_layer_norm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.2/attention/MatMul]\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.2/attention/MatMul_1]\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.2/attention/Reshape_3_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.2/sa_layer_norm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.2/ffn/activation/Mul_1_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.2/output_layer_norm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.3/attention/MatMul]\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.3/attention/MatMul_1]\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.3/attention/Reshape_3_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.3/sa_layer_norm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.3/ffn/activation/Mul_1_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.3/output_layer_norm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.4/attention/MatMul]\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.4/attention/MatMul_1]\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.4/attention/Reshape_3_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.4/sa_layer_norm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.4/ffn/activation/Mul_1_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.4/output_layer_norm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.5/attention/MatMul]\n",
      "21:05:09 INFO Ignore MatMul due to non constant B: /[/transformer/layer.5/attention/MatMul_1]\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.5/attention/Reshape_3_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.5/sa_layer_norm/LayerNormalization_output_0\" not specified\n",
      "21:05:09 INFO Quantization parameters for tensor:\"/transformer/layer.5/ffn/activation/Mul_1_output_0\" not specified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dynamic-quant ONNX written to /home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_dyn.onnx\n"
     ]
    }
   ],
   "source": [
    "# Cell: Dynamic Quantization (weight-only)\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "dyn_path = str(ONNX_DIR/\"distilbert_dyn.onnx\")\n",
    "quantize_dynamic(\n",
    "    model_input=base_path,\n",
    "    model_output=dyn_path,\n",
    "    weight_type=QuantType.QInt8\n",
    ")\n",
    "print(f\"✅ Dynamic-quant ONNX written to {dyn_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "899ed9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:12:45 WARNING Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "21:12:46 WARNING Please use QuantFormat.QDQ for activation type QInt8 and weight type QInt8. Or it will lead to bad performance on x64.\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/embeddings/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor 'attention_mask', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"batch\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"sequence\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/Unsqueeze_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"batch\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"sequence\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/Gather_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/Gather_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/Unsqueeze_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/Concat_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 4\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/ConstantOfShape_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__4\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.0/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.0/attention/Unsqueeze_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.0/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.0/attention/Unsqueeze_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.0/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.0/attention/Unsqueeze_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.0/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.0/attention/Unsqueeze_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.1/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.1/attention/Unsqueeze_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.1/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.1/attention/Unsqueeze_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.1/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.1/attention/Unsqueeze_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.1/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.1/attention/Unsqueeze_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.2/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.2/attention/Unsqueeze_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.2/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.2/attention/Unsqueeze_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.2/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:46 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.2/attention/Unsqueeze_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.2/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.2/attention/Unsqueeze_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.3/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.3/attention/Unsqueeze_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.3/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.3/attention/Unsqueeze_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.3/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.3/attention/Unsqueeze_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.3/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.3/attention/Unsqueeze_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.4/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.4/attention/Unsqueeze_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.4/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.4/attention/Unsqueeze_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.4/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.4/attention/Unsqueeze_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.4/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.4/attention/Unsqueeze_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.5/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.5/attention/Unsqueeze_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.5/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.5/attention/Unsqueeze_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.5/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.5/attention/Unsqueeze_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.5/attention/Gather_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Inference failed or unsupported type to quantize for tensor '/transformer/layer.5/attention/Unsqueeze_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "21:12:47 WARNING Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "21:12:47 WARNING Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Heavy static quant ONNX → /home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_static_heavy.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:12:50 WARNING Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Moderate static quant ONNX → /home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_static_moderate.onnx\n"
     ]
    }
   ],
   "source": [
    "# Cell: Static Quantization (heavy / moderate / low)\n",
    "from onnxruntime.quantization import (\n",
    "    quantize_static, QuantType, QuantFormat, CalibrationDataReader, CalibrationMethod\n",
    ")\n",
    "\n",
    "# Simple DataReader using the dummy inputs\n",
    "class DistilBertDataReader(CalibrationDataReader):\n",
    "    def __init__(self, tokenizer, texts):\n",
    "        enc = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"np\")\n",
    "        self.inputs = [\n",
    "            {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"]}\n",
    "        ]\n",
    "    def get_next(self):\n",
    "        return self.inputs.pop(0) if self.inputs else None\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"/home/pb/projects/course/sem2/mlops/project/mlops/models/artifacts/model/model.sentence_transformer\")\n",
    "# use a handful of samples for calibration\n",
    "cal_texts = [\"This is a test.\", \"Another example.\", \"One more sentence.\"] * 5\n",
    "dr = DistilBertDataReader(tokenizer, cal_texts)\n",
    "\n",
    "# 1) Heavy static: QOperator + int8 activations + int8 weights\n",
    "heavy_path = str(ONNX_DIR/\"distilbert_static_heavy.onnx\")\n",
    "quantize_static(\n",
    "    model_input=base_path,\n",
    "    model_output=heavy_path,\n",
    "    calibration_data_reader=dr,\n",
    "    quant_format=QuantFormat.QOperator,\n",
    "    activation_type=QuantType.QInt8,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    calibrate_method=CalibrationMethod.MinMax\n",
    ")\n",
    "print(f\"✅ Heavy static quant ONNX → {heavy_path}\")\n",
    "\n",
    "# 2) Moderate static: QDQ format + int8 activations + int8 weights\n",
    "dr = DistilBertDataReader(tokenizer, cal_texts)\n",
    "moderate_path = str(ONNX_DIR/\"distilbert_static_moderate.onnx\")\n",
    "quantize_static(\n",
    "    model_input=base_path,\n",
    "    model_output=moderate_path,\n",
    "    calibration_data_reader=dr,\n",
    "    quant_format=QuantFormat.QDQ,\n",
    "    activation_type=QuantType.QInt8,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    calibrate_method=CalibrationMethod.MinMax\n",
    ")\n",
    "print(f\"✅ Moderate static quant ONNX → {moderate_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf42babe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'distilbert-embedding-onnx-graph-opt'.\n",
      "2025/05/11 21:17:25 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: distilbert-embedding-onnx-graph-opt, version 1\n",
      "Created version '1' of model 'distilbert-embedding-onnx-graph-opt'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Registered distilbert-graph-opt ONNX with input_example\n",
      "🏃 View run distilbert-graph-opt-registration at: http://129.114.27.112:8000/#/experiments/8/runs/f88c3d52746f45e8b746cbb9b43bc817\n",
      "🧪 View experiment at: http://129.114.27.112:8000/#/experiments/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'distilbert-embedding-onnx-dynamic-quant'.\n",
      "2025/05/11 21:18:04 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: distilbert-embedding-onnx-dynamic-quant, version 1\n",
      "Created version '1' of model 'distilbert-embedding-onnx-dynamic-quant'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Registered distilbert-dynamic-quant ONNX with input_example\n",
      "🏃 View run distilbert-dynamic-quant-registration at: http://129.114.27.112:8000/#/experiments/8/runs/d045a3cae5c74830af7526933adf5243\n",
      "🧪 View experiment at: http://129.114.27.112:8000/#/experiments/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'distilbert-embedding-onnx-static-heavy'.\n",
      "2025/05/11 21:18:43 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: distilbert-embedding-onnx-static-heavy, version 1\n",
      "Created version '1' of model 'distilbert-embedding-onnx-static-heavy'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Registered distilbert-static-heavy ONNX with input_example\n",
      "🏃 View run distilbert-static-heavy-registration at: http://129.114.27.112:8000/#/experiments/8/runs/c8f4f8f01d464e64a0756b3c445b3c23\n",
      "🧪 View experiment at: http://129.114.27.112:8000/#/experiments/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'distilbert-embedding-onnx-static-moderate'.\n",
      "2025/05/11 21:19:25 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: distilbert-embedding-onnx-static-moderate, version 1\n",
      "Created version '1' of model 'distilbert-embedding-onnx-static-moderate'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Registered distilbert-static-moderate ONNX with input_example\n",
      "🏃 View run distilbert-static-moderate-registration at: http://129.114.27.112:8000/#/experiments/8/runs/31936eea118a4cc2990a2ba55a5bca72\n",
      "🧪 View experiment at: http://129.114.27.112:8000/#/experiments/8\n"
     ]
    }
   ],
   "source": [
    "# Cell: Register all optimized models in MLflow (with input_example)\n",
    "\n",
    "import onnx\n",
    "\n",
    "# Build one input_example up front (NumPy arrays) for the DistilBERT encoder\n",
    "EMBED_MODEL_DIR = \"/home/pb/projects/course/sem2/mlops/project/mlops/models/artifacts/model/model.sentence_transformer\"\n",
    "embed_input_example = make_input_example(\n",
    "    DistilBertModel,\n",
    "    EMBED_MODEL_DIR,\n",
    "    \"default\"\n",
    ")\n",
    "\n",
    "# Now register each variant\n",
    "for tag, path in [\n",
    "    (\"graph-opt\",      opt_path),\n",
    "    (\"dynamic-quant\",  dyn_path),\n",
    "    (\"static-heavy\",   heavy_path),\n",
    "    (\"static-moderate\",moderate_path)\n",
    "]:\n",
    "    onnx_m = onnx.load(path)\n",
    "    with mlflow.start_run(run_name=f\"distilbert-{tag}-registration\"):\n",
    "        mlflow.onnx.log_model(\n",
    "            onnx_model=onnx_m,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=f\"distilbert-embedding-onnx-{tag}\",\n",
    "            input_example=embed_input_example\n",
    "        )\n",
    "        print(f\"✅ Registered distilbert-{tag} ONNX with input_example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) BART summarization ONNX\n",
    "bart_input = make_input_example(\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    \"facebook/bart-large\",\n",
    "    \"seq2seq-lm\"\n",
    ")\n",
    "bart_onnx = onnx.load(\"/home/pb/projects/course/sem2/mlops/project/mlops/models/bart_summarize.onnx\")\n",
    "with mlflow.start_run(run_name=\"bart-summarize-onnx-registration\"):\n",
    "    mlflow.onnx.log_model(\n",
    "        onnx_model=bart_onnx,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"bart-summarize-onnx\",\n",
    "        input_example=bart_input\n",
    "    )\n",
    "    print(\"✅ Registered BART summarization ONNX with input_example\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
