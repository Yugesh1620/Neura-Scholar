{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602fa3b8",
   "metadata": {},
   "source": [
    "# Embedding Model & System-Level Benchmarks\n",
    "\n",
    "This notebook covers:\n",
    "1. Loading the bi-encoder from MLflow  \n",
    "2. PyTorch vs. ONNX vs. quantized inference  \n",
    "3. FAISS end-to-end search latency  \n",
    "4. Triton inference server benchmarking via `perf_analyzer`  \n",
    "5. HTTP-level concurrency tests  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5878d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFLOW_TRACKING_USERNAME=admin\n",
      "env: MLFLOW_TRACKING_PASSWORD=password\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install dependencies\n",
    "# !pip install -q mlflow sentence-transformers torch faiss-cpu onnx onnxruntime onnxruntime-gpu onnxruntime-tools datasets\n",
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "\n",
    "\n",
    "%env MLFLOW_TRACKING_USERNAME=admin\n",
    "%env MLFLOW_TRACKING_PASSWORD=password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be76157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports & Settings\n",
    "\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import requests\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Sample data for FAISS\n",
    "CORPUS = [\"This is document A\", \"Another paper B\", \"Yet another doc C\"]\n",
    "QUERIES = [\"role of mitochondria\", \"deep learning semantics\"]\n",
    "\n",
    "# Local paths\n",
    "LOCAL_MODEL_DIR = \"/home/pb/projects/course/sem2/mlops/project/mlops/models/artifacts/model/model.sentence_transformer\"\n",
    "ONNX_DIR = \"onnx_models\"\n",
    "os.makedirs(ONNX_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5096b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Utilities\n",
    "\n",
    "def time_function(fn, *args, **kwargs):\n",
    "    \"\"\"Return (result, elapsed_seconds).\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    out = fn(*args, **kwargs)\n",
    "    return out, (time.perf_counter() - start)\n",
    "\n",
    "def build_faiss_index(embs: np.ndarray):\n",
    "    dim = embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    faiss.normalize_L2(embs)\n",
    "    index.add(embs)\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961ba80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 4: Load and prepare the embedding model\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# MLFLOW_MODEL_NAME = \"arxiv-bi-encoder-longformer\"\n",
    "# MLFLOW_MODEL_VERSION = \"1\"\n",
    "\n",
    "# _client = MlflowClient()\n",
    "\n",
    "# def load_embedding_model_from_registry(name: str, version: str) -> SentenceTransformer:\n",
    "#     \"\"\"\n",
    "#     Download only the `model/` subdirectory for the given registered model version,\n",
    "#     then load it with SentenceTransformer.\n",
    "#     \"\"\"\n",
    "#     # 1) Fetch the model version metadata\n",
    "#     mv = _client.get_model_version(name, version)\n",
    "#     run_id = mv.run_id\n",
    "\n",
    "#     # 2) Download only the 'model' artifact dir (not checkpoints)\n",
    "#     local_model_dir = _client.download_artifacts(run_id, \"model\")\n",
    "\n",
    "#     # 3) Load with SentenceTransformer\n",
    "#     return SentenceTransformer(local_model_dir)\n",
    "\n",
    "# # Usage:\n",
    "# embed_model = load_embedding_model_from_registry(MLFLOW_MODEL_NAME, MLFLOW_MODEL_VERSION)\n",
    "# print(\"Loaded embedding model from:\", embed_model._first_module().save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced4771e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading SentenceTransformer modules from: /home/pb/projects/course/sem2/mlops/project/mlops/models/artifacts/model/model.sentence_transformer\n",
      "✅ Loaded model onto device: cuda\n",
      "Embedding vector shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Local SentenceTransformer Model\n",
    "\n",
    "print(f\"▶ Loading SentenceTransformer modules from: {LOCAL_MODEL_DIR}\")\n",
    "\n",
    "# Transformer encoder module (offline)\n",
    "transformer_module = models.Transformer(\n",
    "    LOCAL_MODEL_DIR,\n",
    "    max_seq_length=512\n",
    ")\n",
    "# Pooling head\n",
    "pooling_module = models.Pooling(\n",
    "    transformer_module.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "\n",
    "# Assemble & move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embed_model = SentenceTransformer(\n",
    "    modules=[transformer_module, pooling_module],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"✅ Loaded model onto device: {device}\")\n",
    "# Sanity check\n",
    "vec = embed_model.encode(\"Test embedding\", convert_to_numpy=True)\n",
    "print(\"Embedding vector shape:\", vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5044e648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch single-sample latency: 11.77 ms\n",
      "PyTorch batch=1 throughput: 162.7 QPS, latency p50≈6.15 ms\n",
      "PyTorch batch=8 throughput: 255.0 QPS, latency p50≈3.92 ms\n",
      "PyTorch batch=16 throughput: 1365.3 QPS, latency p50≈0.73 ms\n",
      "PyTorch batch=32 throughput: 2209.2 QPS, latency p50≈0.45 ms\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Baseline PyTorch inference (single & batch)\n",
    "# Single sample\n",
    "_, dt = time_function(embed_model.encode, QUERIES[0], convert_to_numpy=True)\n",
    "print(f\"PyTorch single-sample latency: {dt*1000:.2f} ms\")\n",
    "\n",
    "# Batches\n",
    "for B in [1,8,16,32]:\n",
    "    batch = [QUERIES[0]]*B\n",
    "    _, dt = time_function(embed_model.encode, batch, convert_to_numpy=True)\n",
    "    print(f\"PyTorch batch={B} throughput: {B/dt:.1f} QPS, latency p50≈{(dt/B)*1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e6a2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115832/42798711.py:16: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.8, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported ONNX model to: onnx_models/embed.onnx\n",
      "ONNXRuntime (CPU) latency: 269.88 ms\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: ONNX Export & Unoptimized ONNXRuntime Inference\n",
    "\n",
    "# 1) Export the encoder to ONNX\n",
    "encoder = embed_model._first_module().auto_model.eval().cpu()\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR, local_files_only=True)\n",
    "\n",
    "# Prepare dummy input\n",
    "sample = tokenizer(\n",
    "    \"test\", return_tensors=\"pt\",\n",
    "    max_length=300, padding=\"max_length\", truncation=True\n",
    ")\n",
    "torch_inputs = (sample[\"input_ids\"], sample[\"attention_mask\"])\n",
    "\n",
    "onnx_path = os.path.join(ONNX_DIR, \"embed.onnx\")\n",
    "import torch\n",
    "torch.onnx.export(\n",
    "    encoder,\n",
    "    torch_inputs,\n",
    "    onnx_path,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"last_hidden_state\"],\n",
    "    dynamic_axes={\"input_ids\": {0: \"batch_size\"}, \"attention_mask\": {0: \"batch_size\"}},\n",
    "    opset_version=17\n",
    ")\n",
    "print(f\"✅ Exported ONNX model to: {onnx_path}\")\n",
    "\n",
    "# 2) Benchmark unoptimized ONNXRuntime\n",
    "ort_sess = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def ort_encode(texts):\n",
    "    toks = tokenizer(\n",
    "        texts, return_tensors=\"np\",\n",
    "        max_length=300, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    last_hidden = ort_sess.run(\n",
    "        [\"last_hidden_state\"],\n",
    "        {\"input_ids\": toks[\"input_ids\"], \"attention_mask\": toks[\"attention_mask\"]}\n",
    "    )[0]\n",
    "    mask = np.expand_dims(toks[\"attention_mask\"], -1)\n",
    "    embeddings = (last_hidden * mask).sum(1) / np.clip(mask.sum(1), 1e-9, None)\n",
    "    return embeddings\n",
    "\n",
    "_, dt = time_function(ort_encode, [QUERIES[0]])\n",
    "print(f\"ONNXRuntime (CPU) latency: {dt*1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23546e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graph‐optimized ONNX written to onnx_models/embed_opt.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dynamic‐quant ONNX written to onnx_models/embed_dyn.onnx\n",
      "ONNX     | latency  160.80 ms | size 265.5 MB\n",
      "GraphOpt | latency  261.08 ms | size 265.5 MB\n",
      "DynQuant | latency  101.53 ms | size  66.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Graph Optimization & Dynamic Quantization Benchmarks\n",
    "\n",
    "import os\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Define paths\n",
    "ONNX_PATH       = os.path.join(ONNX_DIR, \"embed.onnx\")\n",
    "OPT_ONNX_PATH   = os.path.join(ONNX_DIR, \"embed_opt.onnx\")\n",
    "DYN_QUANT_PATH  = os.path.join(ONNX_DIR, \"embed_dyn.onnx\")\n",
    "\n",
    "# 1) Graph‐optimized ONNX\n",
    "opt_so = ort.SessionOptions()\n",
    "opt_so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "# tell ORT to write out the optimized graph\n",
    "opt_so.optimized_model_filepath = OPT_ONNX_PATH\n",
    "_ = ort.InferenceSession(ONNX_PATH, sess_options=opt_so, providers=[\"CPUExecutionProvider\"])\n",
    "print(f\"✅ Graph‐optimized ONNX written to {OPT_ONNX_PATH}\")\n",
    "\n",
    "# 2) Dynamic quantization\n",
    "quantize_dynamic(\n",
    "    model_input=ONNX_PATH,\n",
    "    model_output=DYN_QUANT_PATH,\n",
    "    weight_type=QuantType.QInt8\n",
    ")\n",
    "print(f\"✅ Dynamic‐quant ONNX written to {DYN_QUANT_PATH}\")\n",
    "\n",
    "# 3) Benchmark helper\n",
    "def bench_onnx(path, label):\n",
    "    sess = ort.InferenceSession(path, providers=[\"CPUExecutionProvider\"])\n",
    "    # reuse `sample` from Cell 6: a dict with numpy input_ids & attention_mask\n",
    "    inputs = {\n",
    "        \"input_ids\":    sample[\"input_ids\"].numpy(),\n",
    "        \"attention_mask\": sample[\"attention_mask\"].numpy()\n",
    "    }\n",
    "    _, dt = time_function(lambda: sess.run([\"last_hidden_state\"], inputs))\n",
    "    size_mb = os.path.getsize(path) / 1e6\n",
    "    print(f\"{label:8s} | latency {dt*1000:7.2f} ms | size {size_mb:5.1f} MB\")\n",
    "\n",
    "# 4) Run benchmarks\n",
    "bench_onnx(ONNX_PATH,      \"ONNX\")\n",
    "bench_onnx(OPT_ONNX_PATH,  \"GraphOpt\")\n",
    "bench_onnx(DYN_QUANT_PATH, \"DynQuant\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abd65263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUExecutionProvider      | latency   59.99 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-11 19:21:21.002371316 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 90 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-11 19:21:21.004418169 [W:onnxruntime:, session_state.cc:1263 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-11 19:21:21.004428495 [W:onnxruntime:, session_state.cc:1265 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDAExecutionProvider     | latency  409.42 ms\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: ExecutionProvider Comparison on Dynamic‐Quantized ONNX Model\n",
    "\n",
    "# List of providers to test\n",
    "PROVIDERS = [\n",
    "    \"CPUExecutionProvider\",\n",
    "    \"CUDAExecutionProvider\",\n",
    "    # \"TensorrtExecutionProvider\",\n",
    "    # \"OpenVINOExecutionProvider\"\n",
    "]\n",
    "\n",
    "# Path to the dynamic-quantized ONNX model from Cell 7\n",
    "DYN_QUANT_PATH = os.path.join(ONNX_DIR, \"embed_dyn.onnx\")\n",
    "\n",
    "# Benchmark on each available provider\n",
    "for prov in PROVIDERS:\n",
    "    try:\n",
    "        sess = ort.InferenceSession(DYN_QUANT_PATH, providers=[prov])\n",
    "    except Exception as e:\n",
    "        print(f\"{prov:25s} not available: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Reuse the same sample inputs from Cell 6\n",
    "    inputs = {\n",
    "        \"input_ids\":    sample[\"input_ids\"].numpy(),\n",
    "        \"attention_mask\": sample[\"attention_mask\"].numpy()\n",
    "    }\n",
    "\n",
    "    _, dt = time_function(lambda: sess.run([\"last_hidden_state\"], inputs))\n",
    "    print(f\"{prov:25s} | latency {dt*1000:7.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4972868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS search + encode latency:  2.4162280024029315 ms\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: End-to-end FAISS search latency\n",
    "# build index\n",
    "embs = embed_model.encode(CORPUS, convert_to_numpy=True)\n",
    "index = build_faiss_index(embs)\n",
    "# query\n",
    "qe = embed_model.encode([QUERIES[0]], convert_to_numpy=True)\n",
    "start = time.perf_counter()\n",
    "D,I = index.search(qe, k=3)\n",
    "print(\"FAISS search + encode latency: \", (time.perf_counter()-start)*1000, \"ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34183e",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Benchmarking Cells\n",
    "\n",
    "The following cells are dedicated to benchmarking the performance of the endpoint its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6d07a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q aiohttp nest_asyncio tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a00107ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, json, statistics, time\n",
    "from pathlib import Path\n",
    "\n",
    "import aiohttp, nest_asyncio, numpy as np\n",
    "from tqdm.notebook import tqdm  # nice progress bars in Jupyter\n",
    "\n",
    "nest_asyncio.apply()            # allows nested event‑loops in notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82b1d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "async def bench_endpoint(url: str,\n",
    "                         requests: list[dict],\n",
    "                         concurrency: int = 1,\n",
    "                         desc: str = \"\") -> dict:\n",
    "    q = asyncio.Queue()\n",
    "    for r in requests:            # push all payloads\n",
    "        q.put_nowait(r)\n",
    "\n",
    "    latencies = []\n",
    "\n",
    "    async def _worker(queue, endpoint, times):\n",
    "        while not queue.empty():\n",
    "            payload = await queue.get()\n",
    "            t0 = time.perf_counter()\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                await client.post(endpoint, json=payload, timeout=60)\n",
    "            times.append(time.perf_counter() - t0)\n",
    "\n",
    "    # launch workers\n",
    "    tasks = [asyncio.create_task(_worker(q, url, latencies))\n",
    "             for _ in range(concurrency)]\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    await asyncio.gather(*tasks)         # <‑‑ await only the gather\n",
    "    total = time.perf_counter() - start\n",
    "\n",
    "    return {\n",
    "        \"concurrency\": concurrency,\n",
    "        \"requests\"   : len(requests),\n",
    "        \"total_s\"    : total,\n",
    "        \"rps\"        : len(requests)/total,\n",
    "        \"p50_ms\"     : 1e3*np.percentile(latencies, 50),\n",
    "        \"p95_ms\"     : 1e3*np.percentile(latencies, 95),\n",
    "        \"p99_ms\"     : 1e3*np.percentile(latencies, 99),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1a1b456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'texts': ['Uncertainty estimation methods in Bayesian deep learning']}, {'texts': ['Efficient transformer architectures for long sequence modeling']}, {'texts': ['Graph neural networks for molecular property prediction']}]\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 3 – prepare test inputs  (no JSONL file needed) ────────────────\n",
    "import random\n",
    "\n",
    "# Three representative research‑style queries\n",
    "sample_queries = [\n",
    "    \"Graph neural networks for molecular property prediction\",\n",
    "    \"Uncertainty estimation methods in Bayesian deep learning\",\n",
    "    \"Efficient transformer architectures for long sequence modeling\",\n",
    "]\n",
    "\n",
    "# Build FastAPI payloads → one request per query\n",
    "payloads = [{\"texts\": [q]} for q in sample_queries]\n",
    "\n",
    "# Optional: simulate perf_analyzer’s `-b` (batch‑size) flag\n",
    "BATCH = 1                 # set >1 if you want to duplicate texts per request\n",
    "if BATCH > 1:\n",
    "    payloads = [{\"texts\": p[\"texts\"] * BATCH} for p in payloads]\n",
    "\n",
    "# (Optional) shuffle to avoid ordering bias in repeated runs\n",
    "random.shuffle(payloads)\n",
    "\n",
    "print(payloads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25c1116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    concurrency  requests   total_s        rps      p50_ms      p95_ms  \\\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.454544</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.600019</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">85.888724</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">279.081097</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.194071</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.458265</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">104.859693</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">141.020897</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.237550</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.628933</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">209.654695</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">234.700538</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.091330</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.847765</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">76.331776</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89.772328</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.103939</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28.863168</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89.676191</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102.454816</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.078831</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38.056230</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">63.433441</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77.043982</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.196145</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.294808</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">181.779774</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">194.646568</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.131361</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.837785</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">105.952042</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128.686387</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.080976</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37.048002</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65.468242</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">79.361906</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.085837</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34.949879</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">71.486283</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84.167876</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.083624</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35.875043</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67.722227</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81.963684</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.080191</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37.410513</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65.053816</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78.391417</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.100139</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29.958397</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">85.230977</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98.396840</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.083580</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35.893850</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67.508319</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81.707830</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.079817</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37.586134</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64.548133</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78.124331</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.096002</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.249466</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80.676784</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">94.393140</span>   \n",
       "\n",
       "        p99_ms  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">296.253752</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">144.235227</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">236.926835</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">90.967044</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">103.590694</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78.253808</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">195.790283</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">130.707217</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80.596898</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">85.295129</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83.229591</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">79.576982</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.567139</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">82.970008</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">79.331104</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">95.612372</span>  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "    concurrency  requests   total_s        rps      p50_ms      p95_ms  \\\n",
       "\u001b[1;36m0\u001b[0m             \u001b[1;36m1\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.454544\u001b[0m   \u001b[1;36m6.600019\u001b[0m   \u001b[1;36m85.888724\u001b[0m  \u001b[1;36m279.081097\u001b[0m   \n",
       "\u001b[1;36m1\u001b[0m             \u001b[1;36m2\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.194071\u001b[0m  \u001b[1;36m15.458265\u001b[0m  \u001b[1;36m104.859693\u001b[0m  \u001b[1;36m141.020897\u001b[0m   \n",
       "\u001b[1;36m2\u001b[0m             \u001b[1;36m3\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.237550\u001b[0m  \u001b[1;36m12.628933\u001b[0m  \u001b[1;36m209.654695\u001b[0m  \u001b[1;36m234.700538\u001b[0m   \n",
       "\u001b[1;36m3\u001b[0m             \u001b[1;36m4\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.091330\u001b[0m  \u001b[1;36m32.847765\u001b[0m   \u001b[1;36m76.331776\u001b[0m   \u001b[1;36m89.772328\u001b[0m   \n",
       "\u001b[1;36m4\u001b[0m             \u001b[1;36m5\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.103939\u001b[0m  \u001b[1;36m28.863168\u001b[0m   \u001b[1;36m89.676191\u001b[0m  \u001b[1;36m102.454816\u001b[0m   \n",
       "\u001b[1;36m5\u001b[0m             \u001b[1;36m6\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.078831\u001b[0m  \u001b[1;36m38.056230\u001b[0m   \u001b[1;36m63.433441\u001b[0m   \u001b[1;36m77.043982\u001b[0m   \n",
       "\u001b[1;36m6\u001b[0m             \u001b[1;36m7\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.196145\u001b[0m  \u001b[1;36m15.294808\u001b[0m  \u001b[1;36m181.779774\u001b[0m  \u001b[1;36m194.646568\u001b[0m   \n",
       "\u001b[1;36m7\u001b[0m             \u001b[1;36m8\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.131361\u001b[0m  \u001b[1;36m22.837785\u001b[0m  \u001b[1;36m105.952042\u001b[0m  \u001b[1;36m128.686387\u001b[0m   \n",
       "\u001b[1;36m8\u001b[0m             \u001b[1;36m9\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.080976\u001b[0m  \u001b[1;36m37.048002\u001b[0m   \u001b[1;36m65.468242\u001b[0m   \u001b[1;36m79.361906\u001b[0m   \n",
       "\u001b[1;36m9\u001b[0m            \u001b[1;36m10\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.085837\u001b[0m  \u001b[1;36m34.949879\u001b[0m   \u001b[1;36m71.486283\u001b[0m   \u001b[1;36m84.167876\u001b[0m   \n",
       "\u001b[1;36m10\u001b[0m           \u001b[1;36m11\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.083624\u001b[0m  \u001b[1;36m35.875043\u001b[0m   \u001b[1;36m67.722227\u001b[0m   \u001b[1;36m81.963684\u001b[0m   \n",
       "\u001b[1;36m11\u001b[0m           \u001b[1;36m12\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.080191\u001b[0m  \u001b[1;36m37.410513\u001b[0m   \u001b[1;36m65.053816\u001b[0m   \u001b[1;36m78.391417\u001b[0m   \n",
       "\u001b[1;36m12\u001b[0m           \u001b[1;36m13\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.100139\u001b[0m  \u001b[1;36m29.958397\u001b[0m   \u001b[1;36m85.230977\u001b[0m   \u001b[1;36m98.396840\u001b[0m   \n",
       "\u001b[1;36m13\u001b[0m           \u001b[1;36m14\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.083580\u001b[0m  \u001b[1;36m35.893850\u001b[0m   \u001b[1;36m67.508319\u001b[0m   \u001b[1;36m81.707830\u001b[0m   \n",
       "\u001b[1;36m14\u001b[0m           \u001b[1;36m15\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.079817\u001b[0m  \u001b[1;36m37.586134\u001b[0m   \u001b[1;36m64.548133\u001b[0m   \u001b[1;36m78.124331\u001b[0m   \n",
       "\u001b[1;36m15\u001b[0m           \u001b[1;36m16\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[1;36m0.096002\u001b[0m  \u001b[1;36m31.249466\u001b[0m   \u001b[1;36m80.676784\u001b[0m   \u001b[1;36m94.393140\u001b[0m   \n",
       "\n",
       "        p99_ms  \n",
       "\u001b[1;36m0\u001b[0m   \u001b[1;36m296.253752\u001b[0m  \n",
       "\u001b[1;36m1\u001b[0m   \u001b[1;36m144.235227\u001b[0m  \n",
       "\u001b[1;36m2\u001b[0m   \u001b[1;36m236.926835\u001b[0m  \n",
       "\u001b[1;36m3\u001b[0m    \u001b[1;36m90.967044\u001b[0m  \n",
       "\u001b[1;36m4\u001b[0m   \u001b[1;36m103.590694\u001b[0m  \n",
       "\u001b[1;36m5\u001b[0m    \u001b[1;36m78.253808\u001b[0m  \n",
       "\u001b[1;36m6\u001b[0m   \u001b[1;36m195.790283\u001b[0m  \n",
       "\u001b[1;36m7\u001b[0m   \u001b[1;36m130.707217\u001b[0m  \n",
       "\u001b[1;36m8\u001b[0m    \u001b[1;36m80.596898\u001b[0m  \n",
       "\u001b[1;36m9\u001b[0m    \u001b[1;36m85.295129\u001b[0m  \n",
       "\u001b[1;36m10\u001b[0m   \u001b[1;36m83.229591\u001b[0m  \n",
       "\u001b[1;36m11\u001b[0m   \u001b[1;36m79.576982\u001b[0m  \n",
       "\u001b[1;36m12\u001b[0m   \u001b[1;36m99.567139\u001b[0m  \n",
       "\u001b[1;36m13\u001b[0m   \u001b[1;36m82.970008\u001b[0m  \n",
       "\u001b[1;36m14\u001b[0m   \u001b[1;36m79.331104\u001b[0m  \n",
       "\u001b[1;36m15\u001b[0m   \u001b[1;36m95.612372\u001b[0m  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"http://localhost:8000/embed\"   # FastAPI endpoint\n",
    "results = []\n",
    "\n",
    "for c in range(1, 17):                # 1 .. 16\n",
    "    res = await bench_endpoint(url, payloads, concurrency=c,\n",
    "                               desc=f\"conc={c}\")\n",
    "    results.append(res)\n",
    "\n",
    "import pandas as pd, rich\n",
    "df = pd.DataFrame(results)\n",
    "rich.print(df)        # pretty table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
