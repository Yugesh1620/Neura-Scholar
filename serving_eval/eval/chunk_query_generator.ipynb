{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc2051c",
   "metadata": {},
   "source": [
    "# Arxiv Eval Data Generation Notebook\n",
    "\n",
    "This notebook builds a BM25 index over chunks, filters ground truth citations, generates semantic-search queries via Ollama, and updates both a Parquet checkpoint and a Postgres table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3493d579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: fastparquet in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (2024.11.0)\n",
      "Requirement already satisfied: sqlalchemy in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (2.0.40)\n",
      "Requirement already satisfied: whoosh in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (2.7.4)\n",
      "Requirement already satisfied: psycopg2-binary in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (2.9.10)\n",
      "Requirement already satisfied: lmstudio in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: cramjam>=2.3 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from fastparquet) (2.10.0)\n",
      "Requirement already satisfied: fsspec in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from fastparquet) (2024.6.1)\n",
      "Requirement already satisfied: packaging in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: greenlet>=1 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from sqlalchemy) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from sqlalchemy) (4.12.2)\n",
      "Requirement already satisfied: httpx>=0.27.2 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from lmstudio) (0.28.1)\n",
      "Requirement already satisfied: httpx-ws>=0.7.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from lmstudio) (0.7.2)\n",
      "Requirement already satisfied: msgspec>=0.18.6 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from lmstudio) (0.19.0)\n",
      "Requirement already satisfied: anyio>=4.8.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from lmstudio) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from anyio>=4.8.0->lmstudio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from anyio>=4.8.0->lmstudio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from httpx>=0.27.2->lmstudio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from httpx>=0.27.2->lmstudio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.2->lmstudio) (0.16.0)\n",
      "Requirement already satisfied: wsproto in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from httpx-ws>=0.7.0->lmstudio) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas fastparquet sqlalchemy whoosh psycopg2-binary lmstudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a46e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from whoosh.index import create_in, open_dir, exists_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser\n",
    "from ollama import chat, ChatResponse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47474b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DB_URL          = \"postgresql://rg5073:rg5073pass@:port/cleaned_meta_data_db\"  # <-- update to your Postgres connection\n",
    "PARQUET_FILE    = \"arxiv_eval_results.parquet\"\n",
    "INDEX_DIR       = \"whoosh_index\"\n",
    "TOP_N           = 3       # BM25 top-N chunks to consider\n",
    "MIN_SCORE       = None    # e.g. 1.0 to enforce a minimum BM25 score\n",
    "OLLAMA_MODEL    = \"phi4\"   # Ollama model name\n",
    "NUM_QUERIES     = 3       # Number of queries to generate per chunk\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(\n",
    "    'postgresql+psycopg2://local:password@localhost:5433/mlops_local',\n",
    "    max_overflow=0,   # disallow “extra” connections beyond pool_size\n",
    "    pool_timeout=30,  # seconds to wait for an idle connection\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e21fa2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 03:40:58 [INFO] Opening existing index in whoosh_index\n",
      "2025-05-11 03:40:58 [INFO] Whoosh index ready at: whoosh_index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from sqlalchemy import create_engine, text\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import Schema, ID, TEXT\n",
    "\n",
    "# ── Configuration ─────────────────────────────────────────────\n",
    "BATCH_SIZE    = 5000      # rows per SQL fetch\n",
    "NUM_PROCS     = 4         # parallel Whoosh writer processes\n",
    "MEM_LIMIT_MB  = 512       # memory cap per process (MB)\n",
    "\n",
    "# ── Logging Setup ──────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def build_whoosh_index():\n",
    "    # If index dir missing or empty, (re)create & populate it\n",
    "    new_index = not (os.path.isdir(INDEX_DIR) and os.listdir(INDEX_DIR))\n",
    "    if new_index:\n",
    "        os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "        logger.info(\"Creating new Whoosh index in %s\", INDEX_DIR)\n",
    "\n",
    "        # Define schema\n",
    "        schema = Schema(\n",
    "            chunk_id   = ID(stored=True),\n",
    "            paper_id   = ID(stored=True),\n",
    "            chunk_data = TEXT\n",
    "        )\n",
    "\n",
    "        ix = create_in(INDEX_DIR, schema)\n",
    "        writer = ix.writer(\n",
    "            procs=NUM_PROCS,\n",
    "            limitmb=MEM_LIMIT_MB,\n",
    "            multisegment=True\n",
    "        )\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            last_chunk_id = None\n",
    "            total_indexed = 0\n",
    "            batch_num = 0\n",
    "\n",
    "            while True:\n",
    "                batch_num += 1\n",
    "                if last_chunk_id is None:\n",
    "                    sql = text(\"\"\"\n",
    "                        SELECT chunk_id, paper_id, chunk_data\n",
    "                          FROM arxiv_chunks_backup\n",
    "                         ORDER BY chunk_id\n",
    "                         LIMIT :batch\n",
    "                    \"\"\")\n",
    "                    params = {\"batch\": BATCH_SIZE}\n",
    "                else:\n",
    "                    sql = text(\"\"\"\n",
    "                        SELECT chunk_id, paper_id, chunk_data\n",
    "                          FROM arxiv_chunks_backup\n",
    "                         WHERE chunk_id > :last\n",
    "                         ORDER BY chunk_id\n",
    "                         LIMIT :batch\n",
    "                    \"\"\")\n",
    "                    params = {\"last\": last_chunk_id, \"batch\": BATCH_SIZE}\n",
    "\n",
    "                # Fetch batch\n",
    "                result = conn.execution_options(stream_results=True) \\\n",
    "                             .execute(sql, params)\n",
    "                rows = result.fetchall()\n",
    "                if not rows:\n",
    "                    logger.info(\"No more rows to fetch, ending.\")\n",
    "                    break\n",
    "\n",
    "                # Index this batch\n",
    "                logger.info(\"Batch %d: fetched %d rows (chunk_id > %s)\",\n",
    "                            batch_num, len(rows), last_chunk_id)\n",
    "                for cid, pid, data in rows:\n",
    "                    writer.add_document(\n",
    "                        chunk_id   = str(cid),\n",
    "                        paper_id   = str(pid),\n",
    "                        chunk_data = data or \"\"\n",
    "                    )\n",
    "                total_indexed += len(rows)\n",
    "                logger.info(\"Batch %d: queued %d docs, total queued %d\",\n",
    "                            batch_num, len(rows), total_indexed)\n",
    "\n",
    "                # Keyset pagination marker\n",
    "                last_chunk_id = rows[-1][0]\n",
    "\n",
    "        # Final commit (merges all parallel segments)\n",
    "        logger.info(\"Committing index (this may take a moment)…\")\n",
    "        writer.commit()\n",
    "        logger.info(\"Indexing complete: %d documents indexed.\", total_indexed)\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Opening existing index in %s\", INDEX_DIR)\n",
    "        ix = open_dir(INDEX_DIR)\n",
    "\n",
    "    return ix\n",
    "\n",
    "ix = build_whoosh_index()\n",
    "logger.info(\"Whoosh index ready at: %s\", INDEX_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce8c5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 03:40:58 [INFO] Initialized BM25 query parser.\n",
      "2025-05-11 03:40:58 [WARNING] Corrupt Parquet 'arxiv_eval_results.parquet' (size=4 bytes): Could not open Parquet input source '<Buffer>': Parquet file size is 4 bytes, smaller than the minimum file footer (8 bytes). Reinitializing checkpoint.\n",
      "2025-05-11 03:40:58 [INFO] Initialized new Parquet checkpoint 'arxiv_eval_results.parquet'.\n",
      "2025-05-11 03:40:58 [INFO] BM25 parser and Parquet checkpoint ready. 0 chunks already processed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def initialize_bm25_and_checkpoint(ix):\n",
    "    # Prepare BM25 query parser\n",
    "    parser = QueryParser(\"chunk_data\", schema=ix.schema)\n",
    "    logger.info(\"Initialized BM25 query parser.\")\n",
    "\n",
    "    # Setup Parquet checkpoint for resumability, with recovery on corrupt file\n",
    "    if os.path.exists(PARQUET_FILE):\n",
    "        try:\n",
    "            df_existing = pd.read_parquet(PARQUET_FILE)\n",
    "            processed   = set(df_existing[\"chunk_id\"].astype(str).tolist())\n",
    "            parquet_writer = None\n",
    "            logger.info(\n",
    "                \"Loaded existing checkpoint '%s' with %d processed chunks.\",\n",
    "                PARQUET_FILE, len(processed)\n",
    "            )\n",
    "        except pa.lib.ArrowInvalid as e:\n",
    "            logger.warning(\n",
    "                \"Corrupt Parquet '%s' (size=%d bytes): %s. Reinitializing checkpoint.\",\n",
    "                PARQUET_FILE, os.path.getsize(PARQUET_FILE), e\n",
    "            )\n",
    "            os.remove(PARQUET_FILE)\n",
    "            # Reinitialize as new\n",
    "            init_df       = pd.DataFrame(columns=[\n",
    "                \"chunk_id\",\n",
    "                \"final_ground_truth\",\n",
    "                \"query_list\",\n",
    "                \"no_related_gt_flag\"\n",
    "            ])\n",
    "            init_table    = pa.Table.from_pandas(init_df, preserve_index=False)\n",
    "            parquet_writer = pq.ParquetWriter(PARQUET_FILE, init_table.schema)\n",
    "            processed      = set()\n",
    "            logger.info(\"Initialized new Parquet checkpoint '%s'.\", PARQUET_FILE)\n",
    "    else:\n",
    "        init_df       = pd.DataFrame(columns=[\n",
    "            \"chunk_id\",\n",
    "            \"final_ground_truth\",\n",
    "            \"query_list\",\n",
    "            \"no_related_gt_flag\"\n",
    "        ])\n",
    "        init_table    = pa.Table.from_pandas(init_df, preserve_index=False)\n",
    "        parquet_writer = pq.ParquetWriter(PARQUET_FILE, init_table.schema)\n",
    "        processed      = set()\n",
    "        logger.info(\"Initialized new Parquet checkpoint '%s'.\", PARQUET_FILE)\n",
    "\n",
    "    return parser, parquet_writer, processed\n",
    "\n",
    "parser, parquet_writer, processed = initialize_bm25_and_checkpoint(ix)\n",
    "\n",
    "logger.info(\"BM25 parser and Parquet checkpoint ready. %d chunks already processed.\",\n",
    "            len(processed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87516e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 03:40:58 [INFO] {\"client\": \"<lmstudio.sync_api.Client object at 0x7f6186379b50>\", \"event\": \"Websocket handling thread started\", \"thread_id\": \"Thread-4\"}\n",
      "2025-05-11 03:40:58 [INFO] {\"event\": \"Websocket handling task started\", \"ws_url\": \"ws://192.168.1.150:1234/llm\"}\n",
      "2025-05-11 03:40:58 [INFO] HTTP Request: GET ws://192.168.1.150:1234/llm \"HTTP/1.1 101 Switching Protocols\"\n",
      "2025-05-11 03:40:58 [INFO] {\"event\": \"Websocket session established (ws://192.168.1.150:1234/llm)\", \"ws_url\": \"ws://192.168.1.150:1234/llm\"}\n"
     ]
    }
   ],
   "source": [
    "import lmstudio as lms\n",
    "from lmstudio import Chat\n",
    "\n",
    "# Load your model once at startup\n",
    "MODEL_NAME     = \"phi-4\"  # or whichever you’ve installed\n",
    "NUM_QUERIES    = 3\n",
    "model = lms.get_default_client('192.168.1.150:1234').llm.model(MODEL_NAME)\n",
    "\n",
    "def generate_queries_with_lmstudio(prompt: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Uses LM Studio's Python SDK to generate NUM_QUERIES semantic-search prompts.\n",
    "    Returns a list of up to NUM_QUERIES strings.\n",
    "    \"\"\"\n",
    "    queries: list[str] = []\n",
    "\n",
    "    # For each query, spin up a fresh chat context so we get independent completions\n",
    "    for _ in range(NUM_QUERIES):\n",
    "        chat = Chat()                       # new chat session\n",
    "        chat.add_user_message(prompt)       # user prompt\n",
    "        response = model.respond(chat)      # get assistant reply\n",
    "        # strip out any leading/trailing whitespace\n",
    "        text = response.strip() if isinstance(response, str) else response\n",
    "        queries.append(text)\n",
    "    \n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2fabd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:29 [INFO] Generating queries for 2746 chunks with 4 threads\n",
      "2025-05-11 05:01:31 [INFO] Chunk 0712.3869v2_13 → stored 3 queries (1.56 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"genus calculation via branching data\",\n",
      "    \"monodromy group reducible case\",\n",
      "    \"Klein functions maximal decompositions\"\n",
      "  ]\n",
      "}\n",
      "['genus calculation via branching data', 'monodromy group reducible case', 'Klein functions maximal decompositions']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:32 [INFO] Chunk 0712.3869v2_1 → stored 3 queries (2.84 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"imprimitivity systems permutation groups\",\n",
      "    \"generalization Ritt theorem rational functions\",\n",
      "    \"Jordan H older theorem maximal decompositions\"\n",
      "  ]\n",
      "}\n",
      "['imprimitivity systems permutation groups', 'generalization Ritt theorem rational functions', 'Jordan H older theorem maximal decompositions']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:33 [INFO] Chunk 0712.3869v2_3 → stored 3 queries (4.03 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"lower semi modular lattices\",\n",
      "    \"Ritt theorem rational functions\",\n",
      "    \"Jordan H older imprimitivity systems\"\n",
      "  ]\n",
      "}\n",
      "['lower semi modular lattices', 'Ritt theorem rational functions', 'Jordan H older imprimitivity systems']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:34 [INFO] Chunk 0712.3869v2_7 → stored 3 queries (5.14 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"Jordan-H\\\"older theorem imprimitivity\",\n",
      "    \"permutable subgroups lattice\",\n",
      "    \"core complementary subgroups\"\n",
      "  ]\n",
      "}\n",
      "['Jordan-H\"older theorem imprimitivity', 'permutable subgroups lattice', 'core complementary subgroups']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:36 [INFO] Chunk 0712.3869v2_8 → stored 3 queries (4.93 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"modular lattice isomorphic sublattice\",\n",
      "    \"Jordan-H\\\"older theorem imprimitivity systems\",\n",
      "    \"Hamiltonian group normal subgroups\"\n",
      "  ]\n",
      "}\n",
      "['modular lattice isomorphic sublattice', 'Jordan-H\"older theorem imprimitivity systems', 'Hamiltonian group normal subgroups']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:37 [INFO] Chunk 1001.2978v1_2 → stored 3 queries (4.41 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"size multiplication preferential relations\",\n",
      "    \"semantical interpolation monotonic non-monotonic\",\n",
      "    \"revision distance relations\"\n",
      "  ]\n",
      "}\n",
      "['size multiplication preferential relations', 'semantical interpolation monotonic non-monotonic', 'revision distance relations']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:38 [INFO] Chunk 0712.3869v2_16 → stored 3 queries (4.48 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"Belyi functions tetrahedron\",\n",
      "    \"maximal decompositions S4\",\n",
      "    \"Ritt theorem counterexamples\"\n",
      "  ]\n",
      "}\n",
      "['Belyi functions tetrahedron', 'maximal decompositions S4', 'Ritt theorem counterexamples']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:39 [INFO] Chunk 0712.2596v2_17 → stored 3 queries (4.53 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"Cooper problem on a ring\",\n",
      "    \"Little Parks period oscillations\",\n",
      "    \"instanton approach flux oscillation\"\n",
      "  ]\n",
      "}\n",
      "['Cooper problem on a ring', 'Little Parks period oscillations', 'instanton approach flux oscillation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:40 [INFO] Chunk 0712.2596v2_18 → stored 3 queries (4.51 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"instanton amplitude single electron\",\n",
      "    \"ground state energy two electrons\",\n",
      "    \"cooper pair separation scales\"\n",
      "  ]\n",
      "}\n",
      "['instanton amplitude single electron', 'ground state energy two electrons', 'cooper pair separation scales']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:42 [INFO] Chunk 0712.2596v2_4 → stored 3 queries (4.99 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"self consistency equation superconductors\",\n",
      "    \"critical temperature Debye frequency\",\n",
      "    \"Cooper pair size critical temperature\"\n",
      "  ]\n",
      "}\n",
      "['self consistency equation superconductors', 'critical temperature Debye frequency', 'Cooper pair size critical temperature']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:43 [INFO] Chunk 0712.2596v2_5 → stored 3 queries (4.98 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"exchange field superconductivity transitions\",\n",
      "    \"flux effect on superconducting states\",\n",
      "    \"phase diagram critical temperature flux\"\n",
      "  ]\n",
      "}\n",
      "['exchange field superconductivity transitions', 'flux effect on superconducting states', 'phase diagram critical temperature flux']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:44 [INFO] Chunk 0712.2596v2_9 → stored 3 queries (4.96 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"superconducting phase transition\",\n",
      "    \"finite size effects superconductivity\",\n",
      "    \"critical temperature oscillations\"\n",
      "  ]\n",
      "}\n",
      "['superconducting phase transition', 'finite size effects superconductivity', 'critical temperature oscillations']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:45 [INFO] Chunk 0712.2596v2_10 → stored 3 queries (4.87 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"finite radius superconducting rings\",\n",
      "    \"oscillations in Tc with magnetic flux\",\n",
      "    \"Little Parks effect modifications\"\n",
      "  ]\n",
      "}\n",
      "['finite radius superconducting rings', 'oscillations in Tc with magnetic flux', 'Little Parks effect modifications']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:46 [INFO] Chunk 0712.2596v2_11 → stored 3 queries (4.79 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"double solutions critical temperature\",\n",
      "    \"orbital pair breaking effect\",\n",
      "    \"Cooper pairs even odd effect\"\n",
      "  ]\n",
      "}\n",
      "['double solutions critical temperature', 'orbital pair breaking effect', 'Cooper pairs even odd effect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:01:48 [INFO] Chunk 0712.2596v2_13 → stored 3 queries (4.74 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    \"Matsubara sum upper cutoff\",\n",
      "    \"critical temperature Tc equation\",\n",
      "    \"finiteness correction radius\"\n",
      "  ]\n",
      "}\n",
      "['Matsubara sum upper cutoff', 'critical temperature Tc equation', 'finiteness correction radius']\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 2: Semantic-Search Query Generation with ETA ──────────────\n",
    "\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sqlalchemy import text\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class QueryList(BaseModel):\n",
    "    queries: List[str]\n",
    "\n",
    "MAX_WORKERS = 2\n",
    "MAX_RETRIES = 3\n",
    "LOG_INTERVAL = 50\n",
    "\n",
    "def generate_for_chunk(task):\n",
    "    chunk_id, chunk_data = task\n",
    "    snippet = chunk_data or \"\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"Below is a snippet from a scientific paper. Generate exactly {NUM_QUERIES} \"\n",
    "        \"(1-6 words each) semantic-search queries a researcher might use to find it.\\n\\n\"\n",
    "        \"IMPORTANT: return ONLY a JSON object with a single key \\\"queries\\\" whose value \"\n",
    "        \"is an array of strings—no markdown, no backticks, no explanations. Exactly:\\n\"\n",
    "        '{\"queries\":[\"query1\",\"query2\",\"query3\"]}\\n\\n'\n",
    "        \"Snippet:\\n\\\"\\\"\\\"\\n\"\n",
    "        f\"{snippet}\\n\"\n",
    "        \"\\\"\\\"\\\"\"\n",
    "    )\n",
    "\n",
    "    t0 = time.monotonic()\n",
    "    queries: List[str] = []\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            result = model.respond(prompt, response_format=QueryList)\n",
    "            queries = result.parsed[\"queries\"][:NUM_QUERIES]\n",
    "            if len(queries) < NUM_QUERIES:\n",
    "                logger.warning(\"Chunk %s: got %d queries, expected %d\", \n",
    "                               chunk_id, len(queries), NUM_QUERIES)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Chunk %s: attempt %d/%d failed (%s)\", \n",
    "                           chunk_id, attempt, MAX_RETRIES, e)\n",
    "            if attempt == MAX_RETRIES:\n",
    "                logger.error(\"Chunk %s: all retries failed, defaulting to empty list\", chunk_id)\n",
    "                queries = []\n",
    "    gen_time = time.monotonic() - t0\n",
    "    logger.debug(\"Chunk %s: generation took %.2f s\", chunk_id, gen_time)\n",
    "\n",
    "    t1 = time.monotonic()\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(\"\"\"\n",
    "            UPDATE arxiv_chunks_eval_4\n",
    "               SET query = :q\n",
    "             WHERE chunk_id = :cid\n",
    "        \"\"\"), {\"q\": json.dumps(queries), \"cid\": chunk_id})\n",
    "    db_time = time.monotonic() - t1\n",
    "    logger.debug(\"Chunk %s: DB update took %.2f s\", chunk_id, db_time)\n",
    "\n",
    "    total_time = gen_time + db_time\n",
    "    logger.info(\"Chunk %s → stored %d queries (%.2f s)\", \n",
    "                chunk_id, len(queries), total_time)\n",
    "    return total_time\n",
    "\n",
    "# Fetch pending chunks\n",
    "with engine.connect() as conn:\n",
    "    rows = conn.execute(text(\"\"\"\n",
    "        SELECT chunk_id,\n",
    "               chunk_data\n",
    "          FROM public.arxiv_chunks_eval_4\n",
    "         WHERE paper_cited IS NOT NULL\n",
    "           AND paper_cited <> ''\n",
    "           AND (query IS NULL OR query = '' OR query = '[]')\n",
    "           AND array_length(\n",
    "                 string_to_array(\n",
    "                   trim(both '{}' FROM paper_cited),\n",
    "                   ','\n",
    "                 ),\n",
    "                 1\n",
    "               ) >= 5\n",
    "    \"\"\")).fetchall()\n",
    "\n",
    "\n",
    "tasks = [(r.chunk_id, r.chunk_data) for r in rows]\n",
    "total = len(tasks)\n",
    "logger.info(\"Generating queries for %d chunks with %d threads\", total, MAX_WORKERS)\n",
    "\n",
    "processed = 0\n",
    "time_accum = 0.0\n",
    "start_all = time.monotonic()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    future_to_cid = {executor.submit(generate_for_chunk, t): t[0] for t in tasks}\n",
    "    for future in as_completed(future_to_cid):\n",
    "        cid = future_to_cid[future]\n",
    "        try:\n",
    "            duration = future.result()\n",
    "            time_accum += duration\n",
    "        except Exception as e:\n",
    "            logger.error(\"Chunk %s: unexpected error %s\", cid, e)\n",
    "        processed += 1\n",
    "\n",
    "        if processed % LOG_INTERVAL == 0 or processed == total:\n",
    "            elapsed = time.monotonic() - start_all\n",
    "            avg = time_accum / processed if processed else 0\n",
    "            remaining = avg * (total - processed)\n",
    "            eta = datetime.datetime.now() + datetime.timedelta(seconds=remaining)\n",
    "            logger.info(\n",
    "                \"Progress %d/%d — Elapsed: %.1f m, ETA in %.1f m (at %s)\",\n",
    "                processed, total,\n",
    "                elapsed/60, remaining/60,\n",
    "                eta.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            )\n",
    "\n",
    "logger.info(\"Query generation complete: %d/%d chunks done\", processed, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb3aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- BM25 Filtering Test for One Sample Row (fixed) ---\n",
    "\n",
    "# import json\n",
    "# from sqlalchemy import text\n",
    "\n",
    "# # Fetch one sample row that still needs a query\n",
    "# with engine.connect() as conn:\n",
    "#     sample_row = conn.execute(text(\"\"\"\n",
    "#         SELECT paper_id, chunk_id, chunk_data, paper_cited\n",
    "#           FROM arxiv_chunks_eval_4\n",
    "#          WHERE query = ''\n",
    "#          LIMIT 1\n",
    "#     \"\"\")).mappings().first()\n",
    "\n",
    "# print(\"=== Sample Row ===\")\n",
    "# for k, v in sample_row.items():\n",
    "#     print(f\"{k}: {v!r}\")\n",
    "\n",
    "# # Preview the snippet\n",
    "# snippet = sample_row[\"chunk_data\"] or \"\"\n",
    "# print(\"\\n=== Snippet Preview ===\")\n",
    "# print(snippet[:200] + (\"...\" if len(snippet) > 200 else \"\"))\n",
    "\n",
    "# # Parse and run BM25 search, extracting fields inside the searcher context\n",
    "# q = parser.parse(snippet[:200])   # use first 200 characters as the query text\n",
    "# with ix.searcher() as searcher:\n",
    "#     hits = searcher.search(q, limit=TOP_N)\n",
    "#     retrieved = set()\n",
    "#     for h in hits:\n",
    "#         if MIN_SCORE is None or h.score >= MIN_SCORE:\n",
    "#             # Access stored field while searcher is open\n",
    "#             retrieved.add(h[\"paper_id\"])\n",
    "\n",
    "# # Original ground truth list\n",
    "# original_gt = (\n",
    "#     sample_row[\"paper_cited\"].strip(\"{}\").split(\",\")\n",
    "#     if sample_row[\"paper_cited\"] else []\n",
    "# )\n",
    "# original_gt = [pid for pid in original_gt if pid]\n",
    "\n",
    "# # Filter ground truths by BM25 hits\n",
    "# filtered_gt = [pid for pid in original_gt if pid in retrieved]\n",
    "\n",
    "# print(\"\\nOriginal Ground Truth IDs:\", original_gt)\n",
    "# print(\"Retrieved by BM25:\", retrieved)\n",
    "# print(\"Filtered Ground Truth IDs:\", filtered_gt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c72a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test snippet to verify BM25 filtering and LM Studio structured query generation for one row\n",
    "\n",
    "# import json\n",
    "# from sqlalchemy import text\n",
    "# from pydantic import BaseModel\n",
    "# import lmstudio as lms\n",
    "\n",
    "# # --- Structured response schema for LM Studio ---\n",
    "# class QuerySchema(BaseModel):\n",
    "#     queries: list[str]\n",
    "\n",
    "# # --- Initialize LM Studio model ---\n",
    "# MODEL_NAME  = \"phi-4\"        # your installed LM Studio model\n",
    "# NUM_QUERIES = 3\n",
    "# model       = lms.llm(MODEL_NAME)\n",
    "\n",
    "# def generate_queries_with_lmstudio(prompt: str) -> list[str]:\n",
    "#     \"\"\"\n",
    "#     Uses LM Studio Python SDK to return exactly {\"queries\": [...]}\n",
    "#     via the QuerySchema, and returns that list.\n",
    "#     \"\"\"\n",
    "#     resp = model.respond(\n",
    "#         prompt,\n",
    "#         response_format=QuerySchema\n",
    "#     )\n",
    "#     parsed = resp.parsed  # this will be a dict conforming to QuerySchema\n",
    "#     if isinstance(parsed, dict):\n",
    "#         return parsed[\"queries\"]\n",
    "#     # fallback if parsed is a pydantic instance\n",
    "#     return parsed.queries\n",
    "\n",
    "# # --- Fetch one sample row that still needs a query ---\n",
    "# with engine.connect() as conn:\n",
    "#     sample_row = conn.execute(text(\"\"\"\n",
    "#         SELECT paper_id, chunk_id, chunk_data, paper_cited\n",
    "#           FROM arxiv_chunks_eval_4\n",
    "#          WHERE query = ''\n",
    "#          LIMIT 1\n",
    "#     \"\"\")).mappings().first()\n",
    "\n",
    "# print(\"=== Sample Row ===\")\n",
    "# for k, v in sample_row.items():\n",
    "#     print(f\"{k}: {v!r}\")\n",
    "\n",
    "# # --- BM25 filtering ---\n",
    "# snippet = sample_row[\"chunk_data\"] or \"\"\n",
    "# print(\"\\n=== Snippet Preview ===\")\n",
    "# print(snippet[:200] + (\"...\" if len(snippet) > 200 else \"\"))\n",
    "\n",
    "# q = parser.parse(snippet[:200])\n",
    "# with ix.searcher() as searcher:\n",
    "#     hits = searcher.search(q, limit=TOP_N)\n",
    "#     retrieved = {\n",
    "#         h[\"paper_id\"] for h in hits\n",
    "#         if MIN_SCORE is None or h.score >= MIN_SCORE\n",
    "#     }\n",
    "\n",
    "# original_gt = (\n",
    "#     sample_row[\"paper_cited\"].strip(\"{}\").split(\",\")\n",
    "#     if sample_row[\"paper_cited\"] else []\n",
    "# )\n",
    "# original_gt = [pid for pid in original_gt if pid]\n",
    "# filtered_gt = [pid for pid in original_gt if pid in retrieved]\n",
    "\n",
    "# print(\"\\nOriginal Ground Truth IDs:\", original_gt)\n",
    "# print(\"Retrieved by BM25:\", retrieved)\n",
    "# print(\"Filtered Ground Truth IDs:\", filtered_gt)\n",
    "\n",
    "# # --- LM Studio structured query generation ---\n",
    "# prompt = (\n",
    "#     f\"Given this scientific snippet, write {NUM_QUERIES} concise \"\n",
    "#     f\"semantic-search queries a researcher might use to find it:\\n\\n\\\"{snippet}\\\"\"\n",
    "# )\n",
    "\n",
    "# print(\"\\n=== Prompt ===\")\n",
    "# print(prompt)\n",
    "\n",
    "# generated_queries = generate_queries_with_lmstudio(prompt)\n",
    "\n",
    "# print(\"\\nGenerated Queries (structured list):\")\n",
    "# print(generated_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ab004c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ── Sample Test: BM25 Top-N Unique Paper Filtering ──────────────────────────\n",
    "\n",
    "# import json\n",
    "# from sqlalchemy import create_engine, text\n",
    "# from whoosh.index import open_dir\n",
    "# from whoosh.qparser import QueryParser\n",
    "\n",
    "# TOP_N      = 5\n",
    "# MIN_SCORE  = None        # or e.g. 1.0 to threshold scores\n",
    "\n",
    "# with engine.connect() as conn:\n",
    "#     sample = conn.execute(text(\"\"\"\n",
    "#         SELECT chunk_id, paper_cited, chunk_data\n",
    "#           FROM arxiv_chunks_eval_4\n",
    "#          WHERE paper_cited IS NOT NULL\n",
    "#            AND paper_cited <> ''\n",
    "#          LIMIT 1\n",
    "#     \"\"\")).mappings().first()\n",
    "\n",
    "# print(\"=== Sample Row ===\")\n",
    "# print(f\"chunk_id   : {sample['chunk_id']}\")\n",
    "# print(f\"paper_cited: {sample['paper_cited']!r}\")\n",
    "# print(f\"snippet    : {sample['chunk_data'][:200]!r}…\")\n",
    "\n",
    "# # ── Parse original citations ────────────────────────────────────────────────\n",
    "# original = sample[\"paper_cited\"].strip(\"{}\").split(\",\")\n",
    "# original = [pid for pid in original if pid]\n",
    "# print(\"\\nOriginal cited papers:\", original)\n",
    "\n",
    "# # ── BM25 search & collect top-N unique paper_ids ────────────────────────────\n",
    "# snippet = (sample[\"chunk_data\"] or \"\")[:200]\n",
    "# q       = parser.parse(snippet)\n",
    "\n",
    "# unique_pids = []\n",
    "# with ix.searcher() as searcher:\n",
    "#     hits = searcher.search(q, limit=TOP_N * 10)\n",
    "#     for hit in hits:\n",
    "#         if MIN_SCORE is not None and hit.score < MIN_SCORE:\n",
    "#             continue\n",
    "#         pid = hit[\"paper_id\"]\n",
    "#         if pid not in unique_pids:\n",
    "#             unique_pids.append(pid)\n",
    "#             if len(unique_pids) == TOP_N:\n",
    "#                 break\n",
    "\n",
    "# print(\"BM25 top-N unique paper_ids:\", unique_pids)\n",
    "\n",
    "# # ── Filter original citations against those unique hits ────────────────────\n",
    "# filtered = [pid for pid in original if pid in unique_pids]\n",
    "# print(\"Filtered cited papers      :\", filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d54346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ── Cell 1: BM25‐Based Ground‐Truth Filtering (Thread‐Isolated, Verbose Logging) ──\n",
    "\n",
    "# import json\n",
    "# import logging\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from sqlalchemy import text\n",
    "# from whoosh.index import open_dir\n",
    "# from whoosh.qparser import QueryParser\n",
    "\n",
    "# # Assumes:\n",
    "# # engine      : SQLAlchemy engine\n",
    "# # INDEX_DIR   : path to your Whoosh index dir\n",
    "# # TOP_N       : int, e.g. 5\n",
    "# # MIN_SCORE   : float or None\n",
    "# # MAX_WORKERS : int, e.g. 4\n",
    "# # logger      : configured logger\n",
    "# MAX_WORKERS = 4\n",
    "\n",
    "# def filter_one(task):\n",
    "#     chunk_id, cited_str, chunk_data = task\n",
    "#     logger.info(\"Thread %s: start filtering\", chunk_id)\n",
    "#     # Re-open index and parser inside each thread for isolation\n",
    "#     ix_local    = open_dir(INDEX_DIR)\n",
    "#     parser_local = QueryParser(\"chunk_data\", schema=ix_local.schema)\n",
    "\n",
    "#     # 1) Parse original citations\n",
    "#     if isinstance(cited_str, str) and cited_str.startswith(\"{\") and cited_str.endswith(\"}\"):\n",
    "#         original = [pid.strip() for pid in cited_str[1:-1].split(\",\") if pid.strip()]\n",
    "#     else:\n",
    "#         original = []\n",
    "#     logger.debug(\"Chunk %s: original citations %s\", chunk_id, original)\n",
    "\n",
    "#     # 2) BM25 search\n",
    "#     snippet     = (chunk_data or \"\")[:200]\n",
    "#     q_local     = parser_local.parse(snippet)\n",
    "#     window_size = TOP_N * 10\n",
    "\n",
    "#     unique_pids = []\n",
    "#     with ix_local.searcher() as searcher:\n",
    "#         hits = searcher.search(q_local, limit=window_size)\n",
    "#         logger.debug(\"Chunk %s: retrieved %d hits\", chunk_id, len(hits))\n",
    "#         for h in hits:\n",
    "#             score = h.score\n",
    "#             pid   = h[\"paper_id\"]\n",
    "#             if MIN_SCORE is not None and score < MIN_SCORE:\n",
    "#                 logger.debug(\"Chunk %s: skip pid %s (score %.2f < %.2f)\", chunk_id, pid, score, MIN_SCORE)\n",
    "#                 continue\n",
    "#             if pid not in unique_pids:\n",
    "#                 unique_pids.append(pid)\n",
    "#                 logger.debug(\"Chunk %s: keep unique pid %s\", chunk_id, pid)\n",
    "#                 if len(unique_pids) >= TOP_N:\n",
    "#                     break\n",
    "\n",
    "#     # 3) Filter citations\n",
    "#     filtered = [pid for pid in original if pid in unique_pids]\n",
    "#     logger.info(\"Chunk %s: filtered down to %d/%d citations\", chunk_id, len(filtered), len(original))\n",
    "\n",
    "#     return chunk_id, filtered\n",
    "\n",
    "# # 1) Load tasks (one-time)\n",
    "# with engine.connect() as conn:\n",
    "#     rows = conn.execute(text(\"\"\"\n",
    "#         SELECT chunk_id, paper_cited, chunk_data\n",
    "#           FROM arxiv_chunks_eval_4\n",
    "#          WHERE paper_cited IS NOT NULL AND paper_cited <> ''\n",
    "#     \"\"\")).fetchall()\n",
    "\n",
    "# tasks = [(r.chunk_id, r.paper_cited, r.chunk_data) for r in rows]\n",
    "# total = len(tasks)\n",
    "# logger.info(\"Loaded %d tasks; dispatching to %d threads\", total, MAX_WORKERS)\n",
    "\n",
    "# results = []\n",
    "# processed = 0\n",
    "# log_interval = 100\n",
    "\n",
    "# # 2) Execute in parallel, but commit only on main thread\n",
    "# with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "#     future_to_task = {executor.submit(filter_one, t): t for t in tasks}\n",
    "#     for future in as_completed(future_to_task):\n",
    "#         chunk_id = future_to_task[future][0]\n",
    "#         try:\n",
    "#             cid, filtered = future.result()\n",
    "#             results.append((cid, filtered))\n",
    "#         except Exception as e:\n",
    "#             logger.error(\"Chunk %s failed: %s\", chunk_id, e)\n",
    "\n",
    "#         processed += 1\n",
    "#         if processed % log_interval == 0 or processed == total:\n",
    "#             logger.info(\"Overall progress: %d/%d\", processed, total)\n",
    "\n",
    "# # 3) Persist all updates in a single transaction batch\n",
    "# logger.info(\"Persisting %d filtered results back to database\", len(results))\n",
    "# with engine.begin() as conn:\n",
    "#     for chunk_id, filtered in results:\n",
    "#         array_str = \"{\" + \",\".join(filtered) + \"}\"\n",
    "#         conn.execute(text(\"\"\"\n",
    "#             UPDATE arxiv_chunks_eval_4\n",
    "#                SET paper_cited = :pc\n",
    "#              WHERE chunk_id = :cid\n",
    "#         \"\"\"), {\"pc\": array_str, \"cid\": chunk_id})\n",
    "\n",
    "# logger.info(\"All updates committed; filtering complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "740b25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# import pyarrow as pa\n",
    "# import concurrent.futures\n",
    "# from sqlalchemy import text\n",
    "\n",
    "# # ── Logging Setup ─────────────────────────────────────────────\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "#     datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # ── Assumes these are already defined in your notebook: ──────────\n",
    "# # engine           : SQLAlchemy engine\n",
    "# # ix               : Whoosh index\n",
    "# # parser           : QueryParser(\"chunk_data\", schema=ix.schema)\n",
    "# # parquet_writer   : pyarrow.parquet.ParquetWriter or None\n",
    "# # processed        : set of chunk_id strings already done\n",
    "# # PARQUET_FILE     : path to your checkpoint file\n",
    "# # generate_queries_with_ollama(prompt) : function returning list of strings\n",
    "\n",
    "# TOP_N       = 3\n",
    "# MIN_SCORE   = None\n",
    "# NUM_QUERIES = 3\n",
    "# MAX_WORKERS = 4\n",
    "\n",
    "# def process_and_save(row):\n",
    "#     chunk_id   = str(row[\"chunk_id\"])\n",
    "#     snippet    = row[\"chunk_data\"] or \"\"\n",
    "#     cited_str  = row.get(\"paper_cited\", \"\")\n",
    "#     original_gt = cited_str.strip(\"{}\").split(\",\") if cited_str else []\n",
    "#     original_gt = [pid for pid in original_gt if pid]\n",
    "\n",
    "#     logger.info(\"Processing chunk %s (original_gt=%s)\", chunk_id, original_gt)\n",
    "\n",
    "#     # BM25 filtering\n",
    "#     q = parser.parse(snippet[:200])\n",
    "#     with ix.searcher() as searcher:\n",
    "#         hits = searcher.search(q, limit=TOP_N)\n",
    "#         retrieved = {\n",
    "#             h[\"paper_id\"] for h in hits\n",
    "#             if MIN_SCORE is None or h.score >= MIN_SCORE\n",
    "#         }\n",
    "#     filtered_gt = [pid for pid in original_gt if pid in retrieved]\n",
    "#     no_gt_flag  = not bool(filtered_gt)\n",
    "#     logger.info(\"Chunk %s: filtered_gt=%s no_related_gt_flag=%s\",\n",
    "#                 chunk_id, filtered_gt, no_gt_flag)\n",
    "\n",
    "#     # Ollama query generation\n",
    "#     prompt = (\n",
    "#         f\"Given this scientific snippet, write {NUM_QUERIES} concise \"\n",
    "#         f\"semantic-search queries a researcher might use to find it:\\n\\n\"\n",
    "#         f\"\\\"{snippet}\\\"\"\n",
    "#     )\n",
    "#     query_list = generate_queries_with_ollama(prompt)\n",
    "#     logger.info(\"Chunk %s: generated %d queries\", chunk_id, len(query_list))\n",
    "\n",
    "#     # Append to Parquet checkpoint\n",
    "#     df_row = pd.DataFrame([{\n",
    "#         \"chunk_id\":           chunk_id,\n",
    "#         \"final_ground_truth\": filtered_gt,\n",
    "#         \"query_list\":         query_list,\n",
    "#         \"no_related_gt_flag\": no_gt_flag\n",
    "#     }])\n",
    "#     if parquet_writer:\n",
    "#         table = pa.Table.from_pandas(df_row, preserve_index=False)\n",
    "#         parquet_writer.write_table(table)\n",
    "#         logger.info(\"Chunk %s: written to Parquet via writer\", chunk_id)\n",
    "#     else:\n",
    "#         df_row.to_parquet(\n",
    "#             PARQUET_FILE,\n",
    "#             engine=\"fastparquet\",\n",
    "#             append=True,\n",
    "#             index=False\n",
    "#         )\n",
    "#         logger.info(\"Chunk %s: appended to Parquet file\", chunk_id)\n",
    "\n",
    "#     processed.add(chunk_id)\n",
    "\n",
    "#     # Update eval table\n",
    "#     with engine.begin() as conn2:\n",
    "#         conn2.execute(text(\"\"\"\n",
    "#             UPDATE arxiv_chunks_eval_4\n",
    "#                SET query               = :q,\n",
    "#                    no_related_gt_flag  = :flag\n",
    "#              WHERE chunk_id = :cid\n",
    "#         \"\"\"), {\n",
    "#             \"q\":    json.dumps(query_list),\n",
    "#             \"flag\": no_gt_flag,\n",
    "#             \"cid\":  chunk_id\n",
    "#         })\n",
    "#     logger.info(\"Chunk %s: database record updated\", chunk_id)\n",
    "\n",
    "#     return chunk_id\n",
    "\n",
    "# # ── Fetch and filter tasks ───────────────────────────────────────\n",
    "# logger.info(\"Fetching chunks needing query generation…\")\n",
    "# with engine.connect() as conn:\n",
    "#     rows = conn.execute(text(\"\"\"\n",
    "#         SELECT paper_id, chunk_id, chunk_data, paper_cited\n",
    "#           FROM arxiv_chunks_eval_4\n",
    "#          WHERE query = ''\n",
    "#     \"\"\")).mappings().all()\n",
    "\n",
    "# tasks = [r for r in rows if str(r[\"chunk_id\"]) not in processed]\n",
    "# logger.info(\"Total chunks to process: %d\", len(tasks))\n",
    "\n",
    "# # ── Process in parallel ─────────────────────────────────────────\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "#     for cid in executor.map(process_and_save, tasks):\n",
    "#         logger.info(\"Finished chunk %s\", cid)\n",
    "\n",
    "# logger.info(\"Done — total processed: %d\", len(processed))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
