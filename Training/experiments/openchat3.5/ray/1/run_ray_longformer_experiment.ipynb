{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9241feac-3d73-49b6-8513-260ce34bd1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7032ecf84d40>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/ray/\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7032ecd9d2b0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/ray/\u001b[0m\u001b[33m\n",
      "Collecting ray[default]\n",
      "  Downloading ray-2.46.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.12/site-packages (from ray[default]) (8.1.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from ray[default]) (3.13.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.12/site-packages (from ray[default]) (4.23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from ray[default]) (1.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from ray[default]) (24.2)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.12/site-packages (from ray[default]) (5.28.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from ray[default]) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from ray[default]) (2.32.3)\n",
      "Requirement already satisfied: aiohttp>=3.7 in /opt/conda/lib/python3.12/site-packages (from ray[default]) (3.11.18)\n",
      "Collecting aiohttp_cors (from ray[default])\n",
      "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting colorful (from ray[default])\n",
      "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting py-spy>=0.4.0 (from ray[default])\n",
      "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
      "Collecting grpcio>=1.42.0 (from ray[default])\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting opencensus (from ray[default])\n",
      "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /opt/conda/lib/python3.12/site-packages (from ray[default]) (2.10.6)\n",
      "Requirement already satisfied: prometheus_client>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from ray[default]) (0.21.1)\n",
      "Collecting smart_open (from ray[default])\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default])\n",
      "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.7->ray[default]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.7->ray[default]) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.7->ray[default]) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.7->ray[default]) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.7->ray[default]) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.7->ray[default]) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.7->ray[default]) (1.20.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]) (4.12.2)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default])\n",
      "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /opt/conda/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]) (4.3.6)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema->ray[default]) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema->ray[default]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema->ray[default]) (0.22.3)\n",
      "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default])\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: six~=1.16 in /opt/conda/lib/python3.12/site-packages (from opencensus->ray[default]) (1.17.0)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0 (from opencensus->ray[default])\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->ray[default]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->ray[default]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->ray[default]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->ray[default]) (2024.12.14)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from smart_open->ray[default]) (1.17.2)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default])\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default])\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/conda/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (2.40.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (4.9.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/conda/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.6.1)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
      "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
      "Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
      "Downloading ray-2.46.0-cp312-cp312-manylinux2014_x86_64.whl (68.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Installing collected packages: py-spy, opencensus-context, distlib, colorful, virtualenv, smart_open, proto-plus, grpcio, googleapis-common-protos, google-api-core, aiohttp_cors, ray, opencensus\n",
      "Successfully installed aiohttp_cors-0.8.1 colorful-0.5.6 distlib-0.3.9 google-api-core-2.24.2 googleapis-common-protos-1.70.0 grpcio-1.71.0 opencensus-0.11.4 opencensus-context-0.1.3 proto-plus-1.26.1 py-spy-0.4.0 ray-2.46.0 smart_open-7.1.0 virtualenv-20.31.2\n"
     ]
    }
   ],
   "source": [
    "!pip install ray[default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feddbdd6-1336-4d4d-9cee-732a72240a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import ray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c43c36ca-46bd-4f7e-9f77-b7aefd624853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html><html lang=\"en\"><head><meta charset=\"utf-8\"/><link rel=\"shortcut icon\" href=\"./favicon.ico\"/><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"/><title>Ray Dashboard</title><script defer=\"defer\" src=\"./static/js/main.f57297a9.js\"></script><link href=\"./static/css/main.388a904b.css\" rel=\"stylesheet\"></head><body><noscript>You need to enable JavaScript to run this app.</noscript><div id=\"root\"></div></body></html>"
     ]
    }
   ],
   "source": [
    "!curl http://ray-head:8265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b2f558-cdce-4a82-a5b2-3ac127724435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 04:35:00,829 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 04:35:02,083\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_b4894399b15d3978.zip.\n",
      "2025-05-09 04:35:02,084\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_Hztfm3LKeuXjUf8m' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_Hztfm3LKeuXjUf8m\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_Hztfm3LKeuXjUf8m\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_Hztfm3LKeuXjUf8m\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "^C\n",
      "\n",
      "Aborted!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_4_mountcheckpoint_fault.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b6daf6-b7bc-4bf3-88d9-8716d1ca4f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 04:41:19,746 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "Attempting to stop job 'raysubmit_Hztfm3LKeuXjUf8m'\n",
      "Waiting for job 'raysubmit_Hztfm3LKeuXjUf8m' to exit (disable with --no-wait):\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "Job has not exited yet. Status: PENDING\n",
      "^C\n",
      "\n",
      "Aborted!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job stop --address http://ray-head:8265  raysubmit_Hztfm3LKeuXjUf8m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aef33f6-6dc4-4110-b6c5-6179e82eef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 04:42:53,878 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "Attempting to stop job 'raysubmit_Hztfm3LKeuXjUf8m'\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!ray job stop --address http://ray-head:8265  raysubmit_Hztfm3LKeuXjUf8m --no-wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2b9b63b-dd37-48a0-852a-b7b32c48e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 04:47:10,023 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 04:47:11,323\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_1f5783213ac0a7f0.zip.\n",
      "2025-05-09 04:47:11,324\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_HV5DXJd1HknEpy7R' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_HV5DXJd1HknEpy7R\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_HV5DXJd1HknEpy7R\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_HV5DXJd1HknEpy7R\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_1f5783213ac0a7f0/longformer_4_mountcheckpoint_fault.py:11 in  │\n",
      "│ <module>                                                                     │\n",
      "│                                                                              │\n",
      "│     8 from torch.utils.data import DataLoader                                │\n",
      "│     9 from sentence_transformers import SentenceTransformer, InputExample, l │\n",
      "│    10 from ray.train.torch import TorchTrainer                               │\n",
      "│ ❱  11 from ray.train import ScalingConfig, Checkpoint, train, init           │\n",
      "│    12 from ray.data import from_items                                        │\n",
      "│    13 from ray.train import RunConfig, FailureConfig                         │\n",
      "│    14                                                                        │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "ImportError: cannot import name 'train' from 'ray.train' \n",
      "(/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/__init__.py)\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_HV5DXJd1HknEpy7R' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "│     8 from torch.utils.data import DataLoader                                │\n",
      "│     9 from sentence_transformers import SentenceTransformer, InputExample, l │\n",
      "│    10 from ray.train.torch import TorchTrainer                               │\n",
      "│ ❱  11 from ray.train import ScalingConfig, Checkpoint, train, init           │\n",
      "│    12 from ray.data import from_items                                        │\n",
      "│    13 from ray.train import RunConfig, FailureConfig                         │\n",
      "│    14                                                                        │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "ImportError: cannot import name 'train' from 'ray.train' \n",
      "(/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/__init__.py)\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_4_mountcheckpoint_fault.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24f88513-39a7-419e-aed1-f96f4c884669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 04:50:47,198 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 04:50:48,591\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_7647f9fdbf1a9b38.zip.\n",
      "2025-05-09 04:50:48,592\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_BgFSsjhAzKqCzVak' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_BgFSsjhAzKqCzVak\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_BgFSsjhAzKqCzVak\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_BgFSsjhAzKqCzVak\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "python: can't open file 'longformer_4_mountcheckpoint_fault_!.py': [Errno 2] No such file or directory\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_BgFSsjhAzKqCzVak' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 2, last available logs (truncated to 20,000 chars):\n",
      "python: can't open file 'longformer_4_mountcheckpoint_fault_!.py': [Errno 2] No such file or directory\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_4_mountcheckpoint_fault_!.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cecd020d-d311-40a1-b8d0-392d54a3f5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 04:51:38,734 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 04:51:39,997\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_80d511f49cfebdd0.zip.\n",
      "2025-05-09 04:51:39,998\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_z7Sk9GyXF37bbLDh' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_z7Sk9GyXF37bbLDh\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_z7Sk9GyXF37bbLDh\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_z7Sk9GyXF37bbLDh\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "python: can't open file 'longformer_4_mountcheckpoint_fault_1.py': [Errno 2] No such file or directory\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_z7Sk9GyXF37bbLDh' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 2, last available logs (truncated to 20,000 chars):\n",
      "python: can't open file 'longformer_4_mountcheckpoint_fault_1.py': [Errno 2] No such file or directory\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_4_mountcheckpoint_fault_1.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14f045f3-37b1-4b92-b4d0-47f052f5aba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 04:52:00,648 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 04:52:01,878\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_e4db236c7544e18a.zip.\n",
      "2025-05-09 04:52:01,879\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_yS8Ej5HWtQnXwCme' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_yS8Ej5HWtQnXwCme\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_yS8Ej5HWtQnXwCme\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_yS8Ej5HWtQnXwCme\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_e4db236c7544e18a/longformer_5_mountcheckpoint_fault_1.py:14   │\n",
      "│ in <module>                                                                  │\n",
      "│                                                                              │\n",
      "│    11 from ray.train import ScalingConfig, Checkpoint                        │\n",
      "│    12 from ray.data import from_items                                        │\n",
      "│    13 from ray.train import RunConfig, FailureConfig                         │\n",
      "│ ❱  14 from ray.train import init                                             │\n",
      "│    15                                                                        │\n",
      "│    16 def train_model():                                                     │\n",
      "│    17 │   os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://129.114.26.235:8000\"   │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "ImportError: cannot import name 'init' from 'ray.train' \n",
      "(/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/__init__.py)\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_yS8Ej5HWtQnXwCme' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "│    11 from ray.train import ScalingConfig, Checkpoint                        │\n",
      "│    12 from ray.data import from_items                                        │\n",
      "│    13 from ray.train import RunConfig, FailureConfig                         │\n",
      "│ ❱  14 from ray.train import init                                             │\n",
      "│    15                                                                        │\n",
      "│    16 def train_model():                                                     │\n",
      "│    17 │   os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://129.114.26.235:8000\"   │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "ImportError: cannot import name 'init' from 'ray.train' \n",
      "(/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/__init__.py)\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_5_mountcheckpoint_fault_1.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab0c3083-95ea-4ea1-9ebb-f514176ae2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 04:55:04,139 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 04:55:05,379\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_4f3b33988c6aed27.zip.\n",
      "2025-05-09 04:55:05,380\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_MkGZMkd8yVLXnFUK' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_MkGZMkd8yVLXnFUK\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_MkGZMkd8yVLXnFUK\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_MkGZMkd8yVLXnFUK\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_4f3b33988c6aed27/longformer_5_mountcheckpoint_fault_1.py:14   │\n",
      "│ in <module>                                                                  │\n",
      "│                                                                              │\n",
      "│    11 from ray.train import ScalingConfig, Checkpoint                        │\n",
      "│    12 from ray.data import from_items                                        │\n",
      "│    13 from ray.train import RunConfig, FailureConfig                         │\n",
      "│ ❱  14 from ray.train import init                                             │\n",
      "│    15                                                                        │\n",
      "│    16 def train_model():                                                     │\n",
      "│    17 │   os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://129.114.26.235:8000\"   │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "ImportError: cannot import name 'init' from 'ray.train' \n",
      "(/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/__init__.py)\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_MkGZMkd8yVLXnFUK' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "│    11 from ray.train import ScalingConfig, Checkpoint                        │\n",
      "│    12 from ray.data import from_items                                        │\n",
      "│    13 from ray.train import RunConfig, FailureConfig                         │\n",
      "│ ❱  14 from ray.train import init                                             │\n",
      "│    15                                                                        │\n",
      "│    16 def train_model():                                                     │\n",
      "│    17 │   os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://129.114.26.235:8000\"   │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "ImportError: cannot import name 'init' from 'ray.train' \n",
      "(/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/__init__.py)\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_5_mountcheckpoint_fault_1.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a5ce409-705b-4e77-811d-c7526a9f20e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 04:56:11,780 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 04:56:13,089\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_8c51f9dab25282b7.zip.\n",
      "2025-05-09 04:56:13,090\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_2wjjXgU7PJHjJCeK' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_2wjjXgU7PJHjJCeK\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_2wjjXgU7PJHjJCeK\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_2wjjXgU7PJHjJCeK\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/engine/base.py:146 in __init__                                        │\n",
      "│                                                                              │\n",
      "│    143 │   │                                                                 │\n",
      "│    144 │   │   if connection is None:                                        │\n",
      "│    145 │   │   │   try:                                                      │\n",
      "│ ❱  146 │   │   │   │   self._dbapi_connection = engine.raw_connection()      │\n",
      "│    147 │   │   │   except dialect.loaded_dbapi.Error as err:                 │\n",
      "│    148 │   │   │   │   Connection._handle_dbapi_exception_noconnection(      │\n",
      "│    149 │   │   │   │   │   err, dialect, engine                              │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/engine/base.py:3298 in raw_connection                                 │\n",
      "│                                                                              │\n",
      "│   3295 │   │   │   :ref:`dbapi_connections`                                  │\n",
      "│   3296 │   │                                                                 │\n",
      "│   3297 │   │   \"\"\"                                                           │\n",
      "│ ❱ 3298 │   │   return self.pool.connect()                                    │\n",
      "│   3299                                                                       │\n",
      "│   3300                                                                       │\n",
      "│   3301 class OptionEngineMixin(log.Identified):                              │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:449 in connect                                           │\n",
      "│                                                                              │\n",
      "│    446 │   │   the pool.                                                     │\n",
      "│    447 │   │                                                                 │\n",
      "│    448 │   │   \"\"\"                                                           │\n",
      "│ ❱  449 │   │   return _ConnectionFairy._checkout(self)                       │\n",
      "│    450 │                                                                     │\n",
      "│    451 │   def _return_conn(self, record: ConnectionPoolEntry) -> None:      │\n",
      "│    452 │   │   \"\"\"Given a _ConnectionRecord, return it to the :class:`_pool. │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:1264 in _checkout                                        │\n",
      "│                                                                              │\n",
      "│   1261 │   │   fairy: Optional[_ConnectionFairy] = None,                     │\n",
      "│   1262 │   ) -> _ConnectionFairy:                                            │\n",
      "│   1263 │   │   if not fairy:                                                 │\n",
      "│ ❱ 1264 │   │   │   fairy = _ConnectionRecord.checkout(pool)                  │\n",
      "│   1265 │   │   │                                                             │\n",
      "│   1266 │   │   │   if threadconns is not None:                               │\n",
      "│   1267 │   │   │   │   threadconns.current = weakref.ref(fairy)              │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:713 in checkout                                          │\n",
      "│                                                                              │\n",
      "│    710 │   │   if TYPE_CHECKING:                                             │\n",
      "│    711 │   │   │   rec = cast(_ConnectionRecord, pool._do_get())             │\n",
      "│    712 │   │   else:                                                         │\n",
      "│ ❱  713 │   │   │   rec = pool._do_get()                                      │\n",
      "│    714 │   │                                                                 │\n",
      "│    715 │   │   try:                                                          │\n",
      "│    716 │   │   │   dbapi_connection = rec.get_connection()                   │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/impl.py:180 in _do_get                                           │\n",
      "│                                                                              │\n",
      "│   177 │   │   │   │   return self._create_connection()                       │\n",
      "│   178 │   │   │   except:                                                    │\n",
      "│   179 │   │   │   │   with util.safe_reraise():                              │\n",
      "│ ❱ 180 │   │   │   │   │   self._dec_overflow()                               │\n",
      "│   181 │   │   │   │   raise                                                  │\n",
      "│   182 │   │   else:                                                          │\n",
      "│   183 │   │   │   return self._do_get()                                      │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/util/langhelpers.py:146 in __exit__                                   │\n",
      "│                                                                              │\n",
      "│    143 │   │   │   exc_type, exc_value, exc_tb = self._exc_info              │\n",
      "│    144 │   │   │   assert exc_value is not None                              │\n",
      "│    145 │   │   │   self._exc_info = None  # remove potential circular refere │\n",
      "│ ❱  146 │   │   │   raise exc_value.with_traceback(exc_tb)                    │\n",
      "│    147 │   │   else:                                                         │\n",
      "│    148 │   │   │   self._exc_info = None  # remove potential circular refere │\n",
      "│    149 │   │   │   assert value is not None                                  │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/impl.py:177 in _do_get                                           │\n",
      "│                                                                              │\n",
      "│   174 │   │                                                                  │\n",
      "│   175 │   │   if self._inc_overflow():                                       │\n",
      "│   176 │   │   │   try:                                                       │\n",
      "│ ❱ 177 │   │   │   │   return self._create_connection()                       │\n",
      "│   178 │   │   │   except:                                                    │\n",
      "│   179 │   │   │   │   with util.safe_reraise():                              │\n",
      "│   180 │   │   │   │   │   self._dec_overflow()                               │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:390 in _create_connection                                │\n",
      "│                                                                              │\n",
      "│    387 │   def _create_connection(self) -> ConnectionPoolEntry:              │\n",
      "│    388 │   │   \"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"  │\n",
      "│    389 │   │                                                                 │\n",
      "│ ❱  390 │   │   return _ConnectionRecord(self)                                │\n",
      "│    391 │                                                                     │\n",
      "│    392 │   def _invalidate(                                                  │\n",
      "│    393 │   │   self,                                                         │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:675 in __init__                                          │\n",
      "│                                                                              │\n",
      "│    672 │   │                                                                 │\n",
      "│    673 │   │   self.__pool = pool                                            │\n",
      "│    674 │   │   if connect:                                                   │\n",
      "│ ❱  675 │   │   │   self.__connect()                                          │\n",
      "│    676 │   │   self.finalize_callback = deque()                              │\n",
      "│    677 │                                                                     │\n",
      "│    678 │   dbapi_connection: Optional[DBAPIConnection]                       │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:902 in __connect                                         │\n",
      "│                                                                              │\n",
      "│    899 │   │   │   self.fresh = True                                         │\n",
      "│    900 │   │   except BaseException as e:                                    │\n",
      "│    901 │   │   │   with util.safe_reraise():                                 │\n",
      "│ ❱  902 │   │   │   │   pool.logger.debug(\"Error on connect(): %s\", e)        │\n",
      "│    903 │   │   else:                                                         │\n",
      "│    904 │   │   │   # in SQLAlchemy 1.4 the first_connect event is not used b │\n",
      "│    905 │   │   │   # the engine, so this will usually not be set             │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/util/langhelpers.py:146 in __exit__                                   │\n",
      "│                                                                              │\n",
      "│    143 │   │   │   exc_type, exc_value, exc_tb = self._exc_info              │\n",
      "│    144 │   │   │   assert exc_value is not None                              │\n",
      "│    145 │   │   │   self._exc_info = None  # remove potential circular refere │\n",
      "│ ❱  146 │   │   │   raise exc_value.with_traceback(exc_tb)                    │\n",
      "│    147 │   │   else:                                                         │\n",
      "│    148 │   │   │   self._exc_info = None  # remove potential circular refere │\n",
      "│    149 │   │   │   assert value is not None                                  │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:897 in __connect                                         │\n",
      "│                                                                              │\n",
      "│    894 │   │   self.dbapi_connection = None                                  │\n",
      "│    895 │   │   try:                                                          │\n",
      "│    896 │   │   │   self.starttime = time.time()                              │\n",
      "│ ❱  897 │   │   │   self.dbapi_connection = connection = pool._invoke_creator │\n",
      "│    898 │   │   │   pool.logger.debug(\"Created new connection %r\", connection │\n",
      "│    899 │   │   │   self.fresh = True                                         │\n",
      "│    900 │   │   except BaseException as e:                                    │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/engine/create.py:646 in connect                                       │\n",
      "│                                                                              │\n",
      "│   643 │   │   │   │   │   if connection is not None:                         │\n",
      "│   644 │   │   │   │   │   │   return connection                              │\n",
      "│   645 │   │   │                                                              │\n",
      "│ ❱ 646 │   │   │   return dialect.connect(*cargs, **cparams)                  │\n",
      "│   647 │   │                                                                  │\n",
      "│   648 │   │   creator = pop_kwarg(\"creator\", connect)                        │\n",
      "│   649                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/engine/default.py:625 in connect                                      │\n",
      "│                                                                              │\n",
      "│    622 │                                                                     │\n",
      "│    623 │   def connect(self, *cargs: Any, **cparams: Any) -> DBAPIConnection │\n",
      "│    624 │   │   # inherits the docstring from interfaces.Dialect.connect      │\n",
      "│ ❱  625 │   │   return self.loaded_dbapi.connect(*cargs, **cparams)  # type:  │\n",
      "│    626 │                                                                     │\n",
      "│    627 │   def create_connect_args(self, url: URL) -> ConnectArgsType:       │\n",
      "│    628 │   │   # inherits the docstring from interfaces.Dialect.create_conne │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/psyc │\n",
      "│ opg2/__init__.py:122 in connect                                              │\n",
      "│                                                                              │\n",
      "│   119 │   │   kwasync['async_'] = kwargs.pop('async_')                       │\n",
      "│   120 │                                                                      │\n",
      "│   121 │   dsn = _ext.make_dsn(dsn, **kwargs)                                 │\n",
      "│ ❱ 122 │   conn = _connect(dsn, connection_factory=connection_factory, **kwas │\n",
      "│   123 │   if cursor_factory is not None:                                     │\n",
      "│   124 │   │   conn.cursor_factory = cursor_factory                           │\n",
      "│   125                                                                        │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "OperationalError: connection to server at \"129.114.26.235\", port 5432 failed: \n",
      "Connection timed out\n",
      "        Is the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_8c51f9dab25282b7/longformer_5_mountcheckpoint_fault_1.py:145  │\n",
      "│ in <module>                                                                  │\n",
      "│                                                                              │\n",
      "│   142 │   trainer.fit()                                                      │\n",
      "│   143                                                                        │\n",
      "│   144 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 145 │   train_model()                                                      │\n",
      "│   146                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_8c51f9dab25282b7/longformer_5_mountcheckpoint_fault_1.py:38   │\n",
      "│ in train_model                                                               │\n",
      "│                                                                              │\n",
      "│    35 │   WHERE query_phrases IS NOT NULL AND LENGTH(TRIM(query_phrases)) >  │\n",
      "│    36 │   LIMIT 100                                                          │\n",
      "│    37 │   \"\"\")                                                               │\n",
      "│ ❱  38 │   with engine.connect() as conn:                                     │\n",
      "│    39 │   │   df = pd.read_sql(query, conn)                                  │\n",
      "│    40 │                                                                      │\n",
      "│    41 │   train_examples = []                                                │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/engine/base.py:3274 in connect                                        │\n",
      "│                                                                              │\n",
      "│   3271 │   │                                                                 │\n",
      "│   3272 │   │   \"\"\"                                                           │\n",
      "│   3273 │   │                                                                 │\n",
      "│ ❱ 3274 │   │   return self._connection_cls(self)                             │\n",
      "│   3275 │                                                                     │\n",
      "│   3276 │   def raw_connection(self) -> PoolProxiedConnection:                │\n",
      "│   3277 │   │   \"\"\"Return a \"raw\" DBAPI connection from the connection pool.  │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/engine/base.py:148 in __init__                                        │\n",
      "│                                                                              │\n",
      "│    145 │   │   │   try:                                                      │\n",
      "│    146 │   │   │   │   self._dbapi_connection = engine.raw_connection()      │\n",
      "│    147 │   │   │   except dialect.loaded_dbapi.Error as err:                 │\n",
      "│ ❱  148 │   │   │   │   Connection._handle_dbapi_exception_noconnection(      │\n",
      "│    149 │   │   │   │   │   err, dialect, engine                              │\n",
      "│    150 │   │   │   │   )                                                     │\n",
      "│    151 │   │   │   │   raise                                                 │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/engine/base.py:2439 in _handle_dbapi_exception_noconnection           │\n",
      "│                                                                              │\n",
      "│   2436 │   │   │   raise newraise.with_traceback(exc_info[2]) from e         │\n",
      "│   2437 │   │   elif should_wrap:                                             │\n",
      "│   2438 │   │   │   assert sqlalchemy_exception is not None                   │\n",
      "│ ❱ 2439 │   │   │   raise sqlalchemy_exception.with_traceback(exc_info[2]) fr │\n",
      "│   2440 │   │   else:                                                         │\n",
      "│   2441 │   │   │   assert exc_info[1] is not None                            │\n",
      "│   2442 │   │   │   raise exc_info[1].with_traceback(exc_info[2])             │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/engine/base.py:146 in __init__                                        │\n",
      "│                                                                              │\n",
      "│    143 │   │                                                                 │\n",
      "│    144 │   │   if connection is None:                                        │\n",
      "│    145 │   │   │   try:                                                      │\n",
      "│ ❱  146 │   │   │   │   self._dbapi_connection = engine.raw_connection()      │\n",
      "│    147 │   │   │   except dialect.loaded_dbapi.Error as err:                 │\n",
      "│    148 │   │   │   │   Connection._handle_dbapi_exception_noconnection(      │\n",
      "│    149 │   │   │   │   │   err, dialect, engine                              │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/engine/base.py:3298 in raw_connection                                 │\n",
      "│                                                                              │\n",
      "│   3295 │   │   │   :ref:`dbapi_connections`                                  │\n",
      "│   3296 │   │                                                                 │\n",
      "│   3297 │   │   \"\"\"                                                           │\n",
      "│ ❱ 3298 │   │   return self.pool.connect()                                    │\n",
      "│   3299                                                                       │\n",
      "│   3300                                                                       │\n",
      "│   3301 class OptionEngineMixin(log.Identified):                              │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:449 in connect                                           │\n",
      "│                                                                              │\n",
      "│    446 │   │   the pool.                                                     │\n",
      "│    447 │   │                                                                 │\n",
      "│    448 │   │   \"\"\"                                                           │\n",
      "│ ❱  449 │   │   return _ConnectionFairy._checkout(self)                       │\n",
      "│    450 │                                                                     │\n",
      "│    451 │   def _return_conn(self, record: ConnectionPoolEntry) -> None:      │\n",
      "│    452 │   │   \"\"\"Given a _ConnectionRecord, return it to the :class:`_pool. │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:1264 in _checkout                                        │\n",
      "│                                                                              │\n",
      "│   1261 │   │   fairy: Optional[_ConnectionFairy] = None,                     │\n",
      "│   1262 │   ) -> _ConnectionFairy:                                            │\n",
      "│   1263 │   │   if not fairy:                                                 │\n",
      "│ ❱ 1264 │   │   │   fairy = _ConnectionRecord.checkout(pool)                  │\n",
      "│   1265 │   │   │                                                             │\n",
      "│   1266 │   │   │   if threadconns is not None:                               │\n",
      "│   1267 │   │   │   │   threadconns.current = weakref.ref(fairy)              │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:713 in checkout                                          │\n",
      "│                                                                              │\n",
      "│    710 │   │   if TYPE_CHECKING:                                             │\n",
      "│    711 │   │   │   rec = cast(_ConnectionRecord, pool._do_get())             │\n",
      "│    712 │   │   else:                                                         │\n",
      "│ ❱  713 │   │   │   rec = pool._do_get()                                      │\n",
      "│    714 │   │                                                                 │\n",
      "│    715 │   │   try:                                                          │\n",
      "│    716 │   │   │   dbapi_connection = rec.get_connection()                   │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/impl.py:180 in _do_get                                           │\n",
      "│                                                                              │\n",
      "│   177 │   │   │   │   return self._create_connection()                       │\n",
      "│   178 │   │   │   except:                                                    │\n",
      "│   179 │   │   │   │   with util.safe_reraise():                              │\n",
      "│ ❱ 180 │   │   │   │   │   self._dec_overflow()                               │\n",
      "│   181 │   │   │   │   raise                                                  │\n",
      "│   182 │   │   else:                                                          │\n",
      "│   183 │   │   │   return self._do_get()                                      │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/util/langhelpers.py:146 in __exit__                                   │\n",
      "│                                                                              │\n",
      "│    143 │   │   │   exc_type, exc_value, exc_tb = self._exc_info              │\n",
      "│    144 │   │   │   assert exc_value is not None                              │\n",
      "│    145 │   │   │   self._exc_info = None  # remove potential circular refere │\n",
      "│ ❱  146 │   │   │   raise exc_value.with_traceback(exc_tb)                    │\n",
      "│    147 │   │   else:                                                         │\n",
      "│    148 │   │   │   self._exc_info = None  # remove potential circular refere │\n",
      "│    149 │   │   │   assert value is not None                                  │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/impl.py:177 in _do_get                                           │\n",
      "│                                                                              │\n",
      "│   174 │   │                                                                  │\n",
      "│   175 │   │   if self._inc_overflow():                                       │\n",
      "│   176 │   │   │   try:                                                       │\n",
      "│ ❱ 177 │   │   │   │   return self._create_connection()                       │\n",
      "│   178 │   │   │   except:                                                    │\n",
      "│   179 │   │   │   │   with util.safe_reraise():                              │\n",
      "│   180 │   │   │   │   │   self._dec_overflow()                               │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:390 in _create_connection                                │\n",
      "│                                                                              │\n",
      "│    387 │   def _create_connection(self) -> ConnectionPoolEntry:              │\n",
      "│    388 │   │   \"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"  │\n",
      "│    389 │   │                                                                 │\n",
      "│ ❱  390 │   │   return _ConnectionRecord(self)                                │\n",
      "│    391 │                                                                     │\n",
      "│    392 │   def _invalidate(                                                  │\n",
      "│    393 │   │   self,                                                         │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:675 in __init__                                          │\n",
      "│                                                                              │\n",
      "│    672 │   │                                                                 │\n",
      "│    673 │   │   self.__pool = pool                                            │\n",
      "│    674 │   │   if connect:                                                   │\n",
      "│ ❱  675 │   │   │   self.__connect()                                          │\n",
      "│    676 │   │   self.finalize_callback = deque()                              │\n",
      "│    677 │                                                                     │\n",
      "│    678 │   dbapi_connection: Optional[DBAPIConnection]                       │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:902 in __connect                                         │\n",
      "│                                                                              │\n",
      "│    899 │   │   │   self.fresh = True                                         │\n",
      "│    900 │   │   except BaseException as e:                                    │\n",
      "│    901 │   │   │   with util.safe_reraise():                                 │\n",
      "│ ❱  902 │   │   │   │   pool.logger.debug(\"Error on connect(): %s\", e)        │\n",
      "│    903 │   │   else:                                                         │\n",
      "│    904 │   │   │   # in SQLAlchemy 1.4 the first_connect event is not used b │\n",
      "│    905 │   │   │   # the engine, so this will usually not be set             │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/util/langhelpers.py:146 in __exit__                                   │\n",
      "│                                                                              │\n",
      "│    143 │   │   │   exc_type, exc_value, exc_tb = self._exc_info              │\n",
      "│    144 │   │   │   assert exc_value is not None                              │\n",
      "│    145 │   │   │   self._exc_info = None  # remove potential circular refere │\n",
      "│ ❱  146 │   │   │   raise exc_value.with_traceback(exc_tb)                    │\n",
      "│    147 │   │   else:                                                         │\n",
      "│    148 │   │   │   self._exc_info = None  # remove potential circular refere │\n",
      "│    149 │   │   │   assert value is not None                                  │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/pool/base.py:897 in __connect                                         │\n",
      "│                                                                              │\n",
      "│    894 │   │   self.dbapi_connection = None                                  │\n",
      "│    895 │   │   try:                                                          │\n",
      "│    896 │   │   │   self.starttime = time.time()                              │\n",
      "│ ❱  897 │   │   │   self.dbapi_connection = connection = pool._invoke_creator │\n",
      "│    898 │   │   │   pool.logger.debug(\"Created new connection %r\", connection │\n",
      "│    899 │   │   │   self.fresh = True                                         │\n",
      "│    900 │   │   except BaseException as e:                                    │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/engine/create.py:646 in connect                                       │\n",
      "│                                                                              │\n",
      "│   643 │   │   │   │   │   if connection is not None:                         │\n",
      "│   644 │   │   │   │   │   │   return connection                              │\n",
      "│   645 │   │   │                                                              │\n",
      "│ ❱ 646 │   │   │   return dialect.connect(*cargs, **cparams)                  │\n",
      "│   647 │   │                                                                  │\n",
      "│   648 │   │   creator = pop_kwarg(\"creator\", connect)                        │\n",
      "│   649                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sqla │\n",
      "│ lchemy/engine/default.py:625 in connect                                      │\n",
      "│                                                                              │\n",
      "│    622 │                                                                     │\n",
      "│    623 │   def connect(self, *cargs: Any, **cparams: Any) -> DBAPIConnection │\n",
      "│    624 │   │   # inherits the docstring from interfaces.Dialect.connect      │\n",
      "│ ❱  625 │   │   return self.loaded_dbapi.connect(*cargs, **cparams)  # type:  │\n",
      "│    626 │                                                                     │\n",
      "│    627 │   def create_connect_args(self, url: URL) -> ConnectArgsType:       │\n",
      "│    628 │   │   # inherits the docstring from interfaces.Dialect.create_conne │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef │\n",
      "│ 91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/psyc │\n",
      "│ opg2/__init__.py:122 in connect                                              │\n",
      "│                                                                              │\n",
      "│   119 │   │   kwasync['async_'] = kwargs.pop('async_')                       │\n",
      "│   120 │                                                                      │\n",
      "│   121 │   dsn = _ext.make_dsn(dsn, **kwargs)                                 │\n",
      "│ ❱ 122 │   conn = _connect(dsn, connection_factory=connection_factory, **kwas │\n",
      "│   123 │   if cursor_factory is not None:                                     │\n",
      "│   124 │   │   conn.cursor_factory = cursor_factory                           │\n",
      "│   125                                                                        │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "OperationalError: (psycopg2.OperationalError) connection to server at \n",
      "\"129.114.26.235\", port 5432 failed: Connection timed out\n",
      "        Is the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_2wjjXgU7PJHjJCeK' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "│ ❱ 122 │   conn = _connect(dsn, connection_factory=connection_factory, **kwas │\n",
      "│   123 │   if cursor_factory is not None:                                     │\n",
      "│   124 │   │   conn.cursor_factory = cursor_factory                           │\n",
      "│   125                                                                        │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "OperationalError: (psycopg2.OperationalError) connection to server at \n",
      "\"129.114.26.235\", port 5432 failed: Connection timed out\n",
      "        Is the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_5_mountcheckpoint_fault_1.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d2d27b3-bcde-4c45-afe9-160743e53a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 05:01:32,356 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 05:01:33,519\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_5bc2c1de7d3c9a95.zip.\n",
      "2025-05-09 05:01:33,520\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_Mz8vb1pUyArLxpNU' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_Mz8vb1pUyArLxpNU\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_Mz8vb1pUyArLxpNU\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_Mz8vb1pUyArLxpNU\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 22:01:40,631\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 22:01:40,631\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 22:01:40,648\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 22:01:41,395\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_22-01-41\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_22-01-41`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=11356, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=11246, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=11246, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=11356) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=11454, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=11356, ip=10.233.120.184)\u001b[0m \n",
      "Downloading config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]\n",
      "Downloading config.json: 100%|██████████| 694/694 [00:00<00:00, 38.0kB/s]\n",
      "2025-05-08 22:09:22,454\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_b228a_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ConnectionError): \u001b[36mray::_Inner.train()\u001b[39m (pid=11246, ip=10.233.120.184, actor_id=e42abe5859492e84e544323703000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(ConnectionError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=11356, ip=10.233.120.184, actor_id=484fc05eb15c47e9873e546803000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7612f463bc70>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_5_mountcheckpoint_fault_1.py\", line 76, in train_func\n",
      "    word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py\", line 29, in __init__\n",
      "    self._load_model(model_name_or_path, config, cache_dir)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py\", line 49, in _load_model\n",
      "    self.auto_model = AutoModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 471, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2450, in from_pretrained\n",
      "    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformers/utils/hub.py\", line 409, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1364, in hf_hub_download\n",
      "    http_get(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 505, in http_get\n",
      "    r = _request_wrapper(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 442, in _request_wrapper\n",
      "    return http_backoff(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py\", line 212, in http_backoff\n",
      "    response = session.request(method=method, url=url, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Max retries exceeded with url: /xet-bridge-us/621ffdc136468d709f178e76/b8f960923351575e1e641a8f113b4434998d820e02eff218286700aceb2cb342?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250509%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250509T050922Z&X-Amz-Expires=3600&X-Amz-Signature=1042961bb72db7707c4608735ebf35e8d6300d7f5bfa83561996e446ab2a215d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&x-id=GetObject&Expires=1746770962&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Njc3MDk2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMTM2NDY4ZDcwOWYxNzhlNzYvYjhmOTYwOTIzMzUxNTc1ZTFlNjQxYThmMTEzYjQ0MzQ5OThkODIwZTAyZWZmMjE4Mjg2NzAwYWNlYjJjYjM0MioifV19&Signature=g4P-XU8e2IHcGmjPwnXLE1JQWkFRgLub~Ch3uFup6EIynmRDS6iAY7emxE1u5eViZQZ151wIRTsrgqt6cid3xP9SqYQLhZT2a20XzujRyP7zV00n9M8Az2TgfKW7UCoUzSEEonCPTq5kAzqyq4~oP6Wh9voRBaZF~OG765EQD8C6xaDrQtktoA23gyLxeZIL0tcQ5X9ADQlZwKeJnZ13~L7SGd2KvWel-qg6gDzz6CS8rmm7n-dScNIy0o5EI5s7jIIK3MZNP6CouZNrag1GCpmZUudN-r9soNevquL-RFf2h8F2DhD21Pp2wM45SaCIQoaeTxVA8vtH7ShijFUyIA__&Key-Pair-Id=K2L8F4GPSG1IFC (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7611a3494c70>: Failed to establish a new connection: [Errno -5] No address associated with hostname'))\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 22:09:22. Total running time: 7min 41s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_22-01-41/TorchTrainer_b228a_00000_0_2025-05-08_22-01-41/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=11443, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=11349, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=11349, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=11443) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=11558, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=11443, ip=10.233.120.183)\u001b[0m \n",
      "Downloading config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]\n",
      "Downloading config.json: 100%|██████████| 694/694 [00:00<00:00, 35.0kB/s]\n",
      "2025-05-08 22:09:32,032\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_b228a_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ConnectionError): \u001b[36mray::_Inner.train()\u001b[39m (pid=11349, ip=10.233.120.183, actor_id=6dd3151a0ceb7da6bef0cb7003000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(ConnectionError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=11443, ip=10.233.120.183, actor_id=bce4422fe001401712d6bb1d03000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7730d457bc40>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_5_mountcheckpoint_fault_1.py\", line 76, in train_func\n",
      "    word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py\", line 29, in __init__\n",
      "    self._load_model(model_name_or_path, config, cache_dir)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py\", line 49, in _load_model\n",
      "    self.auto_model = AutoModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 471, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2450, in from_pretrained\n",
      "    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformers/utils/hub.py\", line 409, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1364, in hf_hub_download\n",
      "    http_get(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 505, in http_get\n",
      "    r = _request_wrapper(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 442, in _request_wrapper\n",
      "    return http_backoff(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py\", line 212, in http_backoff\n",
      "    response = session.request(method=method, url=url, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Max retries exceeded with url: /xet-bridge-us/621ffdc136468d709f178e76/b8f960923351575e1e641a8f113b4434998d820e02eff218286700aceb2cb342?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250509%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250509T050931Z&X-Amz-Expires=3600&X-Amz-Signature=10bddc915080eb199ed32de094f5b250f863466ea0c1b90f024705bbe62361dc&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&x-id=GetObject&Expires=1746770971&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Njc3MDk3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMTM2NDY4ZDcwOWYxNzhlNzYvYjhmOTYwOTIzMzUxNTc1ZTFlNjQxYThmMTEzYjQ0MzQ5OThkODIwZTAyZWZmMjE4Mjg2NzAwYWNlYjJjYjM0MioifV19&Signature=MlOes2BoA05U0uVs60F1H~5ngTQaPZN7T9767aJC6cD21W9z~oFQe0wY11goTtt~EjcTM4tynB8~Fy37tV15S1Fa40jQrYp~PWMVu5MbLtPoXmbbX1Ec~kxDRACDEmAexEDY3ijPBDCX3aQpwjt19znRIq5TpkH8~0t8S8XCyZNOkRxcB9CwSpKb-24BFfBLVZEVIZdkrDmrz0RX7YiDX86YjEaGEjZ9wiKVcIyDSUiwMfFBCNI1WUUavFERpM~9B-6mVoyv7~uIej9zSHgQ1oZlpTpI1KyBqtYM7MAX34fLJaUqpBVmSvaeys-DmnwkYASah1RPFjspKwkBmkfyoA__&Key-Pair-Id=K2L8F4GPSG1IFC (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x772f743b1ca0>: Failed to establish a new connection: [Errno -5] No address associated with hostname'))\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 22:09:32. Total running time: 7min 50s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_22-01-41/TorchTrainer_b228a_00000_0_2025-05-08_22-01-41/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=11717, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=11623, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=11623, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=11717) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=11830, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "2025-05-08 22:09:40,395\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_b228a_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ConnectionError): \u001b[36mray::_Inner.train()\u001b[39m (pid=11623, ip=10.233.120.184, actor_id=e77ca2136b9bf15588ccf0a303000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(ConnectionError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=11717, ip=10.233.120.184, actor_id=e2bf6819e62d39d71c52f8d303000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7771700a2b80>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_5_mountcheckpoint_fault_1.py\", line 76, in train_func\n",
      "    word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py\", line 29, in __init__\n",
      "    self._load_model(model_name_or_path, config, cache_dir)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py\", line 49, in _load_model\n",
      "    self.auto_model = AutoModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 471, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2450, in from_pretrained\n",
      "    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformers/utils/hub.py\", line 409, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1364, in hf_hub_download\n",
      "    http_get(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 505, in http_get\n",
      "    r = _request_wrapper(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 442, in _request_wrapper\n",
      "    return http_backoff(\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py\", line 212, in http_backoff\n",
      "    response = session.request(method=method, url=url, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Max retries exceeded with url: /xet-bridge-us/621ffdc136468d709f178e76/b8f960923351575e1e641a8f113b4434998d820e02eff218286700aceb2cb342?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250509%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250509T050940Z&X-Amz-Expires=3600&X-Amz-Signature=fa3fd23a162f50e14e3e3d8ee40b7862741df40c7bcd92e2b053d956c516117f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&x-id=GetObject&Expires=1746770980&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Njc3MDk4MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMTM2NDY4ZDcwOWYxNzhlNzYvYjhmOTYwOTIzMzUxNTc1ZTFlNjQxYThmMTEzYjQ0MzQ5OThkODIwZTAyZWZmMjE4Mjg2NzAwYWNlYjJjYjM0MioifV19&Signature=gTgaciGvBuohUpwUixQmOhhjfivqSuFYCtTNYqL2WkFY~aB-fGvKYEbP9qkdON4ufwZ~YjDEaFJg4~PLiuZbjQtQKIvEHWpUIQKbnPgbA8HPm-q3KTz6Vpa7jyGBzz1c5wAVBFbS0USHTsbgdRikgmG5hrFYX7jq8qwScl5~dOzzwlruZDm~drPTqdR-Z1NOqooTPq3dR5RGf0X7kXAdRoGlMOBofeTz~v3pelAEL9M1F5khZuoYcLOETxjJ2FuIVoSHm1YQ3y6FWQ25PPt6ZbuqvB1exmTiuFjWS2Jr~cmxtBXrD6Q5d1TcPuCiuTiaDz9-XhAc5sUsLFpHJAPfRA__&Key-Pair-Id=K2L8F4GPSG1IFC (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7770076bc790>: Failed to establish a new connection: [Errno -5] No address associated with hostname'))\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 22:09:40. Total running time: 7min 58s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_22-01-41/TorchTrainer_b228a_00000_0_2025-05-08_22-01-41/error.txt\n",
      "\n",
      "2025-05-08 22:09:40,408\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_b228a_00000]\n",
      "RayTaskError(ConnectionError): \u001b[36mray::_Inner.train()\u001b[39m (pid=11623, \n",
      "ip=10.233.120.184, actor_id=e77ca2136b9bf15588ccf0a303000000, repr=TorchTrainer)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\n",
      "\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(ConnectionError): \n",
      "\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=11717, ip=10.233.120.184,\n",
      "actor_id=e2bf6819e62d39d71c52f8d303000000, \n",
      "repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7771700a2b80>)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_grou\n",
      "p.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_5_mountcheckpoint_fault_1.py\", line 76, in train_func\n",
      "    word_embedding_model = models.Transformer(model_name, \n",
      "max_seq_length=max_seq_length)\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sentence_tr\n",
      "ansformers/models/Transformer.py\", line 29, in __init__\n",
      "    self._load_model(model_name_or_path, config, cache_dir)\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/sentence_tr\n",
      "ansformers/models/Transformer.py\", line 49, in _load_model\n",
      "    self.auto_model = AutoModel.from_pretrained(model_name_or_path, \n",
      "config=config, cache_dir=cache_dir)\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformer\n",
      "s/models/auto/auto_factory.py\", line 471, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformer\n",
      "s/modeling_utils.py\", line 2450, in from_pretrained\n",
      "    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename,\n",
      "**cached_file_kwargs)\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/transformer\n",
      "s/utils/hub.py\", line 409, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface\n",
      "_hub/utils/_validators.py\", line 120, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface\n",
      "_hub/file_download.py\", line 1364, in hf_hub_download\n",
      "    http_get(\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface\n",
      "_hub/file_download.py\", line 505, in http_get\n",
      "    r = _request_wrapper(\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface\n",
      "_hub/file_download.py\", line 442, in _request_wrapper\n",
      "    return http_backoff(\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/huggingface\n",
      "_hub/utils/_http.py\", line 212, in http_backoff\n",
      "    response = session.request(method=method, url=url, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", \n",
      "line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", \n",
      "line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", \n",
      "line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: \n",
      "HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Max retries \n",
      "exceeded with url: \n",
      "/xet-bridge-us/621ffdc136468d709f178e76/b8f960923351575e1e641a8f113b4434998d820e\n",
      "02eff218286700aceb2cb342?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=U\n",
      "NSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250509%2Fus-east-1%2Fs3%2Faws4_request&\n",
      "X-Amz-Date=20250509T050940Z&X-Amz-Expires=3600&X-Amz-Signature=fa3fd23a162f50e14\n",
      "e3e3d8ee40b7862741df40c7bcd92e2b053d956c516117f&X-Amz-SignedHeaders=host&X-Xet-C\n",
      "as-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pyto\n",
      "rch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=app\n",
      "lication%2Foctet-stream&x-id=GetObject&Expires=1746770980&Policy=eyJTdGF0ZW1lbnQ\n",
      "iOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Njc3MDk4MH1\n",
      "9LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy8\n",
      "2MjFmZmRjMTM2NDY4ZDcwOWYxNzhlNzYvYjhmOTYwOTIzMzUxNTc1ZTFlNjQxYThmMTEzYjQ0MzQ5OTh\n",
      "kODIwZTAyZWZmMjE4Mjg2NzAwYWNlYjJjYjM0MioifV19&Signature=gTgaciGvBuohUpwUixQmOhhj\n",
      "fivqSuFYCtTNYqL2WkFY~aB-fGvKYEbP9qkdON4ufwZ~YjDEaFJg4~PLiuZbjQtQKIvEHWpUIQKbnPgb\n",
      "A8HPm-q3KTz6Vpa7jyGBzz1c5wAVBFbS0USHTsbgdRikgmG5hrFYX7jq8qwScl5~dOzzwlruZDm~drPT\n",
      "qdR-Z1NOqooTPq3dR5RGf0X7kXAdRoGlMOBofeTz~v3pelAEL9M1F5khZuoYcLOETxjJ2FuIVoSHm1YQ\n",
      "3y6FWQ25PPt6ZbuqvB1exmTiuFjWS2Jr~cmxtBXrD6Q5d1TcPuCiuTiaDz9-XhAc5sUsLFpHJAPfRA__\n",
      "&Key-Pair-Id=K2L8F4GPSG1IFC (Caused by \n",
      "NewConnectionError('<urllib3.connection.HTTPSConnection object at \n",
      "0x7770076bc790>: Failed to establish a new connection: [Errno -5] No address \n",
      "associated with hostname'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_5bc2c1de7d3c9a95/longformer_5_mountcheckpoint_fault_1.py:145  │\n",
      "│ in <module>                                                                  │\n",
      "│                                                                              │\n",
      "│   142 │   trainer.fit()                                                      │\n",
      "│   143                                                                        │\n",
      "│   144 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 145 │   train_model()                                                      │\n",
      "│   146                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_5bc2c1de7d3c9a95/longformer_5_mountcheckpoint_fault_1.py:142  │\n",
      "│ in train_model                                                               │\n",
      "│                                                                              │\n",
      "│   139 │   │   )                                                              │\n",
      "│   140 │   )                                                                  │\n",
      "│   141 │                                                                      │\n",
      "│ ❱ 142 │   trainer.fit()                                                      │\n",
      "│   143                                                                        │\n",
      "│   144 if __name__ == \"__main__\":                                             │\n",
      "│   145 │   train_model()                                                      │\n",
      "│                                                                              │\n",
      "│ /home/ray/anaconda3/lib/python3.8/site-packages/ray/train/base_trainer.py:64 │\n",
      "│ 0 in fit                                                                     │\n",
      "│                                                                              │\n",
      "│   637 │   │   if result.error:                                               │\n",
      "│   638 │   │   │   # Raise trainable errors to the user with a message to res │\n",
      "│   639 │   │   │   # or configure `FailureConfig` in a new run.               │\n",
      "│ ❱ 640 │   │   │   raise TrainingFailedError(                                 │\n",
      "│   641 │   │   │   │   \"\\n\".join([restore_msg, TrainingFailedError._FAILURE_C │\n",
      "│   642 │   │   │   ) from result.error                                        │\n",
      "│   643 │   │   return result                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_22-01-41\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_Mz8vb1pUyArLxpNU' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_22-01-41\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_5_mountcheckpoint_fault_1.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fad74c7-5ebd-4e35-876d-55f79305b4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 05:48:20,064 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 05:48:21,328\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_1d3584142837486b.zip.\n",
      "2025-05-09 05:48:21,330\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_BujwvSQNmF935GEC' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_BujwvSQNmF935GEC\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_BujwvSQNmF935GEC\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_BujwvSQNmF935GEC\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 22:48:27,570\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 22:48:27,571\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 22:48:27,587\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 22:48:28,401\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_22-48-28\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_22-48-28`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=20054, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=19944, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=19944, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=20054) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=20152, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=20054, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "\u001b[36m(RayTrainWorker pid=20054, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=20054, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 22:48:39,669\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_3b469_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=19944, ip=10.233.120.183, actor_id=2062325921e81228deb7695506000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=20054, ip=10.233.120.183, actor_id=84c1d51ebc53cc3e3c77eba806000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x73939c156be0>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 85, in train_func\n",
      "    dataloader = DataLoader(get_context().get_dataset_shard(\"train_examples\"), shuffle=True, batch_size=batch_size)\n",
      "AttributeError: 'TrainContext' object has no attribute 'get_dataset_shard'\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 22:48:39. Total running time: 11s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_22-48-28/TorchTrainer_3b469_00000_0_2025-05-08_22-48-28/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=20289, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=20179, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=20179, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=20289) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=20387, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=20289, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "\u001b[36m(RayTrainWorker pid=20289, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=20289, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 22:48:50,854\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_3b469_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=20179, ip=10.233.120.184, actor_id=c506d8e3e4b49bbc48598f4106000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=20289, ip=10.233.120.184, actor_id=b8ea86e8a3cd66c17607f39906000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7d462421abe0>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 85, in train_func\n",
      "    dataloader = DataLoader(get_context().get_dataset_shard(\"train_examples\"), shuffle=True, batch_size=batch_size)\n",
      "AttributeError: 'TrainContext' object has no attribute 'get_dataset_shard'\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 22:48:50. Total running time: 22s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_22-48-28/TorchTrainer_3b469_00000_0_2025-05-08_22-48-28/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=20431, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=20321, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=20321, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=20431) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=20528, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=20431, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "\u001b[36m(RayTrainWorker pid=20431, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=20431, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 22:49:01,004\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_3b469_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=20321, ip=10.233.120.183, actor_id=198e3fe7a7a9e2eae0e7012e06000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=20431, ip=10.233.120.183, actor_id=54a2c7a421d5fde1e897afb106000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7c33bc1d2c70>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 85, in train_func\n",
      "    dataloader = DataLoader(get_context().get_dataset_shard(\"train_examples\"), shuffle=True, batch_size=batch_size)\n",
      "AttributeError: 'TrainContext' object has no attribute 'get_dataset_shard'\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 22:49:01. Total running time: 32s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_22-48-28/TorchTrainer_3b469_00000_0_2025-05-08_22-48-28/error.txt\n",
      "\n",
      "2025-05-08 22:49:01,015\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_3b469_00000]\n",
      "RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=20321, \n",
      "ip=10.233.120.183, actor_id=198e3fe7a7a9e2eae0e7012e06000000, repr=TorchTrainer)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\n",
      "\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \n",
      "\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=20431, ip=10.233.120.183,\n",
      "actor_id=54a2c7a421d5fde1e897afb106000000, \n",
      "repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7c33bc1d2c70>)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_grou\n",
      "p.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 85, in train_func\n",
      "    dataloader = DataLoader(get_context().get_dataset_shard(\"train_examples\"), \n",
      "shuffle=True, batch_size=batch_size)\n",
      "AttributeError: 'TrainContext' object has no attribute 'get_dataset_shard'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_1d3584142837486b/longformer_6_mountcheckpoint_fault_2.py:145  │\n",
      "│ in <module>                                                                  │\n",
      "│                                                                              │\n",
      "│   142 │   trainer.fit()                                                      │\n",
      "│   143                                                                        │\n",
      "│   144 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 145 │   train_model()                                                      │\n",
      "│   146                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_1d3584142837486b/longformer_6_mountcheckpoint_fault_2.py:142  │\n",
      "│ in train_model                                                               │\n",
      "│                                                                              │\n",
      "│   139 │   │   )                                                              │\n",
      "│   140 │   )                                                                  │\n",
      "│   141 │                                                                      │\n",
      "│ ❱ 142 │   trainer.fit()                                                      │\n",
      "│   143                                                                        │\n",
      "│   144 if __name__ == \"__main__\":                                             │\n",
      "│   145 │   train_model()                                                      │\n",
      "│                                                                              │\n",
      "│ /home/ray/anaconda3/lib/python3.8/site-packages/ray/train/base_trainer.py:64 │\n",
      "│ 0 in fit                                                                     │\n",
      "│                                                                              │\n",
      "│   637 │   │   if result.error:                                               │\n",
      "│   638 │   │   │   # Raise trainable errors to the user with a message to res │\n",
      "│   639 │   │   │   # or configure `FailureConfig` in a new run.               │\n",
      "│ ❱ 640 │   │   │   raise TrainingFailedError(                                 │\n",
      "│   641 │   │   │   │   \"\\n\".join([restore_msg, TrainingFailedError._FAILURE_C │\n",
      "│   642 │   │   │   ) from result.error                                        │\n",
      "│   643 │   │   return result                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_22-48-28\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_BujwvSQNmF935GEC' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_22-48-28\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_6_mountcheckpoint_fault_2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f2ffce3-5d60-4368-adb1-5235d4e9099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:00:29,944 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:00:31,188\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_6f6ef3baf5f4c7db.zip.\n",
      "2025-05-09 06:00:31,189\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_pcXBUuDaMx2Nyiza' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_pcXBUuDaMx2Nyiza\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_pcXBUuDaMx2Nyiza\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_pcXBUuDaMx2Nyiza\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 23:00:37,110\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 23:00:37,111\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 23:00:37,129\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 23:00:38,260\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_23-00-38\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_23-00-38`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=22979, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=22869, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22869, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=22979) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=23076, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=22979, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "\u001b[36m(RayTrainWorker pid=22979, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=22979, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:00:49,685\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_ee4f3_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=22869, ip=10.233.120.184, actor_id=4a1cdb0d3ccc69d4c770863207000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=22979, ip=10.233.120.184, actor_id=b2f9ba7d0e6145faebf9bc6407000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x76025c087ca0>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 93, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 351, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 106, in __init__\n",
      "    if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 114, in num_samples\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type 'StreamSplitDataIterator' has no len()\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:00:49. Total running time: 11s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-00-38/TorchTrainer_ee4f3_00000_0_2025-05-08_23-00-38/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=23122, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=23012, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=23012, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=23122) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=23219, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=23122, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "\u001b[36m(RayTrainWorker pid=23122, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=23122, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:01:00,661\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_ee4f3_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=23012, ip=10.233.120.183, actor_id=f074ab46a641049c8fb7b64b07000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=23122, ip=10.233.120.183, actor_id=41d1ad6b3374db7fcb45532f07000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7ba0a02dbb80>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 93, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 351, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 106, in __init__\n",
      "    if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 114, in num_samples\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type 'StreamSplitDataIterator' has no len()\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:01:00. Total running time: 22s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-00-38/TorchTrainer_ee4f3_00000_0_2025-05-08_23-00-38/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=23355, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=23245, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=23245, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=23355) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=23452, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=23355, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "\u001b[36m(RayTrainWorker pid=23355, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=23355, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:01:11,275\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_ee4f3_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=23245, ip=10.233.120.184, actor_id=5df7c234f3623fd4f9595ad107000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=23355, ip=10.233.120.184, actor_id=68a0974868fc8dd67f3a8faa07000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x75d7780d9c40>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 93, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 351, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 106, in __init__\n",
      "    if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 114, in num_samples\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type 'StreamSplitDataIterator' has no len()\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:01:11. Total running time: 32s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-00-38/TorchTrainer_ee4f3_00000_0_2025-05-08_23-00-38/error.txt\n",
      "\n",
      "2025-05-08 23:01:11,288\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_ee4f3_00000]\n",
      "RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=23245, \n",
      "ip=10.233.120.184, actor_id=5df7c234f3623fd4f9595ad107000000, repr=TorchTrainer)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\n",
      "\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \n",
      "\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=23355, ip=10.233.120.184,\n",
      "actor_id=68a0974868fc8dd67f3a8faa07000000, \n",
      "repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x75d7780d9c40>)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_grou\n",
      "p.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 93, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils\n",
      "/data/dataloader.py\", line 351, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: \n",
      "ignore[arg-type]\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "10402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils\n",
      "/data/sampler.py\", line 106, in __init__\n",
      "    if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils\n",
      "/data/sampler.py\", line 114, in num_samples\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type 'StreamSplitDataIterator' has no len()\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_6f6ef3baf5f4c7db/longformer_6_mountcheckpoint_fault_2.py:154  │\n",
      "│ in <module>                                                                  │\n",
      "│                                                                              │\n",
      "│   151 │   trainer.fit()                                                      │\n",
      "│   152                                                                        │\n",
      "│   153 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 154 │   train_model()                                                      │\n",
      "│   155                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_6f6ef3baf5f4c7db/longformer_6_mountcheckpoint_fault_2.py:151  │\n",
      "│ in train_model                                                               │\n",
      "│                                                                              │\n",
      "│   148 │   │   )                                                              │\n",
      "│   149 │   )                                                                  │\n",
      "│   150 │                                                                      │\n",
      "│ ❱ 151 │   trainer.fit()                                                      │\n",
      "│   152                                                                        │\n",
      "│   153 if __name__ == \"__main__\":                                             │\n",
      "│   154 │   train_model()                                                      │\n",
      "│                                                                              │\n",
      "│ /home/ray/anaconda3/lib/python3.8/site-packages/ray/train/base_trainer.py:64 │\n",
      "│ 0 in fit                                                                     │\n",
      "│                                                                              │\n",
      "│   637 │   │   if result.error:                                               │\n",
      "│   638 │   │   │   # Raise trainable errors to the user with a message to res │\n",
      "│   639 │   │   │   # or configure `FailureConfig` in a new run.               │\n",
      "│ ❱ 640 │   │   │   raise TrainingFailedError(                                 │\n",
      "│   641 │   │   │   │   \"\\n\".join([restore_msg, TrainingFailedError._FAILURE_C │\n",
      "│   642 │   │   │   ) from result.error                                        │\n",
      "│   643 │   │   return result                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-00-38\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_pcXBUuDaMx2Nyiza' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-00-38\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_6_mountcheckpoint_fault_2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4295d212-649b-49bf-ac9d-e375660a8459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:02:15,530 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:02:16,710\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_aa6f5e363a9e4c59.zip.\n",
      "2025-05-09 06:02:16,711\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_nMKY3UzkJ5jndfap' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_nMKY3UzkJ5jndfap\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_nMKY3UzkJ5jndfap\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_nMKY3UzkJ5jndfap\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 23:02:22,999\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 23:02:23,000\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 23:02:23,012\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 23:02:23,891\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_23-02-23\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_23-02-23`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=23807, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=23697, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=23697, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=23807) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=23905, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=23807, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "\u001b[36m(RayTrainWorker pid=23807, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=23807, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:02:36,098\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_2d452_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=23697, ip=10.233.120.183, actor_id=654eeb8995d79ba079f9ea8108000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=23807, ip=10.233.120.183, actor_id=0024fb7ea07a31d2e2d02a2d08000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x77eb8839bb80>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 102, in train_func\n",
      "    model.set_logger(custom_logger)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1614, in __getattr__\n",
      "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "AttributeError: 'SentenceTransformer' object has no attribute 'set_logger'\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:02:36. Total running time: 12s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-02-23/TorchTrainer_2d452_00000_0_2025-05-08_23-02-23/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=24039, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=23945, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=23945, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=24039) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=24153, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=24039, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "\u001b[36m(RayTrainWorker pid=24039, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=24039, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:02:48,442\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_2d452_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=23945, ip=10.233.120.184, actor_id=6751d2936c65ea6fa2b0efcc08000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=24039, ip=10.233.120.184, actor_id=87f80e66172b0424eac52aaa08000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x76ff2c25abe0>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 102, in train_func\n",
      "    model.set_logger(custom_logger)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1614, in __getattr__\n",
      "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "AttributeError: 'SentenceTransformer' object has no attribute 'set_logger'\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:02:48. Total running time: 24s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-02-23/TorchTrainer_2d452_00000_0_2025-05-08_23-02-23/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=24200, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=24090, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=24090, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=24200) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=24297, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=24200, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "\u001b[36m(RayTrainWorker pid=24200, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=24200, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:02:59,658\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_2d452_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=24090, ip=10.233.120.183, actor_id=901f434985f8751cb27857a308000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=24200, ip=10.233.120.183, actor_id=0e1a430eef4736c64ad2c14e08000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x72b4547bcbb0>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 102, in train_func\n",
      "    model.set_logger(custom_logger)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1614, in __getattr__\n",
      "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "AttributeError: 'SentenceTransformer' object has no attribute 'set_logger'\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:02:59. Total running time: 35s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-02-23/TorchTrainer_2d452_00000_0_2025-05-08_23-02-23/error.txt\n",
      "\n",
      "2025-05-08 23:02:59,671\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_2d452_00000]\n",
      "RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=24090, \n",
      "ip=10.233.120.183, actor_id=901f434985f8751cb27857a308000000, repr=TorchTrainer)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\n",
      "\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \n",
      "\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=24200, ip=10.233.120.183,\n",
      "actor_id=0e1a430eef4736c64ad2c14e08000000, \n",
      "repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x72b4547bcbb0>)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_grou\n",
      "p.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_6_mountcheckpoint_fault_2.py\", line 102, in train_func\n",
      "    model.set_logger(custom_logger)\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/nn/mo\n",
      "dules/module.py\", line 1614, in __getattr__\n",
      "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "AttributeError: 'SentenceTransformer' object has no attribute 'set_logger'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_aa6f5e363a9e4c59/longformer_6_mountcheckpoint_fault_2.py:154  │\n",
      "│ in <module>                                                                  │\n",
      "│                                                                              │\n",
      "│   151 │   trainer.fit()                                                      │\n",
      "│   152                                                                        │\n",
      "│   153 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 154 │   train_model()                                                      │\n",
      "│   155                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_aa6f5e363a9e4c59/longformer_6_mountcheckpoint_fault_2.py:151  │\n",
      "│ in train_model                                                               │\n",
      "│                                                                              │\n",
      "│   148 │   │   )                                                              │\n",
      "│   149 │   )                                                                  │\n",
      "│   150 │                                                                      │\n",
      "│ ❱ 151 │   trainer.fit()                                                      │\n",
      "│   152                                                                        │\n",
      "│   153 if __name__ == \"__main__\":                                             │\n",
      "│   154 │   train_model()                                                      │\n",
      "│                                                                              │\n",
      "│ /home/ray/anaconda3/lib/python3.8/site-packages/ray/train/base_trainer.py:64 │\n",
      "│ 0 in fit                                                                     │\n",
      "│                                                                              │\n",
      "│   637 │   │   if result.error:                                               │\n",
      "│   638 │   │   │   # Raise trainable errors to the user with a message to res │\n",
      "│   639 │   │   │   # or configure `FailureConfig` in a new run.               │\n",
      "│ ❱ 640 │   │   │   raise TrainingFailedError(                                 │\n",
      "│   641 │   │   │   │   \"\\n\".join([restore_msg, TrainingFailedError._FAILURE_C │\n",
      "│   642 │   │   │   ) from result.error                                        │\n",
      "│   643 │   │   return result                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-02-23\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_nMKY3UzkJ5jndfap' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-02-23\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_6_mountcheckpoint_fault_2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ca3e1d7-0800-43c3-9b1a-daabf12fb13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:08:29,388 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:08:30,663\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_94aef06d0acc7618.zip.\n",
      "2025-05-09 06:08:30,664\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_TZWNSuTRNCxnrdfG' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_TZWNSuTRNCxnrdfG\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_TZWNSuTRNCxnrdfG\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_TZWNSuTRNCxnrdfG\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 23:08:37,362\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 23:08:37,362\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 23:08:37,380\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 23:08:38,331\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_23-08-38\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_23-08-38`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=25588, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=25478, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=25478, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=25588) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=25686, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=25588, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "\u001b[36m(RayTrainWorker pid=25588, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=25588, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:08:49,820\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_0c74e_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=25478, ip=10.233.120.184, actor_id=578e91a137e1d3ec42bcb8d509000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=25588, ip=10.233.120.184, actor_id=355f29e99d5bb2762f01372609000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x76b0d1f3fc10>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 86, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 351, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 106, in __init__\n",
      "    if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 114, in num_samples\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type 'StreamSplitDataIterator' has no len()\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:08:49. Total running time: 11s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-08-38/TorchTrainer_0c74e_00000_0_2025-05-08_23-08-38/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=25734, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=25624, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=25624, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=25734) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=25832, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=25734, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "\u001b[36m(RayTrainWorker pid=25734, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=25734, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:09:01,279\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_0c74e_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=25624, ip=10.233.120.183, actor_id=1bcc4ab65f75e77aee108e3709000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=25734, ip=10.233.120.183, actor_id=f98188523d0d310a819c8cb809000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x779a681d6c10>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 86, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 351, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 106, in __init__\n",
      "    if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 114, in num_samples\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type 'StreamSplitDataIterator' has no len()\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:09:01. Total running time: 22s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-08-38/TorchTrainer_0c74e_00000_0_2025-05-08_23-08-38/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=25964, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=25854, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=25854, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=25964) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=26076, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=25964, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "\u001b[36m(RayTrainWorker pid=25964, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=25964, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:09:11,381\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_0c74e_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=25854, ip=10.233.120.184, actor_id=8c0c51fb3a6c92b8c8f81a2009000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=25964, ip=10.233.120.184, actor_id=7708bbe377d4c072358825e709000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x71116c072c40>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 86, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 351, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 106, in __init__\n",
      "    if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 114, in num_samples\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type 'StreamSplitDataIterator' has no len()\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:09:11. Total running time: 32s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-08-38/TorchTrainer_0c74e_00000_0_2025-05-08_23-08-38/error.txt\n",
      "\n",
      "2025-05-08 23:09:11,392\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_0c74e_00000]\n",
      "RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=25854, \n",
      "ip=10.233.120.184, actor_id=8c0c51fb3a6c92b8c8f81a2009000000, repr=TorchTrainer)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\n",
      "\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \n",
      "\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=25964, ip=10.233.120.184,\n",
      "actor_id=7708bbe377d4c072358825e709000000, \n",
      "repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x71116c072c40>)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_grou\n",
      "p.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 86, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils\n",
      "/data/dataloader.py\", line 351, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: \n",
      "ignore[arg-type]\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils\n",
      "/data/sampler.py\", line 106, in __init__\n",
      "    if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils\n",
      "/data/sampler.py\", line 114, in num_samples\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type 'StreamSplitDataIterator' has no len()\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_94aef06d0acc7618/longformer_7.py:156 in <module>              │\n",
      "│                                                                              │\n",
      "│   153 │   trainer.fit()                                                      │\n",
      "│   154                                                                        │\n",
      "│   155 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 156 │   train_model()                                                      │\n",
      "│   157                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_94aef06d0acc7618/longformer_7.py:153 in train_model           │\n",
      "│                                                                              │\n",
      "│   150 │   │   )                                                              │\n",
      "│   151 │   )                                                                  │\n",
      "│   152 │                                                                      │\n",
      "│ ❱ 153 │   trainer.fit()                                                      │\n",
      "│   154                                                                        │\n",
      "│   155 if __name__ == \"__main__\":                                             │\n",
      "│   156 │   train_model()                                                      │\n",
      "│                                                                              │\n",
      "│ /home/ray/anaconda3/lib/python3.8/site-packages/ray/train/base_trainer.py:64 │\n",
      "│ 0 in fit                                                                     │\n",
      "│                                                                              │\n",
      "│   637 │   │   if result.error:                                               │\n",
      "│   638 │   │   │   # Raise trainable errors to the user with a message to res │\n",
      "│   639 │   │   │   # or configure `FailureConfig` in a new run.               │\n",
      "│ ❱ 640 │   │   │   raise TrainingFailedError(                                 │\n",
      "│   641 │   │   │   │   \"\\n\".join([restore_msg, TrainingFailedError._FAILURE_C │\n",
      "│   642 │   │   │   ) from result.error                                        │\n",
      "│   643 │   │   return result                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-08-38\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_TZWNSuTRNCxnrdfG' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-08-38\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_7.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "069db3c1-386a-4fc5-ac0c-84e2dcf72f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:11:29,675 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:11:30,802\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_a757913acc5335b4.zip.\n",
      "2025-05-09 06:11:30,803\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_kaheZmDvjm3hSMYm' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_kaheZmDvjm3hSMYm\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_kaheZmDvjm3hSMYm\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_kaheZmDvjm3hSMYm\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 23:11:37,287\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 23:11:37,287\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 23:11:37,305\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 23:11:38,099\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_23-11-38\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_23-11-38`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=26856, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=26746, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=26746, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=26856) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=26953, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=26856, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "\u001b[36m(RayTrainWorker pid=26856, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=26856, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:11:50,679\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_779a0_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(NameError): \u001b[36mray::_Inner.train()\u001b[39m (pid=26746, ip=10.233.120.184, actor_id=58be32ed23d595c680fd78840a000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(NameError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=26856, ip=10.233.120.184, actor_id=3410937ce4ec9f07c4c89a570a000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7e35a441bc70>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 86, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "NameError: name 'train_dataset' is not defined\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:11:50. Total running time: 12s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-11-38/TorchTrainer_779a0_00000_0_2025-05-08_23-11-38/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=26693, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=26582, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=26582, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=26693) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=26807, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=26693, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "\u001b[36m(RayTrainWorker pid=26693, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=26693, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:12:02,200\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_779a0_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(NameError): \u001b[36mray::_Inner.train()\u001b[39m (pid=26582, ip=10.233.120.183, actor_id=4931afa9bd3efcadae9f5f360a000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(NameError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=26693, ip=10.233.120.183, actor_id=f6bd8838ef6e057d1ee1de160a000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x712b3c31bb50>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 86, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "NameError: name 'train_dataset' is not defined\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:12:02. Total running time: 24s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-11-38/TorchTrainer_779a0_00000_0_2025-05-08_23-11-38/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=27234, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=27140, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=27140, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=27234) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=27346, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=27234, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "\u001b[36m(RayTrainWorker pid=27234, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=27234, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:12:12,405\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_779a0_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(NameError): \u001b[36mray::_Inner.train()\u001b[39m (pid=27140, ip=10.233.120.184, actor_id=d6306ed062895423fdad79e20a000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(NameError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=27234, ip=10.233.120.184, actor_id=6efc564def1f235507d718a30a000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7d998449bbe0>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 86, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "NameError: name 'train_dataset' is not defined\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:12:12. Total running time: 34s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-11-38/TorchTrainer_779a0_00000_0_2025-05-08_23-11-38/error.txt\n",
      "\n",
      "2025-05-08 23:12:12,416\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_779a0_00000]\n",
      "RayTaskError(NameError): \u001b[36mray::_Inner.train()\u001b[39m (pid=27140, \n",
      "ip=10.233.120.184, actor_id=d6306ed062895423fdad79e20a000000, repr=TorchTrainer)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\n",
      "\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(NameError): \n",
      "\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=27234, ip=10.233.120.184,\n",
      "actor_id=6efc564def1f235507d718a30a000000, \n",
      "repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7d998449bbe0>)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_grou\n",
      "p.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 86, in train_func\n",
      "    dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
      "NameError: name 'train_dataset' is not defined\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_a757913acc5335b4/longformer_7.py:156 in <module>              │\n",
      "│                                                                              │\n",
      "│   153 │   trainer.fit()                                                      │\n",
      "│   154                                                                        │\n",
      "│   155 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 156 │   train_model()                                                      │\n",
      "│   157                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_a757913acc5335b4/longformer_7.py:153 in train_model           │\n",
      "│                                                                              │\n",
      "│   150 │   │   )                                                              │\n",
      "│   151 │   )                                                                  │\n",
      "│   152 │                                                                      │\n",
      "│ ❱ 153 │   trainer.fit()                                                      │\n",
      "│   154                                                                        │\n",
      "│   155 if __name__ == \"__main__\":                                             │\n",
      "│   156 │   train_model()                                                      │\n",
      "│                                                                              │\n",
      "│ /home/ray/anaconda3/lib/python3.8/site-packages/ray/train/base_trainer.py:64 │\n",
      "│ 0 in fit                                                                     │\n",
      "│                                                                              │\n",
      "│   637 │   │   if result.error:                                               │\n",
      "│   638 │   │   │   # Raise trainable errors to the user with a message to res │\n",
      "│   639 │   │   │   # or configure `FailureConfig` in a new run.               │\n",
      "│ ❱ 640 │   │   │   raise TrainingFailedError(                                 │\n",
      "│   641 │   │   │   │   \"\\n\".join([restore_msg, TrainingFailedError._FAILURE_C │\n",
      "│   642 │   │   │   ) from result.error                                        │\n",
      "│   643 │   │   return result                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-11-38\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_kaheZmDvjm3hSMYm' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-11-38\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_7.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1be9dab-fdd4-4c2b-bbd2-68c8c3596cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:12:34,015 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:12:35,218\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_e9aef87937d5b3f0.zip.\n",
      "2025-05-09 06:12:35,219\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_BwKBNsyY59NscEhz' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_BwKBNsyY59NscEhz\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_BwKBNsyY59NscEhz\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_BwKBNsyY59NscEhz\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 23:12:41,719\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 23:12:41,720\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 23:12:41,737\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 23:12:42,669\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_23-12-42\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_23-12-42`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=27232, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=27138, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=27138, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=27232) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=27346, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=27232, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "\u001b[36m(RayTrainWorker pid=27232, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=27232, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36m(SplitCoordinator pid=27346, ip=10.233.120.183)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=27346, ip=10.233.120.183)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=27346, ip=10.233.120.183)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning 0:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.01 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.01 MiB/5.05 GiB object_store_memory:   0%|          | 1/200 [00:00<00:22,  8.85it/s]\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.02 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.85it/s]2025-05-08 23:12:54,552\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_9e16e_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=27138, ip=10.233.120.183, actor_id=3f207f12eaa2909215f100800b000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=27232, ip=10.233.120.183, actor_id=0bc29dc00fe65e276718f1a30b000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7145d4156c70>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 85, in train_func\n",
      "    train_dataset = list(raw_dataset.iter_torch_batches(batch_size=batch_size))\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 183, in _create_iterator\n",
      "    for batch in iterator:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 176, in iter_batches\n",
      "    next_batch = next(async_batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 167, in _async_iter_batches\n",
      "    yield from extract_data_from_batch(batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 210, in extract_data_from_batch\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 306, in restore_original_order\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 203, in finalize_batches\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 218, in threadpool_computations_format_collate\n",
      "    yield from formatted_batch_iter\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 181, in collate\n",
      "    collated_batch = collate_fn(batch.data)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 358, in collate_fn\n",
      "    return convert_ndarray_batch_to_torch_tensor_batch(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 240, in convert_ndarray_batch_to_torch_tensor_batch\n",
      "    batch = {\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 241, in <dictcomp>\n",
      "    col_name: convert_ndarray_to_torch_tensor(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 194, in convert_ndarray_to_torch_tensor\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Numpy array of object dtype cannot be converted to a Torch Tensor. This may because the numpy array is a ragged tensor--it contains items of different sizes. If using `iter_torch_batches()` API, you can pass in a `collate_fn` argument to specify custom logic to convert the Numpy array batch to a Torch tensor batch.\n",
      "\n",
      "                                                                                                                                                         \n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.02 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "Training errored after 0 iterations at 2025-05-08 23:12:54. Total running time: 11s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-12-42/TorchTrainer_9e16e_00000_0_2025-05-08_23-12-42/error.txt\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.02 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.85it/s]\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:03<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "Training started without custom configuration.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:03<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(RayTrainWorker pid=27789, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:06<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(TorchTrainer pid=27679, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:06<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(TorchTrainer pid=27679, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=27789) world_rank=0, local_rank=0, node_rank=0\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:06<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(SplitCoordinator pid=27886, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:08<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(RayTrainWorker pid=27789, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(RayTrainWorker pid=27789, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(RayTrainWorker pid=27789, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(SplitCoordinator pid=27886, ip=10.233.120.184)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(SplitCoordinator pid=27886, ip=10.233.120.184)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.85it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(SplitCoordinator pid=27886, ip=10.233.120.184)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning 0:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.01 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.01 MiB/5.05 GiB object_store_memory:   0%|          | 1/200 [00:00<00:22,  8.75it/s]\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.02 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.75it/s]\u001b[A2025-05-08 23:13:05,435\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_9e16e_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=27679, ip=10.233.120.184, actor_id=1e43707070443ddd9e01ef340b000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=27789, ip=10.233.120.184, actor_id=d762812e54d56c12bf9709c70b000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x77a6c873cbb0>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 85, in train_func\n",
      "    train_dataset = list(raw_dataset.iter_torch_batches(batch_size=batch_size))\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 183, in _create_iterator\n",
      "    for batch in iterator:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 176, in iter_batches\n",
      "    next_batch = next(async_batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 167, in _async_iter_batches\n",
      "    yield from extract_data_from_batch(batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 210, in extract_data_from_batch\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 306, in restore_original_order\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 203, in finalize_batches\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 218, in threadpool_computations_format_collate\n",
      "    yield from formatted_batch_iter\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 181, in collate\n",
      "    collated_batch = collate_fn(batch.data)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 358, in collate_fn\n",
      "    return convert_ndarray_batch_to_torch_tensor_batch(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 240, in convert_ndarray_batch_to_torch_tensor_batch\n",
      "    batch = {\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 241, in <dictcomp>\n",
      "    col_name: convert_ndarray_to_torch_tensor(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 194, in convert_ndarray_to_torch_tensor\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Numpy array of object dtype cannot be converted to a Torch Tensor. This may because the numpy array is a ragged tensor--it contains items of different sizes. If using `iter_torch_batches()` API, you can pass in a `collate_fn` argument to specify custom logic to convert the Numpy array batch to a Torch tensor batch.\n",
      "\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.02 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.75it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[ATraining errored after 0 iterations at 2025-05-08 23:13:05. Total running time: 22s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-12-42/TorchTrainer_9e16e_00000_0_2025-05-08_23-12-42/error.txt\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.02 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.75it/s]\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.75it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:14<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:03<00:22,  8.75it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[ATraining started without custom configuration.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:14<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:03<00:22,  8.75it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(RayTrainWorker pid=27631, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:16<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:05<00:22,  8.75it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(TorchTrainer pid=27521, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:16<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:06<00:22,  8.75it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(TorchTrainer pid=27521, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=27631) world_rank=0, local_rank=0, node_rank=0\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:16<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:06<00:22,  8.75it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(SplitCoordinator pid=27728, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:18<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:07<00:22,  8.75it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(RayTrainWorker pid=27631, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:21<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.75it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(RayTrainWorker pid=27631, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:21<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.75it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(RayTrainWorker pid=27631, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:21<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning 0:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                \n",
      "\u001b[A\u001b[A\u001b[36m(SplitCoordinator pid=27728, ip=10.233.120.183)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:21<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                \n",
      "\u001b[A\u001b[A\u001b[36m(SplitCoordinator pid=27728, ip=10.233.120.183)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:21<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                \n",
      "\u001b[A\u001b[A\u001b[36m(SplitCoordinator pid=27728, ip=10.233.120.183)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:21<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:22,  8.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.01 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.01 MiB/5.05 GiB object_store_memory:   0%|          | 1/200 [00:00<00:23,  8.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.02 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:23,  8.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:23,  8.52it/s]\u001b[A\u001b[A2025-05-08 23:13:16,456\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_9e16e_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=27521, ip=10.233.120.183, actor_id=f264f39dcf1da08ef51b71670b000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=27631, ip=10.233.120.183, actor_id=e8e252f232848028f286da110b000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7059d467cb50>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 85, in train_func\n",
      "    train_dataset = list(raw_dataset.iter_torch_batches(batch_size=batch_size))\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 183, in _create_iterator\n",
      "    for batch in iterator:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 176, in iter_batches\n",
      "    next_batch = next(async_batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 167, in _async_iter_batches\n",
      "    yield from extract_data_from_batch(batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 210, in extract_data_from_batch\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 306, in restore_original_order\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 203, in finalize_batches\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 218, in threadpool_computations_format_collate\n",
      "    yield from formatted_batch_iter\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 181, in collate\n",
      "    collated_batch = collate_fn(batch.data)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 358, in collate_fn\n",
      "    return convert_ndarray_batch_to_torch_tensor_batch(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 240, in convert_ndarray_batch_to_torch_tensor_batch\n",
      "    batch = {\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 241, in <dictcomp>\n",
      "    col_name: convert_ndarray_to_torch_tensor(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 194, in convert_ndarray_to_torch_tensor\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Numpy array of object dtype cannot be converted to a Torch Tensor. This may because the numpy array is a ragged tensor--it contains items of different sizes. If using `iter_torch_batches()` API, you can pass in a `collate_fn` argument to specify custom logic to convert the Numpy array batch to a Torch tensor batch.\n",
      "\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:22,  8.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:23,  8.52it/s]\u001b[A\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[ATraining errored after 0 iterations at 2025-05-08 23:13:16. Total running time: 33s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-12-42/TorchTrainer_9e16e_00000_0_2025-05-08_23-12-42/error.txt\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:22,  8.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:23,  8.52it/s]\u001b[A\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27346, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.85it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27886, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:22,  8.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27728, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:23,  8.52it/s]\u001b[A\u001b[A2025-05-08 23:13:16,473\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_9e16e_00000]\n",
      "RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=27521, \n",
      "ip=10.233.120.183, actor_id=f264f39dcf1da08ef51b71670b000000, repr=TorchTrainer)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\n",
      "\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(RuntimeError): \n",
      "\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=27631, ip=10.233.120.183,\n",
      "actor_id=e8e252f232848028f286da110b000000, \n",
      "repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7059d467cb50>)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_grou\n",
      "p.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 85, in train_func\n",
      "    train_dataset = list(raw_dataset.iter_torch_batches(batch_size=batch_size))\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", \n",
      "line 183, in _create_iterator\n",
      "    for batch in iterator:\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/iter_batches.py\", line 176, in iter_batches\n",
      "    next_batch = next(async_batch_iter)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", \n",
      "line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", \n",
      "line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/iter_batches.py\", line 167, in _async_iter_batches\n",
      "    yield from extract_data_from_batch(batch_iter)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/util.py\", line 210, in extract_data_from_batch\n",
      "    for batch in batch_iter:\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/iter_batches.py\", line 306, in restore_original_order\n",
      "    for batch in batch_iter:\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/util.py\", line 203, in finalize_batches\n",
      "    for batch in batch_iter:\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", \n",
      "line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", \n",
      "line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/iter_batches.py\", line 218, in threadpool_computations_format_collate\n",
      "    yield from formatted_batch_iter\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/util.py\", line 181, in collate\n",
      "    collated_batch = collate_fn(batch.data)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", \n",
      "line 358, in collate_fn\n",
      "    return convert_ndarray_batch_to_torch_tensor_batch(\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.p\n",
      "y\", line 240, in convert_ndarray_batch_to_torch_tensor_batch\n",
      "    batch = {\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.p\n",
      "y\", line 241, in <dictcomp>\n",
      "    col_name: convert_ndarray_to_torch_tensor(\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.p\n",
      "y\", line 194, in convert_ndarray_to_torch_tensor\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Numpy array of object dtype cannot be converted to a Torch Tensor.\n",
      "This may because the numpy array is a ragged tensor--it contains items of \n",
      "different sizes. If using `iter_torch_batches()` API, you can pass in a \n",
      "`collate_fn` argument to specify custom logic to convert the Numpy array batch \n",
      "to a Torch tensor batch.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_e9aef87937d5b3f0/longformer_7.py:156 in <module>              │\n",
      "│                                                                              │\n",
      "│   153 │   trainer.fit()                                                      │\n",
      "│   154                                                                        │\n",
      "│   155 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 156 │   train_model()                                                      │\n",
      "│   157                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_e9aef87937d5b3f0/longformer_7.py:153 in train_model           │\n",
      "│                                                                              │\n",
      "│   150 │   │   )                                                              │\n",
      "│   151 │   )                                                                  │\n",
      "│   152 │                                                                      │\n",
      "│ ❱ 153 │   trainer.fit()                                                      │\n",
      "│   154                                                                        │\n",
      "│   155 if __name__ == \"__main__\":                                             │\n",
      "│   156 │   train_model()                                                      │\n",
      "│                                                                              │\n",
      "│ /home/ray/anaconda3/lib/python3.8/site-packages/ray/train/base_trainer.py:64 │\n",
      "│ 0 in fit                                                                     │\n",
      "│                                                                              │\n",
      "│   637 │   │   if result.error:                                               │\n",
      "│   638 │   │   │   # Raise trainable errors to the user with a message to res │\n",
      "│   639 │   │   │   # or configure `FailureConfig` in a new run.               │\n",
      "│ ❱ 640 │   │   │   raise TrainingFailedError(                                 │\n",
      "│   641 │   │   │   │   \"\\n\".join([restore_msg, TrainingFailedError._FAILURE_C │\n",
      "│   642 │   │   │   ) from result.error                                        │\n",
      "│   643 │   │   return result                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-12-42\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                         \u001b[A\u001b[A\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_BwKBNsyY59NscEhz' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                         \u001b[A\u001b[A\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_7.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a72a387-6fc3-4a3d-836a-a9a6a99c6a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:14:38,603 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:14:39,783\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_9a793eaf52b65b9a.zip.\n",
      "2025-05-09 06:14:39,784\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_pWamTZabiYDAVEev' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_pWamTZabiYDAVEev\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_pWamTZabiYDAVEev\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_pWamTZabiYDAVEev\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 23:14:45,862\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 23:14:45,862\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 23:14:45,880\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 23:14:46,827\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_23-14-46\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_23-14-46`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=28525, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=28431, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=28431, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=28525) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=28639, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=28525, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "\u001b[36m(RayTrainWorker pid=28525, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=28525, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:14:58,296\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_e8182_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=28431, ip=10.233.120.184, actor_id=e8d278d28a5bc29670c1cd860c000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=28525, ip=10.233.120.184, actor_id=28a0cca15ce4f4c7051c53790c000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7219d863cc70>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_8.py\", line 84, in train_func\n",
      "    raw_dataset = get_dataset_shard(\"train_examples\").take_all()\n",
      "AttributeError: 'StreamSplitDataIterator' object has no attribute 'take_all'\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:14:58. Total running time: 11s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-14-46/TorchTrainer_e8182_00000_0_2025-05-08_23-14-46/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=28365, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=28271, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=28271, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=28365) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=28478, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=28365, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "\u001b[36m(RayTrainWorker pid=28365, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=28365, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:15:09,449\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_e8182_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=28271, ip=10.233.120.183, actor_id=f23fc8850f3a889d38bbc0110c000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=28365, ip=10.233.120.183, actor_id=1aaf6a0c287fd7b3952471190c000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x705e5035ba90>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_8.py\", line 84, in train_func\n",
      "    raw_dataset = get_dataset_shard(\"train_examples\").take_all()\n",
      "AttributeError: 'StreamSplitDataIterator' object has no attribute 'take_all'\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:15:09. Total running time: 22s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-14-46/TorchTrainer_e8182_00000_0_2025-05-08_23-14-46/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=28917, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=28807, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=28807, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=28917) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=29013, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=28917, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "\u001b[36m(RayTrainWorker pid=28917, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=28917, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-05-08 23:15:20,336\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_e8182_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=28807, ip=10.233.120.184, actor_id=1d2155ea0b4022d4e9dd9c1d0c000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=28917, ip=10.233.120.184, actor_id=821b1d176d0642e13225f8810c000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7c80c46fbca0>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_8.py\", line 84, in train_func\n",
      "    raw_dataset = get_dataset_shard(\"train_examples\").take_all()\n",
      "AttributeError: 'StreamSplitDataIterator' object has no attribute 'take_all'\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:15:20. Total running time: 33s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-14-46/TorchTrainer_e8182_00000_0_2025-05-08_23-14-46/error.txt\n",
      "\n",
      "2025-05-08 23:15:20,347\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_e8182_00000]\n",
      "RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=28807, \n",
      "ip=10.233.120.184, actor_id=1d2155ea0b4022d4e9dd9c1d0c000000, repr=TorchTrainer)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\n",
      "\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \n",
      "\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=28917, ip=10.233.120.184,\n",
      "actor_id=821b1d176d0642e13225f8810c000000, \n",
      "repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7c80c46fbca0>)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_grou\n",
      "p.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_8.py\", line 84, in train_func\n",
      "    raw_dataset = get_dataset_shard(\"train_examples\").take_all()\n",
      "AttributeError: 'StreamSplitDataIterator' object has no attribute 'take_all'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_9a793eaf52b65b9a/longformer_8.py:155 in <module>              │\n",
      "│                                                                              │\n",
      "│   152 │   trainer.fit()                                                      │\n",
      "│   153                                                                        │\n",
      "│   154 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 155 │   train_model()                                                      │\n",
      "│   156                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_9a793eaf52b65b9a/longformer_8.py:152 in train_model           │\n",
      "│                                                                              │\n",
      "│   149 │   │   )                                                              │\n",
      "│   150 │   )                                                                  │\n",
      "│   151 │                                                                      │\n",
      "│ ❱ 152 │   trainer.fit()                                                      │\n",
      "│   153                                                                        │\n",
      "│   154 if __name__ == \"__main__\":                                             │\n",
      "│   155 │   train_model()                                                      │\n",
      "│                                                                              │\n",
      "│ /home/ray/anaconda3/lib/python3.8/site-packages/ray/train/base_trainer.py:64 │\n",
      "│ 0 in fit                                                                     │\n",
      "│                                                                              │\n",
      "│   637 │   │   if result.error:                                               │\n",
      "│   638 │   │   │   # Raise trainable errors to the user with a message to res │\n",
      "│   639 │   │   │   # or configure `FailureConfig` in a new run.               │\n",
      "│ ❱ 640 │   │   │   raise TrainingFailedError(                                 │\n",
      "│   641 │   │   │   │   \"\\n\".join([restore_msg, TrainingFailedError._FAILURE_C │\n",
      "│   642 │   │   │   ) from result.error                                        │\n",
      "│   643 │   │   return result                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-14-46\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_pWamTZabiYDAVEev' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-14-46\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_8.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c1165c8-e9b2-4ab1-af3b-5b6b9866fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:20:33,723 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:20:34,872\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_c43e002ec4ba11f7.zip.\n",
      "2025-05-09 06:20:34,873\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_mZpkEgiHFmxGfNbw' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_mZpkEgiHFmxGfNbw\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_mZpkEgiHFmxGfNbw\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_mZpkEgiHFmxGfNbw\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 23:20:41,084\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 23:20:41,085\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 23:20:41,099\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 23:20:41,919\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_23-20-41\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_23-20-41`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=29849, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=29755, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=29755, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=29849) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=29962, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=29849, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "\u001b[36m(RayTrainWorker pid=29849, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=29849, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning 0:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "                                                                                                                                                \n",
      "\u001b[36m(SplitCoordinator pid=29962, ip=10.233.120.183)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "                                                                                                                                                \n",
      "\u001b[36m(SplitCoordinator pid=29962, ip=10.233.120.183)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "                                                                                                                                                \n",
      "\u001b[36m(SplitCoordinator pid=29962, ip=10.233.120.183)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.01 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.01 MiB/5.05 GiB object_store_memory:   0%|          | 1/200 [00:00<00:23,  8.64it/s]\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.02 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.64it/s]2025-05-08 23:20:53,225\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_bbbbd_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=29755, ip=10.233.120.183, actor_id=bc6145fcd11792e9d73d6b1f0d000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=29849, ip=10.233.120.183, actor_id=354a98773cf9e8f7f5ff66b00d000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x76cd380dbbb0>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 85, in train_func\n",
      "    train_dataset = list(raw_dataset.iter_torch_batches(batch_size=batch_size))\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 183, in _create_iterator\n",
      "    for batch in iterator:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 176, in iter_batches\n",
      "    next_batch = next(async_batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 167, in _async_iter_batches\n",
      "    yield from extract_data_from_batch(batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 210, in extract_data_from_batch\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 306, in restore_original_order\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 203, in finalize_batches\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 218, in threadpool_computations_format_collate\n",
      "    yield from formatted_batch_iter\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 181, in collate\n",
      "    collated_batch = collate_fn(batch.data)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 358, in collate_fn\n",
      "    return convert_ndarray_batch_to_torch_tensor_batch(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 240, in convert_ndarray_batch_to_torch_tensor_batch\n",
      "    batch = {\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 241, in <dictcomp>\n",
      "    col_name: convert_ndarray_to_torch_tensor(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 194, in convert_ndarray_to_torch_tensor\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Numpy array of object dtype cannot be converted to a Torch Tensor. This may because the numpy array is a ragged tensor--it contains items of different sizes. If using `iter_torch_batches()` API, you can pass in a `collate_fn` argument to specify custom logic to convert the Numpy array batch to a Torch tensor batch.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "Training errored after 0 iterations at 2025-05-08 23:20:53. Total running time: 11s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-20-41/TorchTrainer_bbbbd_00000_0_2025-05-08_23-20-41/error.txt\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:03<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "Training started without custom configuration.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:03<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(RayTrainWorker pid=30403, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:06<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(TorchTrainer pid=30293, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:06<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(TorchTrainer pid=30293, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=30403) world_rank=0, local_rank=0, node_rank=0\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:06<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(SplitCoordinator pid=30501, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:08<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(RayTrainWorker pid=30403, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(RayTrainWorker pid=30403, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(RayTrainWorker pid=30403, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(SplitCoordinator pid=30501, ip=10.233.120.184)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(SplitCoordinator pid=30501, ip=10.233.120.184)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:22,  8.64it/s]\n",
      "                                                                                                                                                         \n",
      "\u001b[36m(SplitCoordinator pid=30501, ip=10.233.120.184)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning 0:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 1/200 [00:00<00:23,  8.56it/s]\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.01 MiB/5.05 GiB object_store_memory:   0%|          | 1/200 [00:00<00:23,  8.56it/s]\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.02 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:23,  8.56it/s]\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:23,  8.56it/s]\u001b[A2025-05-08 23:21:05,161\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_bbbbd_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=30293, ip=10.233.120.184, actor_id=42fd50482428f3746a2d62c50d000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=30403, ip=10.233.120.184, actor_id=2bc932260dbb88641c6158c20d000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7bf35c41bc70>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 85, in train_func\n",
      "    train_dataset = list(raw_dataset.iter_torch_batches(batch_size=batch_size))\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 183, in _create_iterator\n",
      "    for batch in iterator:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 176, in iter_batches\n",
      "    next_batch = next(async_batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 167, in _async_iter_batches\n",
      "    yield from extract_data_from_batch(batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 210, in extract_data_from_batch\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 306, in restore_original_order\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 203, in finalize_batches\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 218, in threadpool_computations_format_collate\n",
      "    yield from formatted_batch_iter\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 181, in collate\n",
      "    collated_batch = collate_fn(batch.data)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 358, in collate_fn\n",
      "    return convert_ndarray_batch_to_torch_tensor_batch(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 240, in convert_ndarray_batch_to_torch_tensor_batch\n",
      "    batch = {\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 241, in <dictcomp>\n",
      "    col_name: convert_ndarray_to_torch_tensor(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 194, in convert_ndarray_to_torch_tensor\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Numpy array of object dtype cannot be converted to a Torch Tensor. This may because the numpy array is a ragged tensor--it contains items of different sizes. If using `iter_torch_batches()` API, you can pass in a `collate_fn` argument to specify custom logic to convert the Numpy array batch to a Torch tensor batch.\n",
      "\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:12<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[ATraining errored after 0 iterations at 2025-05-08 23:21:05. Total running time: 23s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-20-41/TorchTrainer_bbbbd_00000_0_2025-05-08_23-20-41/error.txt\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:12<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:15<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:03<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[ATraining started without custom configuration.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:15<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:03<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(RayTrainWorker pid=30246, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:17<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:06<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(TorchTrainer pid=30136, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:17<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:06<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(TorchTrainer pid=30136, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=30246) world_rank=0, local_rank=0, node_rank=0\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:17<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:06<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(SplitCoordinator pid=30343, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:19<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:07<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(RayTrainWorker pid=30246, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(RayTrainWorker pid=30246, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(RayTrainWorker pid=30246, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(SplitCoordinator pid=30343, ip=10.233.120.183)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(SplitCoordinator pid=30343, ip=10.233.120.183)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:23,  8.56it/s]\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[36m(SplitCoordinator pid=30343, ip=10.233.120.183)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:23,  8.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30343, ip=10.233.120.183) \u001b[0mRunning 0:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30343, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.0 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30343, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.01 MiB/5.05 GiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30343, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.01 MiB/5.05 GiB object_store_memory:   0%|          | 1/200 [00:00<00:23,  8.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30343, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.02 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30343, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.58it/s]\u001b[A\u001b[A2025-05-08 23:21:15,713\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_bbbbd_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=30136, ip=10.233.120.183, actor_id=7995cc22cee090ea15d75ac30d000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=30246, ip=10.233.120.183, actor_id=c4ad8bb63fe70722dc9317fa0d000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7828343dbc70>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 85, in train_func\n",
      "    train_dataset = list(raw_dataset.iter_torch_batches(batch_size=batch_size))\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 183, in _create_iterator\n",
      "    for batch in iterator:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 176, in iter_batches\n",
      "    next_batch = next(async_batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 167, in _async_iter_batches\n",
      "    yield from extract_data_from_batch(batch_iter)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 210, in extract_data_from_batch\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 306, in restore_original_order\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 203, in finalize_batches\n",
      "    for batch in batch_iter:\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py\", line 218, in threadpool_computations_format_collate\n",
      "    yield from formatted_batch_iter\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py\", line 181, in collate\n",
      "    collated_batch = collate_fn(batch.data)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", line 358, in collate_fn\n",
      "    return convert_ndarray_batch_to_torch_tensor_batch(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 240, in convert_ndarray_batch_to_torch_tensor_batch\n",
      "    batch = {\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 241, in <dictcomp>\n",
      "    col_name: convert_ndarray_to_torch_tensor(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py\", line 194, in convert_ndarray_to_torch_tensor\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Numpy array of object dtype cannot be converted to a Torch Tensor. This may because the numpy array is a ragged tensor--it contains items of different sizes. If using `iter_torch_batches()` API, you can pass in a `collate_fn` argument to specify custom logic to convert the Numpy array batch to a Torch tensor batch.\n",
      "\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:23,  8.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30343, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.58it/s]\u001b[A\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[ATraining errored after 0 iterations at 2025-05-08 23:21:15. Total running time: 33s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-20-41/TorchTrainer_bbbbd_00000_0_2025-05-08_23-20-41/error.txt\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:10<00:23,  8.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30343, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.58it/s]\u001b[A\u001b[A\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                         \n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29962, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:22<00:22,  8.64it/s]\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30501, ip=10.233.120.184) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:11<00:23,  8.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30343, ip=10.233.120.183) \u001b[0mRunning: 0.0/33.0 CPU, 0.0/1.0 GPU, 0.03 MiB/5.05 GiB object_store_memory:   2%|▏         | 3/200 [00:00<00:22,  8.58it/s]\u001b[A\u001b[A2025-05-08 23:21:15,728\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_bbbbd_00000]\n",
      "RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=30136, \n",
      "ip=10.233.120.183, actor_id=7995cc22cee090ea15d75ac30d000000, repr=TorchTrainer)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\n",
      "\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(RuntimeError): \n",
      "\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=30246, ip=10.233.120.183,\n",
      "actor_id=c4ad8bb63fe70722dc9317fa0d000000, \n",
      "repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7828343dbc70>)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_grou\n",
      "p.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_7.py\", line 85, in train_func\n",
      "    train_dataset = list(raw_dataset.iter_torch_batches(batch_size=batch_size))\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", \n",
      "line 183, in _create_iterator\n",
      "    for batch in iterator:\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/iter_batches.py\", line 176, in iter_batches\n",
      "    next_batch = next(async_batch_iter)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", \n",
      "line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", \n",
      "line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/iter_batches.py\", line 167, in _async_iter_batches\n",
      "    yield from extract_data_from_batch(batch_iter)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/util.py\", line 210, in extract_data_from_batch\n",
      "    for batch in batch_iter:\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/iter_batches.py\", line 306, in restore_original_order\n",
      "    for batch in batch_iter:\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/util.py\", line 203, in finalize_batches\n",
      "    for batch in batch_iter:\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", \n",
      "line 926, in make_async_gen\n",
      "    raise next_item\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/util.py\", \n",
      "line 903, in execute_computation\n",
      "    for item in fn(thread_safe_generator):\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/iter_batches.py\", line 218, in threadpool_computations_format_collate\n",
      "    yield from formatted_batch_iter\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/block_batchi\n",
      "ng/util.py\", line 181, in collate\n",
      "    collated_batch = collate_fn(batch.data)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/iterator.py\", \n",
      "line 358, in collate_fn\n",
      "    return convert_ndarray_batch_to_torch_tensor_batch(\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.p\n",
      "y\", line 240, in convert_ndarray_batch_to_torch_tensor_batch\n",
      "    batch = {\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.p\n",
      "y\", line 241, in <dictcomp>\n",
      "    col_name: convert_ndarray_to_torch_tensor(\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/_internal/torch_utils.p\n",
      "y\", line 194, in convert_ndarray_to_torch_tensor\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Numpy array of object dtype cannot be converted to a Torch Tensor.\n",
      "This may because the numpy array is a ragged tensor--it contains items of \n",
      "different sizes. If using `iter_torch_batches()` API, you can pass in a \n",
      "`collate_fn` argument to specify custom logic to convert the Numpy array batch \n",
      "to a Torch tensor batch.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_c43e002ec4ba11f7/longformer_7.py:156 in <module>              │\n",
      "│                                                                              │\n",
      "│   153 │   trainer.fit()                                                      │\n",
      "│   154                                                                        │\n",
      "│   155 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 156 │   train_model()                                                      │\n",
      "│   157                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_c43e002ec4ba11f7/longformer_7.py:153 in train_model           │\n",
      "│                                                                              │\n",
      "│   150 │   │   )                                                              │\n",
      "│   151 │   )                                                                  │\n",
      "│   152 │                                                                      │\n",
      "│ ❱ 153 │   trainer.fit()                                                      │\n",
      "│   154                                                                        │\n",
      "│   155 if __name__ == \"__main__\":                                             │\n",
      "│   156 │   train_model()                                                      │\n",
      "│                                                                              │\n",
      "│ /home/ray/anaconda3/lib/python3.8/site-packages/ray/train/base_trainer.py:64 │\n",
      "│ 0 in fit                                                                     │\n",
      "│                                                                              │\n",
      "│   637 │   │   if result.error:                                               │\n",
      "│   638 │   │   │   # Raise trainable errors to the user with a message to res │\n",
      "│   639 │   │   │   # or configure `FailureConfig` in a new run.               │\n",
      "│ ❱ 640 │   │   │   raise TrainingFailedError(                                 │\n",
      "│   641 │   │   │   │   \"\\n\".join([restore_msg, TrainingFailedError._FAILURE_C │\n",
      "│   642 │   │   │   ) from result.error                                        │\n",
      "│   643 │   │   return result                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-20-41\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                         \u001b[A\u001b[A\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_mZpkEgiHFmxGfNbw' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                         \u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                         \u001b[A\u001b[A\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_7.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4f12592-a5c3-4ec1-a765-530a9b068fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:22:12,764 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:22:13,927\tINFO dashboard_sdk.py:385 -- Package gcs://_ray_pkg_c43e002ec4ba11f7.zip already exists, skipping upload.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_KjgJ5nn5fyaBC6fY' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_KjgJ5nn5fyaBC6fY\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_KjgJ5nn5fyaBC6fY\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_KjgJ5nn5fyaBC6fY\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 23:22:19,709\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 23:22:19,710\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 23:22:19,726\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 23:22:20,566\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_23-22-20\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_23-22-20`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=31760, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=31666, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=31666, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=31760) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=31873, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=31760, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "\u001b[36m(RayTrainWorker pid=31760, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=31760, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36m(RayTrainWorker pid=31760, ip=10.233.120.184)\u001b[0m 2025/05/08 23:22:33 INFO mlflow.tracking._tracking_service.client: 🏃 View run fortunate-stoat-793 at: http://129.114.26.124:8000/#/experiments/2/runs/ec3d892cf4f447148b20936907d61ec4.\n",
      "\u001b[36m(RayTrainWorker pid=31760, ip=10.233.120.184)\u001b[0m 2025/05/08 23:22:33 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://129.114.26.124:8000/#/experiments/2.\n",
      "2025-05-08 23:22:33,665\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_f68a7_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=31666, ip=10.233.120.184, actor_id=186f2a598e46c077174fbb410f000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=31760, ip=10.233.120.184, actor_id=b0a57480f1739a9ad37fa7a90f000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x73086c35bc10>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_8.py\", line 120, in train_func\n",
      "    warmup_steps=int(0.1 * len(dataloader)),\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 486, in __len__\n",
      "    return len(self._index_sampler)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 79, in __len__\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type '_IterableFromIterator' has no len()\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:22:33. Total running time: 13s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-22-20/TorchTrainer_f68a7_00000_0_2025-05-08_23-22-20/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=31300, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=31190, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=31190, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=31300) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=31398, ip=10.233.120.183)\u001b[0m Auto configuring locality_with_output=['e12ef704e32efb5210d0b9d349d7534e05d103580423542853a3c39f']\n",
      "\u001b[36m(RayTrainWorker pid=31300, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "\u001b[36m(RayTrainWorker pid=31300, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=31300, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36m(RayTrainWorker pid=31300, ip=10.233.120.183)\u001b[0m 2025/05/08 23:22:46 INFO mlflow.tracking._tracking_service.client: 🏃 View run silent-boar-515 at: http://129.114.26.124:8000/#/experiments/2/runs/595a284112bc4cc9bd16b827f5259e93.\n",
      "\u001b[36m(RayTrainWorker pid=31300, ip=10.233.120.183)\u001b[0m 2025/05/08 23:22:46 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://129.114.26.124:8000/#/experiments/2.\n",
      "2025-05-08 23:22:46,808\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_f68a7_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=31190, ip=10.233.120.183, actor_id=c890d7be7c1958554b3c597d0f000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=31300, ip=10.233.120.183, actor_id=69df7cc378308279212322b40f000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7849a425bb50>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_8.py\", line 120, in train_func\n",
      "    warmup_steps=int(0.1 * len(dataloader)),\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 486, in __len__\n",
      "    return len(self._index_sampler)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 79, in __len__\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type '_IterableFromIterator' has no len()\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:22:46. Total running time: 26s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-22-20/TorchTrainer_f68a7_00000_0_2025-05-08_23-22-20/error.txt\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=32157, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=32063, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=32063, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=32157) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(SplitCoordinator pid=32270, ip=10.233.120.184)\u001b[0m Auto configuring locality_with_output=['187ef995765a49a2883baa9ccb9a47bb103a33f2701d59793bf3bfa4']\n",
      "\u001b[36m(RayTrainWorker pid=32157, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "\u001b[36m(RayTrainWorker pid=32157, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=32157, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36m(RayTrainWorker pid=32157, ip=10.233.120.184)\u001b[0m 2025/05/08 23:22:58 INFO mlflow.tracking._tracking_service.client: 🏃 View run bittersweet-elk-104 at: http://129.114.26.124:8000/#/experiments/2/runs/7e710605f39143f2a6d93e55ea7feb06.\n",
      "\u001b[36m(RayTrainWorker pid=32157, ip=10.233.120.184)\u001b[0m 2025/05/08 23:22:58 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://129.114.26.124:8000/#/experiments/2.\n",
      "2025-05-08 23:22:58,916\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_f68a7_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=32063, ip=10.233.120.184, actor_id=89890b24cca56bbe892ea6210f000000, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=32157, ip=10.233.120.184, actor_id=690b1588ac2695d241981d0c0f000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x704e1029bb80>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_8.py\", line 120, in train_func\n",
      "    warmup_steps=int(0.1 * len(dataloader)),\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 486, in __len__\n",
      "    return len(self._index_sampler)\n",
      "  File \"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91bece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils/data/sampler.py\", line 79, in __len__\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type '_IterableFromIterator' has no len()\n",
      "\n",
      "Training errored after 0 iterations at 2025-05-08 23:22:58. Total running time: 38s\n",
      "Error file: /home/ray/ray_results/TorchTrainer_2025-05-08_23-22-20/TorchTrainer_f68a7_00000_0_2025-05-08_23-22-20/error.txt\n",
      "\n",
      "2025-05-08 23:22:58,928\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_f68a7_00000]\n",
      "RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=32063, \n",
      "ip=10.233.120.184, actor_id=89890b24cca56bbe892ea6210f000000, repr=TorchTrainer)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\n",
      "\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \n",
      "\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=32157, ip=10.233.120.184,\n",
      "actor_id=690b1588ac2695d241981d0c0f000000, \n",
      "repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x704e1029bb80>)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_grou\n",
      "p.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \n",
      "\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", \n",
      "line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"longformer_8.py\", line 120, in train_func\n",
      "    warmup_steps=int(0.1 * len(dataloader)),\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils\n",
      "/data/dataloader.py\", line 486, in __len__\n",
      "    return len(self._index_sampler)\n",
      "  File \n",
      "\"/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/045202ef91b\n",
      "ece2cdecc4610402e5b6dd29269a5/virtualenv/lib/python3.8/site-packages/torch/utils\n",
      "/data/sampler.py\", line 79, in __len__\n",
      "    return len(self.data_source)\n",
      "TypeError: object of type '_IterableFromIterator' has no len()\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_c43e002ec4ba11f7/longformer_8.py:150 in <module>              │\n",
      "│                                                                              │\n",
      "│   147 │   trainer.fit()                                                      │\n",
      "│   148                                                                        │\n",
      "│   149 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 150 │   train_model()                                                      │\n",
      "│   151                                                                        │\n",
      "│                                                                              │\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_c43e002ec4ba11f7/longformer_8.py:147 in train_model           │\n",
      "│                                                                              │\n",
      "│   144 │   │   run_config=RunConfig(failure_config=FailureConfig(max_failures │\n",
      "│   145 │   )                                                                  │\n",
      "│   146 │                                                                      │\n",
      "│ ❱ 147 │   trainer.fit()                                                      │\n",
      "│   148                                                                        │\n",
      "│   149 if __name__ == \"__main__\":                                             │\n",
      "│   150 │   train_model()                                                      │\n",
      "│                                                                              │\n",
      "│ /home/ray/anaconda3/lib/python3.8/site-packages/ray/train/base_trainer.py:64 │\n",
      "│ 0 in fit                                                                     │\n",
      "│                                                                              │\n",
      "│   637 │   │   if result.error:                                               │\n",
      "│   638 │   │   │   # Raise trainable errors to the user with a message to res │\n",
      "│   639 │   │   │   # or configure `FailureConfig` in a new run.               │\n",
      "│ ❱ 640 │   │   │   raise TrainingFailedError(                                 │\n",
      "│   641 │   │   │   │   \"\\n\".join([restore_msg, TrainingFailedError._FAILURE_C │\n",
      "│   642 │   │   │   ) from result.error                                        │\n",
      "│   643 │   │   return result                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-22-20\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_KjgJ5nn5fyaBC6fY' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "TrainingFailedError: The Ray Train run failed. Please inspect the previous error\n",
      "messages for a cause. After fixing the issue (assuming that the error is not \n",
      "caused by your own application logic, but rather an error such as OOM), you can \n",
      "restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = \n",
      "TorchTrainer.restore(\"/home/ray/ray_results/TorchTrainer_2025-05-08_23-22-20\")`.\n",
      "To start a new run that will retry on training failures, set \n",
      "`train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the \n",
      "Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for \n",
      "unlimited retries.\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_8.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b33221b3-ee6c-4133-94fb-4fed37bf4bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:28:01,065 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:28:02,272\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_c9e015e3bc4d3e30.zip.\n",
      "2025-05-09 06:28:02,273\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_Nv97H7rhD5WQcZKQ' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_Nv97H7rhD5WQcZKQ\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_Nv97H7rhD5WQcZKQ\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_Nv97H7rhD5WQcZKQ\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 23:28:08,962\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 23:28:08,962\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 23:28:08,988\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 23:28:09,148\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_23-28-09\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_23-28-09`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=32648, ip=10.233.120.183)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=32648, ip=10.233.120.183)\u001b[0m - (ip=10.233.120.183, pid=32758) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:   2%|▏         | 1/59 [00:00<00:44,  1.30it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:   3%|▎         | 2/59 [00:01<00:29,  1.94it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:   5%|▌         | 3/59 [00:01<00:23,  2.34it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:   7%|▋         | 4/59 [00:01<00:21,  2.56it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:   8%|▊         | 5/59 [00:01<00:19,  2.72it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  10%|█         | 6/59 [00:02<00:18,  2.84it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  12%|█▏        | 7/59 [00:02<00:17,  2.91it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  14%|█▎        | 8/59 [00:02<00:17,  2.98it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  15%|█▌        | 9/59 [00:03<00:16,  3.05it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  17%|█▋        | 10/59 [00:03<00:15,  3.10it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  19%|█▊        | 11/59 [00:03<00:15,  3.14it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  20%|██        | 12/59 [00:03<00:14,  3.17it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  22%|██▏       | 13/59 [00:04<00:14,  3.20it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  24%|██▎       | 14/59 [00:04<00:13,  3.22it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  25%|██▌       | 15/59 [00:04<00:13,  3.25it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  27%|██▋       | 16/59 [00:05<00:13,  3.23it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  29%|██▉       | 17/59 [00:05<00:12,  3.25it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  31%|███       | 18/59 [00:05<00:12,  3.27it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  32%|███▏      | 19/59 [00:05<00:12,  3.29it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  34%|███▍      | 20/59 [00:06<00:11,  3.31it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  36%|███▌      | 21/59 [00:06<00:11,  3.33it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  37%|███▋      | 22/59 [00:06<00:11,  3.34it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  39%|███▉      | 23/59 [00:07<00:10,  3.36it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  41%|████      | 24/59 [00:07<00:10,  3.37it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  42%|████▏     | 25/59 [00:07<00:10,  3.38it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  44%|████▍     | 26/59 [00:07<00:09,  3.39it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  46%|████▌     | 27/59 [00:08<00:09,  3.40it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  47%|████▋     | 28/59 [00:08<00:09,  3.41it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  49%|████▉     | 29/59 [00:08<00:08,  3.41it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  51%|█████     | 30/59 [00:09<00:08,  3.42it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  53%|█████▎    | 31/59 [00:09<00:08,  3.42it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  54%|█████▍    | 32/59 [00:09<00:07,  3.42it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  56%|█████▌    | 33/59 [00:09<00:07,  3.43it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  58%|█████▊    | 34/59 [00:10<00:07,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  59%|█████▉    | 35/59 [00:10<00:06,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  61%|██████    | 36/59 [00:10<00:06,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  63%|██████▎   | 37/59 [00:11<00:06,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  64%|██████▍   | 38/59 [00:11<00:06,  3.45it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  66%|██████▌   | 39/59 [00:11<00:05,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  68%|██████▊   | 40/59 [00:11<00:05,  3.45it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  69%|██████▉   | 41/59 [00:12<00:05,  3.45it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  71%|███████   | 42/59 [00:12<00:04,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  73%|███████▎  | 43/59 [00:12<00:04,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  75%|███████▍  | 44/59 [00:13<00:04,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  76%|███████▋  | 45/59 [00:13<00:04,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  78%|███████▊  | 46/59 [00:13<00:03,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  80%|███████▉  | 47/59 [00:13<00:03,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  81%|████████▏ | 48/59 [00:14<00:03,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  83%|████████▎ | 49/59 [00:14<00:02,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  85%|████████▍ | 50/59 [00:14<00:02,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  86%|████████▋ | 51/59 [00:15<00:02,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  88%|████████▊ | 52/59 [00:15<00:02,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  90%|████████▉ | 53/59 [00:15<00:01,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  92%|█████████▏| 54/59 [00:15<00:01,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  93%|█████████▎| 55/59 [00:16<00:01,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  95%|█████████▍| 56/59 [00:16<00:00,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  97%|█████████▋| 57/59 [00:16<00:00,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration:  98%|█████████▊| 58/59 [00:17<00:00,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Iteration: 100%|██████████| 59/59 [00:18<00:00,  2.98it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 59/59 [00:18<00:00,  3.23it/s]\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m \n",
      "Epoch: 100%|██████████| 1/1 [00:18<00:00, 18.28s/it]\n",
      "Epoch: 100%|██████████| 1/1 [00:18<00:00, 18.28s/it]\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m 2025/05/08 23:28:52 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpra9h_9dt/model, flavor: sentence_transformers). Fall back to return ['sentence-transformers==2.2.2', 'transformers==4.28.1', 'torch==2.0.1']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m Registered model 'arxiv-bi-encoder-longformer' already exists. Creating a new version of this model...\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m 2025/05/08 23:29:19 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: arxiv-bi-encoder-longformer, version 3\n",
      "\u001b[36m(RayTrainWorker pid=32758, ip=10.233.120.183)\u001b[0m Created version '3' of model 'arxiv-bi-encoder-longformer'.\n",
      "\n",
      "Training finished iteration 1 at 2025-05-08 23:29:19. Total running time: 1min 10s\n",
      "╭───────────────────────────────╮\n",
      "│ Training result               │\n",
      "├───────────────────────────────┤\n",
      "│ checkpoint_dir_name           │\n",
      "│ time_this_iter_s      65.9802 │\n",
      "│ time_total_s          65.9802 │\n",
      "│ training_iteration          1 │\n",
      "╰───────────────────────────────╯\n",
      "\n",
      "Training completed after 1 iterations at 2025-05-08 23:29:19. Total running time: 1min 10s\n",
      "\n",
      "\n",
      "\u001b[32m------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_Nv97H7rhD5WQcZKQ' succeeded\u001b[39m\n",
      "\u001b[32m------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_8.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95da3a68-7a87-4404-8629-8483e358b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:36:52,911 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:36:54,643\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_5f3a4357eb16419c.zip.\n",
      "2025-05-09 06:36:54,644\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_5FUpfq8crNdawWVi' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_5FUpfq8crNdawWVi\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_5FUpfq8crNdawWVi\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_5FUpfq8crNdawWVi\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 23:37:00,840\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 23:37:00,841\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 23:37:00,856\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 23:37:00,981\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_23-37-00\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_23-37-00`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=35199, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=35199, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=35293) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   2%|▏         | 1/59 [00:00<00:47,  1.21it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   3%|▎         | 2/59 [00:01<00:30,  1.85it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   5%|▌         | 3/59 [00:01<00:24,  2.25it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   7%|▋         | 4/59 [00:01<00:22,  2.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   8%|▊         | 5/59 [00:01<00:20,  2.65it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  10%|█         | 6/59 [00:02<00:19,  2.78it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  12%|█▏        | 7/59 [00:02<00:18,  2.89it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  14%|█▎        | 8/59 [00:02<00:17,  2.96it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  15%|█▌        | 9/59 [00:03<00:16,  3.03it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  17%|█▋        | 10/59 [00:03<00:15,  3.08it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  19%|█▊        | 11/59 [00:03<00:15,  3.12it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  20%|██        | 12/59 [00:03<00:14,  3.16it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  22%|██▏       | 13/59 [00:04<00:14,  3.19it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  24%|██▎       | 14/59 [00:04<00:13,  3.21it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  25%|██▌       | 15/59 [00:04<00:13,  3.24it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  27%|██▋       | 16/59 [00:05<00:13,  3.27it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  29%|██▉       | 17/59 [00:05<00:12,  3.29it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  31%|███       | 18/59 [00:05<00:12,  3.31it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  32%|███▏      | 19/59 [00:05<00:12,  3.32it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  34%|███▍      | 20/59 [00:06<00:11,  3.34it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  36%|███▌      | 21/59 [00:06<00:11,  3.35it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  37%|███▋      | 22/59 [00:06<00:11,  3.36it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  39%|███▉      | 23/59 [00:07<00:10,  3.37it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  41%|████      | 24/59 [00:07<00:10,  3.38it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  42%|████▏     | 25/59 [00:07<00:10,  3.39it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  44%|████▍     | 26/59 [00:07<00:09,  3.39it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  46%|████▌     | 27/59 [00:08<00:09,  3.40it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  47%|████▋     | 28/59 [00:08<00:09,  3.41it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  49%|████▉     | 29/59 [00:08<00:08,  3.42it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  51%|█████     | 30/59 [00:09<00:08,  3.43it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  53%|█████▎    | 31/59 [00:09<00:08,  3.43it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  54%|█████▍    | 32/59 [00:09<00:07,  3.43it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  56%|█████▌    | 33/59 [00:09<00:07,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  58%|█████▊    | 34/59 [00:10<00:07,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  59%|█████▉    | 35/59 [00:10<00:06,  3.45it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  61%|██████    | 36/59 [00:10<00:06,  3.45it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  63%|██████▎   | 37/59 [00:11<00:06,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  64%|██████▍   | 38/59 [00:11<00:06,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  66%|██████▌   | 39/59 [00:11<00:05,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  68%|██████▊   | 40/59 [00:11<00:05,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  69%|██████▉   | 41/59 [00:12<00:05,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  71%|███████   | 42/59 [00:12<00:04,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  73%|███████▎  | 43/59 [00:12<00:04,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  75%|███████▍  | 44/59 [00:13<00:04,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  76%|███████▋  | 45/59 [00:13<00:04,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  78%|███████▊  | 46/59 [00:13<00:03,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  80%|███████▉  | 47/59 [00:13<00:03,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  81%|████████▏ | 48/59 [00:14<00:03,  3.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  83%|████████▎ | 49/59 [00:14<00:02,  3.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  85%|████████▍ | 50/59 [00:14<00:02,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  86%|████████▋ | 51/59 [00:15<00:02,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  88%|████████▊ | 52/59 [00:15<00:02,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  90%|████████▉ | 53/59 [00:15<00:01,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  92%|█████████▏| 54/59 [00:15<00:01,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  93%|█████████▎| 55/59 [00:16<00:01,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  95%|█████████▍| 56/59 [00:16<00:00,  3.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  97%|█████████▋| 57/59 [00:16<00:00,  3.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  98%|█████████▊| 58/59 [00:17<00:00,  3.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Iteration: 100%|██████████| 59/59 [00:18<00:00,  3.01it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 59/59 [00:18<00:00,  3.24it/s]\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m \n",
      "Epoch: 100%|██████████| 1/1 [00:18<00:00, 18.20s/it]\n",
      "Epoch: 100%|██████████| 1/1 [00:18<00:00, 18.20s/it]\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m 2025/05/08 23:37:44 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpkggine_f/model, flavor: sentence_transformers). Fall back to return ['sentence-transformers==2.2.2', 'transformers==4.28.1', 'torch==2.0.1']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m Registered model 'arxiv-bi-encoder-longformer' already exists. Creating a new version of this model...\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m 2025/05/08 23:38:14 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: arxiv-bi-encoder-longformer, version 4\n",
      "\u001b[36m(RayTrainWorker pid=35293, ip=10.233.120.184)\u001b[0m Created version '4' of model 'arxiv-bi-encoder-longformer'.\n",
      "\n",
      "Training finished iteration 1 at 2025-05-08 23:38:14. Total running time: 1min 13s\n",
      "╭───────────────────────────────╮\n",
      "│ Training result               │\n",
      "├───────────────────────────────┤\n",
      "│ checkpoint_dir_name           │\n",
      "│ time_this_iter_s      69.0909 │\n",
      "│ time_total_s          69.0909 │\n",
      "│ training_iteration          1 │\n",
      "╰───────────────────────────────╯\n",
      "\n",
      "Training completed after 1 iterations at 2025-05-08 23:38:14. Total running time: 1min 13s\n",
      "\n",
      "\n",
      "\u001b[32m------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_5FUpfq8crNdawWVi' succeeded\u001b[39m\n",
      "\u001b[32m------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_9.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f22bf8e-fbe0-4cd3-85c8-38dd79e2490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:42:34,020 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:42:35,344\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_afcbb6aed9428a05.zip.\n",
      "2025-05-09 06:42:35,345\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_m9WK4nEGZ3ZmiEjv' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_m9WK4nEGZ3ZmiEjv\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_m9WK4nEGZ3ZmiEjv\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_m9WK4nEGZ3ZmiEjv\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "2025-05-08 23:42:40,768\tINFO worker.py:1405 -- Using address 10.233.120.182:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-05-08 23:42:40,768\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.233.120.182:6379...\n",
      "2025-05-08 23:42:40,778\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.233.120.182:8265 \u001b[39m\u001b[22m\n",
      "2025-05-08 23:42:41,123\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\n",
      "View detailed results here: /home/ray/ray_results/TorchTrainer_2025-05-08_23-42-41\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2025-05-08_23-42-41`\n",
      "\n",
      "Training started without custom configuration.\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=36590, ip=10.233.120.184)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=36590, ip=10.233.120.184)\u001b[0m - (ip=10.233.120.184, pid=36684) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m - This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m - This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   0%|          | 0/147 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   1%|          | 1/147 [00:00<01:55,  1.27it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   1%|▏         | 2/147 [00:01<01:21,  1.78it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   2%|▏         | 3/147 [00:01<01:06,  2.18it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   3%|▎         | 4/147 [00:01<00:58,  2.42it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   3%|▎         | 5/147 [00:01<00:54,  2.60it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   4%|▍         | 6/147 [00:02<00:58,  2.39it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   5%|▍         | 7/147 [00:02<00:55,  2.52it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   5%|▌         | 8/147 [00:03<00:52,  2.63it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   6%|▌         | 9/147 [00:03<00:50,  2.72it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   7%|▋         | 10/147 [00:03<00:50,  2.73it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   7%|▋         | 11/147 [00:04<00:48,  2.81it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   8%|▊         | 12/147 [00:04<00:47,  2.87it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:   9%|▉         | 13/147 [00:04<00:46,  2.91it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  10%|▉         | 14/147 [00:04<00:44,  2.96it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  10%|█         | 15/147 [00:05<00:43,  3.00it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  11%|█         | 16/147 [00:05<00:43,  3.04it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  12%|█▏        | 17/147 [00:05<00:42,  3.08it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  12%|█▏        | 18/147 [00:06<00:41,  3.10it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  13%|█▎        | 19/147 [00:06<00:40,  3.13it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  14%|█▎        | 20/147 [00:06<00:40,  3.10it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  14%|█▍        | 21/147 [00:06<00:40,  3.13it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  15%|█▍        | 22/147 [00:07<00:39,  3.16it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  16%|█▌        | 23/147 [00:07<00:39,  3.17it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  16%|█▋        | 24/147 [00:07<00:38,  3.19it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  17%|█▋        | 25/147 [00:08<00:37,  3.21it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  18%|█▊        | 26/147 [00:08<00:37,  3.23it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  18%|█▊        | 27/147 [00:08<00:36,  3.25it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  19%|█▉        | 28/147 [00:08<00:36,  3.26it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  20%|█▉        | 29/147 [00:09<00:36,  3.27it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  20%|██        | 30/147 [00:09<00:35,  3.28it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  21%|██        | 31/147 [00:09<00:35,  3.29it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  22%|██▏       | 32/147 [00:10<00:34,  3.31it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  22%|██▏       | 33/147 [00:10<00:34,  3.32it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  23%|██▎       | 34/147 [00:10<00:33,  3.32it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  24%|██▍       | 35/147 [00:11<00:34,  3.28it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  24%|██▍       | 36/147 [00:11<00:33,  3.29it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  25%|██▌       | 37/147 [00:11<00:33,  3.30it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  26%|██▌       | 38/147 [00:11<00:32,  3.32it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  27%|██▋       | 39/147 [00:12<00:32,  3.33it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  27%|██▋       | 40/147 [00:12<00:32,  3.34it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  28%|██▊       | 41/147 [00:12<00:32,  3.30it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  29%|██▊       | 42/147 [00:13<00:31,  3.31it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  29%|██▉       | 43/147 [00:13<00:31,  3.32it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  30%|██▉       | 44/147 [00:13<00:30,  3.33it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  31%|███       | 45/147 [00:14<00:30,  3.34it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  31%|███▏      | 46/147 [00:14<00:30,  3.35it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  32%|███▏      | 47/147 [00:14<00:29,  3.36it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  33%|███▎      | 48/147 [00:14<00:29,  3.37it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  33%|███▎      | 49/147 [00:15<00:29,  3.37it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  34%|███▍      | 50/147 [00:15<00:28,  3.38it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  35%|███▍      | 51/147 [00:15<00:28,  3.39it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  35%|███▌      | 52/147 [00:16<00:27,  3.39it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  36%|███▌      | 53/147 [00:16<00:27,  3.40it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  37%|███▋      | 54/147 [00:16<00:27,  3.36it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  37%|███▋      | 55/147 [00:16<00:27,  3.36it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  38%|███▊      | 56/147 [00:17<00:27,  3.37it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  39%|███▉      | 57/147 [00:17<00:26,  3.37it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  39%|███▉      | 58/147 [00:17<00:26,  3.34it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  40%|████      | 59/147 [00:18<00:26,  3.35it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  41%|████      | 60/147 [00:18<00:26,  3.31it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  41%|████▏     | 61/147 [00:18<00:25,  3.32it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  42%|████▏     | 62/147 [00:19<00:25,  3.33it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  43%|████▎     | 63/147 [00:19<00:25,  3.34it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  44%|████▎     | 64/147 [00:19<00:24,  3.35it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  44%|████▍     | 65/147 [00:19<00:24,  3.35it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  45%|████▍     | 66/147 [00:20<00:24,  3.36it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  46%|████▌     | 67/147 [00:20<00:23,  3.36it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  46%|████▋     | 68/147 [00:20<00:23,  3.37it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  47%|████▋     | 69/147 [00:21<00:23,  3.37it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  48%|████▊     | 70/147 [00:21<00:22,  3.38it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  48%|████▊     | 71/147 [00:21<00:22,  3.39it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  49%|████▉     | 72/147 [00:21<00:22,  3.39it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  50%|████▉     | 73/147 [00:22<00:21,  3.40it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  50%|█████     | 74/147 [00:22<00:21,  3.40it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  51%|█████     | 75/147 [00:22<00:21,  3.40it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  52%|█████▏    | 76/147 [00:23<00:20,  3.41it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  52%|█████▏    | 77/147 [00:23<00:20,  3.41it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  53%|█████▎    | 78/147 [00:23<00:20,  3.42it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  54%|█████▎    | 79/147 [00:23<00:19,  3.42it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  54%|█████▍    | 80/147 [00:24<00:19,  3.42it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  55%|█████▌    | 81/147 [00:24<00:19,  3.42it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  56%|█████▌    | 82/147 [00:24<00:18,  3.43it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  56%|█████▋    | 83/147 [00:25<00:18,  3.43it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  57%|█████▋    | 84/147 [00:25<00:18,  3.43it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  58%|█████▊    | 85/147 [00:25<00:18,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  59%|█████▊    | 86/147 [00:25<00:17,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  59%|█████▉    | 87/147 [00:26<00:17,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  60%|█████▉    | 88/147 [00:26<00:17,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  61%|██████    | 89/147 [00:26<00:16,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  61%|██████    | 90/147 [00:27<00:16,  3.40it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  62%|██████▏   | 91/147 [00:27<00:16,  3.41it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  63%|██████▎   | 92/147 [00:27<00:16,  3.41it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  63%|██████▎   | 93/147 [00:28<00:15,  3.42it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  64%|██████▍   | 94/147 [00:28<00:15,  3.42it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  65%|██████▍   | 95/147 [00:28<00:15,  3.43it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  65%|██████▌   | 96/147 [00:28<00:14,  3.43it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  66%|██████▌   | 97/147 [00:29<00:14,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  67%|██████▋   | 98/147 [00:29<00:14,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  67%|██████▋   | 99/147 [00:29<00:13,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  68%|██████▊   | 100/147 [00:30<00:13,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  69%|██████▊   | 101/147 [00:30<00:13,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  69%|██████▉   | 102/147 [00:30<00:13,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  70%|███████   | 103/147 [00:30<00:12,  3.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  71%|███████   | 104/147 [00:31<00:12,  3.45it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  71%|███████▏  | 105/147 [00:31<00:12,  3.45it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  72%|███████▏  | 106/147 [00:31<00:11,  3.45it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  73%|███████▎  | 107/147 [00:32<00:11,  3.45it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  73%|███████▎  | 108/147 [00:32<00:11,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  74%|███████▍  | 109/147 [00:32<00:10,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  75%|███████▍  | 110/147 [00:32<00:10,  3.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  76%|███████▌  | 111/147 [00:33<00:10,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  76%|███████▌  | 112/147 [00:33<00:10,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  77%|███████▋  | 113/147 [00:33<00:09,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  78%|███████▊  | 114/147 [00:34<00:09,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  78%|███████▊  | 115/147 [00:34<00:09,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  79%|███████▉  | 116/147 [00:34<00:08,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  80%|███████▉  | 117/147 [00:34<00:08,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  80%|████████  | 118/147 [00:35<00:08,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  81%|████████  | 119/147 [00:35<00:08,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  82%|████████▏ | 120/147 [00:35<00:07,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  82%|████████▏ | 121/147 [00:36<00:07,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  83%|████████▎ | 122/147 [00:36<00:07,  3.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  84%|████████▎ | 123/147 [00:36<00:06,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  84%|████████▍ | 124/147 [00:36<00:06,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  85%|████████▌ | 125/147 [00:37<00:06,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  86%|████████▌ | 126/147 [00:37<00:06,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  86%|████████▋ | 127/147 [00:37<00:05,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  87%|████████▋ | 128/147 [00:38<00:05,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  88%|████████▊ | 129/147 [00:38<00:05,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  88%|████████▊ | 130/147 [00:38<00:04,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  89%|████████▉ | 131/147 [00:38<00:04,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  90%|████████▉ | 132/147 [00:39<00:04,  3.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  90%|█████████ | 133/147 [00:39<00:04,  3.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  91%|█████████ | 134/147 [00:39<00:03,  3.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  92%|█████████▏| 135/147 [00:40<00:03,  3.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  93%|█████████▎| 136/147 [00:40<00:03,  3.48it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  93%|█████████▎| 137/147 [00:40<00:02,  3.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  94%|█████████▍| 138/147 [00:40<00:02,  3.50it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  95%|█████████▍| 139/147 [00:41<00:02,  3.50it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  95%|█████████▌| 140/147 [00:41<00:02,  3.50it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  96%|█████████▌| 141/147 [00:41<00:01,  3.50it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  97%|█████████▋| 142/147 [00:42<00:01,  3.50it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  97%|█████████▋| 143/147 [00:42<00:01,  3.50it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  98%|█████████▊| 144/147 [00:42<00:00,  3.50it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  99%|█████████▊| 145/147 [00:42<00:00,  3.50it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration:  99%|█████████▉| 146/147 [00:43<00:00,  3.50it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Iteration: 100%|██████████| 147/147 [00:44<00:00,  3.06it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 147/147 [00:44<00:00,  3.32it/s]\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m \n",
      "Epoch: 100%|██████████| 1/1 [00:44<00:00, 44.31s/it]\n",
      "Epoch: 100%|██████████| 1/1 [00:44<00:00, 44.31s/it]\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m 2025/05/08 23:43:51 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpte2bus8f/model, flavor: sentence_transformers). Fall back to return ['sentence-transformers==2.2.2', 'transformers==4.28.1', 'torch==2.0.1']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m Registered model 'arxiv-bi-encoder-longformer' already exists. Creating a new version of this model...\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m 2025/05/08 23:44:22 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: arxiv-bi-encoder-longformer, version 5\n",
      "\u001b[36m(RayTrainWorker pid=36684, ip=10.233.120.184)\u001b[0m Created version '5' of model 'arxiv-bi-encoder-longformer'.\n",
      "\n",
      "Training finished iteration 1 at 2025-05-08 23:44:22. Total running time: 1min 41s\n",
      "╭───────────────────────────────╮\n",
      "│ Training result               │\n",
      "├───────────────────────────────┤\n",
      "│ checkpoint_dir_name           │\n",
      "│ time_this_iter_s      96.6608 │\n",
      "│ time_total_s          96.6608 │\n",
      "│ training_iteration          1 │\n",
      "╰───────────────────────────────╯\n",
      "\n",
      "Training completed after 1 iterations at 2025-05-08 23:44:22. Total running time: 1min 41s\n",
      "\n",
      "\n",
      "\u001b[32m------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_m9WK4nEGZ3ZmiEjv' succeeded\u001b[39m\n",
      "\u001b[32m------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.2.2\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_10.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e120c9e4-1ec4-4926-ac1b-21544a5899f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 06:55:34,029 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 06:55:35,234\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_6cc46746933862d7.zip.\n",
      "2025-05-09 06:55:35,235\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_xWwpQKFndpstXuyw' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_xWwpQKFndpstXuyw\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_xWwpQKFndpstXuyw\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_xWwpQKFndpstXuyw\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_xWwpQKFndpstXuyw' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: runtime_env setup failed: Failed to set up runtime environment.\n",
      "Could not create the actor because its associated runtime env failed to be created.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/agent/runtime_env_agent.py\", line 369, in _create_runtime_env_with_retry\n",
      "    runtime_env_context = await asyncio.wait_for(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/asyncio/tasks.py\", line 494, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/agent/runtime_env_agent.py\", line 329, in _setup_runtime_env\n",
      "    await create_for_plugin_if_needed(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/plugin.py\", line 254, in create_for_plugin_if_needed\n",
      "    size_bytes = await plugin.create(uri, runtime_env, context, logger=logger)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/pip.py\", line 518, in create\n",
      "    bytes = await task\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/pip.py\", line 498, in _create_for_hash\n",
      "    await PipProcessor(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/pip.py\", line 400, in _run\n",
      "    await self._install_pip_packages(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/pip.py\", line 376, in _install_pip_packages\n",
      "    await check_output_cmd(pip_install_cmd, logger=logger, cwd=cwd, env=pip_env)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/utils.py\", line 101, in check_output_cmd\n",
      "    raise SubprocessCalledProcessError(\n",
      "ray._private.runtime_env.utils.SubprocessCalledProcessError: Run cmd[13] failed with the following details.\n",
      "Command '['/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/14f249f8d8b388e317687675ae526c341d0fbdd0/virtualenv/bin/python', '-m', 'pip', 'install', '--disable-pip-version-check', '--no-cache-dir', '-r', '/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/14f249f8d8b388e317687675ae526c341d0fbdd0/ray_runtime_env_internal_pip_requirements.txt']' returned non-zero exit status 1.\n",
      "Last 50 lines of stdout:\n",
      "    Collecting torch<2.1,>=1.12.0\n",
      "      Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 6.1 MB/s eta 0:00:00\n",
      "    Collecting sentence-transformers==2.3.1\n",
      "      Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.8/132.8 kB 8.8 MB/s eta 0:00:00\n",
      "    Collecting transformers==4.28.1\n",
      "      Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 6.3 MB/s eta 0:00:00\n",
      "    Collecting huggingface_hub==0.14.1\n",
      "      Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 7.8 MB/s eta 0:00:00\n",
      "    Collecting accelerate==0.20.3\n",
      "      Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 9.2 MB/s eta 0:00:00\n",
      "    Collecting mlflow>=2.2.0\n",
      "      Downloading mlflow-2.17.2-py3-none-any.whl (26.7 MB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.7/26.7 MB 7.3 MB/s eta 0:00:00\n",
      "    Collecting datasets\n",
      "      Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 480.6/480.6 kB 6.5 MB/s eta 0:00:00\n",
      "    Requirement already satisfied: pandas in /home/ray/anaconda3/lib/python3.8/site-packages (from -r /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/14f249f8d8b388e317687675ae526c341d0fbdd0/ray_runtime_env_internal_pip_requirements.txt (line 8)) (1.5.3)\n",
      "    Collecting sqlalchemy\n",
      "      Downloading sqlalchemy-2.0.40-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 7.6 MB/s eta 0:00:00\n",
      "    Collecting psycopg2-binary\n",
      "      Downloading psycopg2_binary-2.9.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 7.2 MB/s eta 0:00:00\n",
      "    ERROR: Cannot install -r /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/14f249f8d8b388e317687675ae526c341d0fbdd0/ray_runtime_env_internal_pip_requirements.txt (line 2) and huggingface_hub==0.14.1 because these package versions have conflicting dependencies.\n",
      "\n",
      "    The conflict is caused by:\n",
      "        The user requested huggingface_hub==0.14.1\n",
      "        sentence-transformers 2.3.1 depends on huggingface-hub>=0.15.1\n",
      "\n",
      "    To fix this you could try to:\n",
      "    1. loosen the range of package versions you've specified\n",
      "    2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "    ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.3.1\",\"transformers==4.28.1\",\"huggingface_hub==0.14.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_11.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6035b68-8c10-4f44-9d42-6fa007560b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 07:02:56,608 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 07:02:57,747\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_3949d6ce25fadc6a.zip.\n",
      "2025-05-09 07:02:57,748\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_hsmaqmwShKEQM53b' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_hsmaqmwShKEQM53b\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_hsmaqmwShKEQM53b\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_hsmaqmwShKEQM53b\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_hsmaqmwShKEQM53b' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: runtime_env setup failed: Failed to set up runtime environment.\n",
      "Could not create the actor because its associated runtime env failed to be created.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/agent/runtime_env_agent.py\", line 369, in _create_runtime_env_with_retry\n",
      "    runtime_env_context = await asyncio.wait_for(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/asyncio/tasks.py\", line 494, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/agent/runtime_env_agent.py\", line 329, in _setup_runtime_env\n",
      "    await create_for_plugin_if_needed(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/plugin.py\", line 254, in create_for_plugin_if_needed\n",
      "    size_bytes = await plugin.create(uri, runtime_env, context, logger=logger)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/pip.py\", line 518, in create\n",
      "    bytes = await task\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/pip.py\", line 498, in _create_for_hash\n",
      "    await PipProcessor(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/pip.py\", line 400, in _run\n",
      "    await self._install_pip_packages(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/pip.py\", line 376, in _install_pip_packages\n",
      "    await check_output_cmd(pip_install_cmd, logger=logger, cwd=cwd, env=pip_env)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runtime_env/utils.py\", line 101, in check_output_cmd\n",
      "    raise SubprocessCalledProcessError(\n",
      "ray._private.runtime_env.utils.SubprocessCalledProcessError: Run cmd[22] failed with the following details.\n",
      "Command '['/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/9379f8c361f3e632e7a4e31ba405a297d6a97d2c/virtualenv/bin/python', '-m', 'pip', 'install', '--disable-pip-version-check', '--no-cache-dir', '-r', '/tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/9379f8c361f3e632e7a4e31ba405a297d6a97d2c/ray_runtime_env_internal_pip_requirements.txt']' returned non-zero exit status 1.\n",
      "Last 50 lines of stdout:\n",
      "    WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a0f915da2e0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/torch/\n",
      "    WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a0f915da5b0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/torch/\n",
      "    Collecting torch<2.1,>=1.12.0\n",
      "      Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 5.0 MB/s eta 0:00:00\n",
      "    Collecting sentence-transformers==2.3.1\n",
      "      Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.8/132.8 kB 8.8 MB/s eta 0:00:00\n",
      "    Collecting transformers==4.28.1\n",
      "      Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 6.0 MB/s eta 0:00:00\n",
      "    Collecting huggingface_hub==0.15.1\n",
      "      Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 5.8 MB/s eta 0:00:00\n",
      "    Collecting accelerate==0.20.3\n",
      "      Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 6.1 MB/s eta 0:00:00\n",
      "    Collecting mlflow>=2.2.0\n",
      "      Downloading mlflow-2.17.2-py3-none-any.whl (26.7 MB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.7/26.7 MB 6.5 MB/s eta 0:00:00\n",
      "    Collecting datasets\n",
      "      Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 480.6/480.6 kB 6.3 MB/s eta 0:00:00\n",
      "    Requirement already satisfied: pandas in /home/ray/anaconda3/lib/python3.8/site-packages (from -r /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/9379f8c361f3e632e7a4e31ba405a297d6a97d2c/ray_runtime_env_internal_pip_requirements.txt (line 8)) (1.5.3)\n",
      "    Collecting sqlalchemy\n",
      "      Downloading sqlalchemy-2.0.40-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 5.7 MB/s eta 0:00:00\n",
      "    Collecting psycopg2-binary\n",
      "      Downloading psycopg2_binary-2.9.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 6.4 MB/s eta 0:00:00\n",
      "    Requirement already satisfied: tqdm in /home/ray/anaconda3/lib/python3.8/site-packages (from sentence-transformers==2.3.1->-r /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/9379f8c361f3e632e7a4e31ba405a297d6a97d2c/ray_runtime_env_internal_pip_requirements.txt (line 2)) (4.65.0)\n",
      "    Requirement already satisfied: numpy in /home/ray/anaconda3/lib/python3.8/site-packages (from sentence-transformers==2.3.1->-r /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/9379f8c361f3e632e7a4e31ba405a297d6a97d2c/ray_runtime_env_internal_pip_requirements.txt (line 2)) (1.23.5)\n",
      "    ERROR: Cannot install -r /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/pip/9379f8c361f3e632e7a4e31ba405a297d6a97d2c/ray_runtime_env_internal_pip_requirements.txt (line 2) and transformers==4.28.1 because these package versions have conflicting dependencies.\n",
      "\n",
      "    The conflict is caused by:\n",
      "        The user requested transformers==4.28.1\n",
      "        sentence-transformers 2.3.1 depends on transformers<5.0.0 and >=4.32.0\n",
      "\n",
      "    To fix this you could try to:\n",
      "    1. loosen the range of package versions you've specified\n",
      "    2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "    ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.3.1\",\"transformers==4.28.1\",\"huggingface_hub==0.15.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_11.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08b06b66-b686-45e6-aaf4-339774c65fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 07:10:41,024 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 07:10:42,333\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_ef101c5889ba60f9.zip.\n",
      "2025-05-09 07:10:42,334\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_ThHkDADGcJGpCLF4' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_ThHkDADGcJGpCLF4\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_ThHkDADGcJGpCLF4\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_ThHkDADGcJGpCLF4\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /tmp/ray/session_2025-05-08_21-12-32_399652_8/runtime_resources/working_dir_ │\n",
      "│ files/_ray_pkg_ef101c5889ba60f9/longformer_11.py:9 in <module>               │\n",
      "│                                                                              │\n",
      "│     6 from datetime import datetime                                          │\n",
      "│     7 from sqlalchemy import create_engine, text                             │\n",
      "│     8 from sentence_transformers import SentenceTransformer, InputExample, l │\n",
      "│ ❱   9 from sentence_transformers.trainers import SentenceTransformerTrainer  │\n",
      "│    10 from sentence_transformers.evaluation import TripletEvaluator          │\n",
      "│    11 from sentence_transformers.training_args import SentenceTransformerTra │\n",
      "│    12 from ray.train.torch import TorchTrainer                               │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "ModuleNotFoundError: No module named 'sentence_transformers.trainers'\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_ThHkDADGcJGpCLF4' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "│                                                                              │\n",
      "│     6 from datetime import datetime                                          │\n",
      "│     7 from sqlalchemy import create_engine, text                             │\n",
      "│     8 from sentence_transformers import SentenceTransformer, InputExample, l │\n",
      "│ ❱   9 from sentence_transformers.trainers import SentenceTransformerTrainer  │\n",
      "│    10 from sentence_transformers.evaluation import TripletEvaluator          │\n",
      "│    11 from sentence_transformers.training_args import SentenceTransformerTra │\n",
      "│    12 from ray.train.torch import TorchTrainer                               │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "ModuleNotFoundError: No module named 'sentence_transformers.trainers'\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.3.1\",\"transformers>=4.32.0,<5.0.0\",\"huggingface_hub==0.15.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ]}' \\\n",
    "-- python longformer_11.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b6d1d-f757-4ae8-99a4-c021abba9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\",\"sentence-transformers==2.3.1\",\"transformers>=4.32.0,<5.0.0\",\"huggingface_hub>=0.15.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ],\"env_vars\": {\"FORCE_REBUILD\": \"yes_2025_05_09_3\"}}' \\\n",
    "-- python longformer_11.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5fc877-14f1-4133-b7e5-3e40bae9c3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 07:37:39,603 - INFO - NumExpr defaulting to 4 threads.\n",
      "\u001b[37mJob submission server address\u001b[39m: \u001b[1mhttp://ray-head:8265\u001b[22m\n",
      "2025-05-09 07:37:40,797\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_e181af62feb475e2.zip.\n",
      "2025-05-09 07:37:40,798\tINFO packaging.py:576 -- Creating a file package for local module '.'.\n",
      "\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\u001b[32mJob 'raysubmit_KXmyGBtJaEgjm5NY' submitted successfully\u001b[39m\n",
      "\u001b[32m-------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  Query the logs of the job:\n",
      "    \u001b[1mray job logs raysubmit_KXmyGBtJaEgjm5NY\u001b[22m\n",
      "  Query the status of the job:\n",
      "    \u001b[1mray job status raysubmit_KXmyGBtJaEgjm5NY\u001b[22m\n",
      "  Request the job to be stopped:\n",
      "    \u001b[1mray job stop raysubmit_KXmyGBtJaEgjm5NY\u001b[22m\n",
      "\n",
      "Tailing logs until the job exits (disable with --no-wait):\n",
      "Traceback (most recent call last):\n",
      "  File \"longformer_11.py\", line 9, in <module>\n",
      "    from sentence_transformers.trainers import SentenceTransformerTrainer\n",
      "ModuleNotFoundError: No module named 'sentence_transformers.trainers'\n",
      "\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\u001b[31mJob 'raysubmit_KXmyGBtJaEgjm5NY' failed\u001b[39m\n",
      "\u001b[31m---------------------------------------\u001b[39m\n",
      "\n",
      "Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):\n",
      "Traceback (most recent call last):\n",
      "  File \"longformer_11.py\", line 9, in <module>\n",
      "    from sentence_transformers.trainers import SentenceTransformerTrainer\n",
      "ModuleNotFoundError: No module named 'sentence_transformers.trainers'\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\", \"sentence-transformers==2.3.1\", \"transformers>=4.32.0,<5.0.0\", \"huggingface_hub>=0.15.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ], \"env_vars\": {\"FORCE_REBUILD\": \"yes_2025_05_09_3\"}}' \\\n",
    "-- python longformer_11.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0edb6-56ea-4e3c-8019-3a7a30723915",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray job submit \\\n",
    "--address http://ray-head:8265 \\\n",
    "--working-dir . \\\n",
    "--runtime-env-json '{\"pip\": [ \"torch>=1.12.0,<2.1\", \"sentence-transformers==2.3.1\", \"transformers>=4.32.0,<5.0.0\", \"huggingface_hub>=0.15.1\", \"accelerate==0.20.3\", \"mlflow>=2.2.0\", \"datasets\", \"pandas\", \"sqlalchemy\", \"psycopg2-binary\" ], \"env_vars\": {\"FORCE_REBUILD\": \"yes_2025_05_09_3\"}}' \\\n",
    "-- python longformer_11.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
