{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db674340",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2-binary requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e6a1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Imports & cleanup unused\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import requests\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c329315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Configuration\n",
    "BATCH_SIZE      = 16\n",
    "NUM_WORKERS     = 4\n",
    "DATABASE_URI    = 'postgresql+psycopg2://rg5073:rg5073pass@129.114.27.112:5432/cleaned_meta_data_db'\n",
    "REMOTE_EMBED_URL = 'http://localhost:8000/batch-embed'   # your hosted FastAPI batch-embed URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db815f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Ensure the vector column exists\n",
    "engine = create_engine(DATABASE_URI, pool_size=8, max_overflow=0)\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "      ALTER TABLE arxiv_chunks_eval_5\n",
    "      ADD COLUMN IF NOT EXISTS chunk_embedding_768 vector(768)\n",
    "    \"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9ae38b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to embed: 52554\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 5: Count total rows\n",
    "with engine.connect() as conn:\n",
    "    total = conn.execute(text(\"SELECT COUNT(*) FROM arxiv_chunks_eval_5\")).scalar_one()\n",
    "print(f\"Total rows to embed: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dc1e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Remote embedding via batch-embed endpoint\n",
    "def embed_texts(texts: list[str]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Send a list of texts to the remote batch-embed endpoint,\n",
    "    receive back list-of-list embeddings.\n",
    "    \"\"\"\n",
    "    resp = requests.post(REMOTE_EMBED_URL, json={\"texts\": texts})\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"embeddings\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d09a5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Fetch, embed, and update one batch\n",
    "def process_batch(offset: int) -> int:\n",
    "    # 1) fetch batch\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(\n",
    "            text(\"\"\"\n",
    "              SELECT paper_id, chunk_id, chunk_data\n",
    "                FROM arxiv_chunks_eval_5\n",
    "               ORDER BY paper_id, chunk_id\n",
    "               LIMIT :limit OFFSET :offset\n",
    "            \"\"\"),\n",
    "            {\"limit\": BATCH_SIZE, \"offset\": offset}\n",
    "        ).fetchall()\n",
    "    if not rows:\n",
    "        return 0\n",
    "\n",
    "    # 2) compute embeddings remotely\n",
    "    ids   = [(r.paper_id, r.chunk_id) for r in rows]\n",
    "    texts = [r.chunk_data for r in rows]\n",
    "    embs  = embed_texts(texts)  # calls your FastAPI\n",
    "\n",
    "    # 3) bulk update back into Postgres\n",
    "    params = [\n",
    "        {\"pid\": pid, \"cid\": cid, \"vec\": vec}\n",
    "        for (pid, cid), vec in zip(ids, embs)\n",
    "    ]\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(\n",
    "            text(\"\"\"\n",
    "              UPDATE arxiv_chunks_eval_5\n",
    "                 SET chunk_embedding_768 = :vec\n",
    "               WHERE paper_id = :pid\n",
    "                 AND chunk_id   = :cid\n",
    "            \"\"\"),\n",
    "            params\n",
    "        )\n",
    "\n",
    "    return len(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d9acc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285 batches, offsets: [0, 16, 32, 48, 64]…\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 8: Compute all offsets\n",
    "n_batches = math.ceil(total / BATCH_SIZE)\n",
    "offsets   = [i * BATCH_SIZE for i in range(n_batches)]\n",
    "print(f\"{n_batches} batches, offsets: {offsets[:5]}…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc4e5197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:48:50 INFO Prepared 3 sample texts for embedding test\n",
      "19:48:50 INFO Sending request to remote /batch-embed endpoint\n",
      "19:48:50 INFO Received embeddings in 0.246 seconds\n",
      "19:48:50 INFO Each embedding has dimension: 768\n",
      "19:48:50 INFO  Sample 0: first 5 dims = [-0.1988082490473365, 0.003090559815367063, -0.30425620086801547, -0.05810393524977068, 0.40287098614498973], L2 norm = 11.5390\n",
      "19:48:50 INFO  Sample 1: first 5 dims = [-0.4815902259142604, -0.19317454716656357, -0.12821805587009294, -0.17972209495928837, -0.15113638225011528], L2 norm = 11.8691\n",
      "19:48:50 INFO  Sample 2: first 5 dims = [-0.15580386124715648, -0.4081037603866528, 0.6036769642549402, -0.05800980198032716, 0.04591636808917803], L2 norm = 10.8987\n"
     ]
    }
   ],
   "source": [
    "# %% Cell: Sample Test with Detailed Logging\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"OpenAI's GPT models are powerful for NLP tasks.\",\n",
    "    \"FastAPI + ONNX Runtime is great for serving ML models!\"\n",
    "]\n",
    "\n",
    "logging.info(f\"Prepared {len(sample_texts)} sample texts for embedding test\")\n",
    "\n",
    "# Measure round-trip time for the remote call\n",
    "t0 = time.time()\n",
    "logging.info(\"Sending request to remote /batch-embed endpoint\")\n",
    "embs = embed_texts(sample_texts)\n",
    "t1 = time.time()\n",
    "\n",
    "elapsed = t1 - t0\n",
    "logging.info(f\"Received embeddings in {elapsed:.3f} seconds\")\n",
    "\n",
    "# Validate and inspect embeddings\n",
    "assert isinstance(embs, list) and len(embs) == len(sample_texts), \"Unexpected response format\"\n",
    "dim = len(embs[0])\n",
    "logging.info(f\"Each embedding has dimension: {dim}\")\n",
    "\n",
    "for i, vec in enumerate(embs):\n",
    "    norm = np.linalg.norm(vec)\n",
    "    logging.info(f\" Sample {i}: first 5 dims = {vec[:5]}, L2 norm = {norm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a62d14fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "… done 52554/52554 rows\n",
      "✅ Finished embedding & updating 52554 rows\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 9: Run batches in parallel and report progress\n",
    "processed = 0\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = {executor.submit(process_batch, off): off for off in offsets}\n",
    "    for fut in as_completed(futures):\n",
    "        done = fut.result()\n",
    "        processed += done\n",
    "        print(f\"… done {processed}/{total} rows\", end=\"\\r\")\n",
    "\n",
    "print(f\"\\n✅ Finished embedding & updating {processed} rows\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
