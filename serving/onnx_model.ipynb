{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea8b06f",
   "metadata": {},
   "source": [
    "# Export HuggingFace Models to ONNX\n",
    "\n",
    "This notebook shows how to export:\n",
    "- A distilbert embedding model (encoder-only)  \n",
    "- A BART summarization model (seq2seq-LM)\n",
    "\n",
    "…to ONNX format, ready for GPU inference (e.g. with TensorRT).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313b1489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: onnxruntime in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (1.21.1)\n",
      "Requirement already satisfied: filelock in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (80.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: coloredlogs in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from onnxruntime) (4.25.7)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pb/.pyenv/versions/jupyter-cuda/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch onnxruntime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba0a37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from itertools import chain\n",
    "from transformers import (\n",
    "    DistilBertModel, DistilBertTokenizer,\n",
    "    AutoModelForSeq2SeqLM, AutoTokenizer\n",
    ")\n",
    "from transformers.onnx import FeaturesManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f808dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from torch.jit._trace import TracerWarning\n",
    "import warnings\n",
    "\n",
    "# --- suppress tracer warnings if you like ---\n",
    "warnings.filterwarnings(\"ignore\", category=TracerWarning)\n",
    "\n",
    "# --- basic logging setup ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be50f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_embed_model(\n",
    "    model_dir: str,\n",
    "    onnx_output_path: str,\n",
    "    opset: int = 17\n",
    "):\n",
    "    \"\"\"\n",
    "    Export a DistilBERT bi-encoder from `model_dir` to ONNX format at `onnx_output_path`.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Path or HF identifier of the pretrained DistilBERT model.\n",
    "        onnx_output_path (str): Path where the ONNX file will be written.\n",
    "        opset (int): ONNX opset version to target (default: 17).\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    logger.info(f\"⏳ Loading DistilBERT model from `{model_dir}`\")\n",
    "    # load the DistilBERT encoder\n",
    "    model = DistilBertModel.from_pretrained(model_dir)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
    "    model.eval()\n",
    "    logger.info(f\"✅ Loaded model & tokenizer in {time.time() - t0:.1f}s\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    logger.info(\"⏳ Building ONNX config for encoder-only export\")\n",
    "    # ensure the model is supported and get the ONNXConfig class\n",
    "    model_kind, onnx_config_cls = FeaturesManager.check_supported_model_or_raise(\n",
    "        model, feature=\"default\"\n",
    "    )\n",
    "    onnx_config = onnx_config_cls(model.config)\n",
    "    # create dummy inputs (PyTorch tensors) for export\n",
    "    dummy_inputs = onnx_config.generate_dummy_inputs(tokenizer, framework=\"pt\")\n",
    "    logger.info(f\"✅ Prepared ONNX config & dummy inputs in {time.time() - t1:.1f}s\")\n",
    "\n",
    "    # extract the names & dynamic axes\n",
    "    input_names = list(onnx_config.inputs.keys())     # e.g. ['input_ids','attention_mask']\n",
    "    output_names = list(onnx_config.outputs.keys())   # e.g. ['last_hidden_state']\n",
    "    dynamic_axes = {**onnx_config.inputs, **onnx_config.outputs}\n",
    "\n",
    "    # build the tuple of example inputs in the correct order\n",
    "    example_inputs = tuple(dummy_inputs[name] for name in input_names)\n",
    "\n",
    "    t2 = time.time()\n",
    "    logger.info(f\"⏳ Exporting to ONNX (opset {opset}) → {onnx_output_path}\")\n",
    "    # perform the export\n",
    "    torch.onnx.export(\n",
    "        model,                        # model to export\n",
    "        example_inputs,               # tuple of torch.Tensor matching input_names\n",
    "        onnx_output_path,             # where to save the .onnx\n",
    "        input_names=input_names,      # names of the model inputs\n",
    "        output_names=output_names,    # names of the model outputs\n",
    "        dynamic_axes=dynamic_axes,    # which axes are dynamic (batch, seq)\n",
    "        opset_version=opset,          # ONNX opset version\n",
    "        do_constant_folding=True      # fold constants for optimization\n",
    "    )\n",
    "    logger.info(f\"✅ Exported ONNX in {time.time() - t2:.1f}s\")\n",
    "    logger.info(f\"🏁 Total embed export time: {time.time() - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43894922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_summarization_model(\n",
    "    hf_name_or_dir: str,\n",
    "    onnx_output_path: str,\n",
    "    opset: int = 17\n",
    "):\n",
    "    t0 = time.time()\n",
    "    logger.info(f\"⏳ Loading seq2seq model from `{hf_name_or_dir}`\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(hf_name_or_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_name_or_dir)\n",
    "    model.eval()\n",
    "    logger.info(f\"✅ Loaded model & tokenizer in {time.time() - t0:.1f}s\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    logger.info(\"⏳ Building ONNX config for seq2seq-LM export\")\n",
    "    _, onnx_config_class = FeaturesManager.check_supported_model_or_raise(\n",
    "        model, feature=\"seq2seq-lm\"\n",
    "    )\n",
    "    onnx_config = onnx_config_class(model.config)\n",
    "    dummy_inputs = onnx_config.generate_dummy_inputs(tokenizer, framework=\"pt\")\n",
    "    logger.info(f\"✅ Prepared ONNX config & dummy inputs in {time.time() - t1:.1f}s\")\n",
    "\n",
    "    # Prepare names & axes\n",
    "    input_names = list(onnx_config.inputs.keys())\n",
    "    output_names = list(onnx_config.outputs.keys())\n",
    "    dynamic_axes = {**onnx_config.inputs, **onnx_config.outputs}\n",
    "    example_inputs = tuple(dummy_inputs[n] for n in input_names)\n",
    "\n",
    "    t2 = time.time()\n",
    "    logger.info(f\"⏳ Exporting seq2seq model to ONNX (opset {opset}) → {onnx_output_path}\")\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        example_inputs,\n",
    "        onnx_output_path,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=opset,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    logger.info(f\"✅ Exported ONNX in {time.time() - t2:.1f}s\")\n",
    "    logger.info(f\"🏁 Total summarization export time: {time.time() - t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10abfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:32:55 INFO ⏳ Loading DistilBERT model from `/home/pb/projects/course/sem2/mlops/project/mlops/models/artifacts/model/model.sentence_transformer`\n",
      "19:32:55 INFO ✅ Loaded model & tokenizer in 0.1s\n",
      "19:32:55 INFO ⏳ Building ONNX config for encoder-only export\n",
      "19:32:55 INFO ✅ Prepared ONNX config & dummy inputs in 0.0s\n",
      "19:32:55 INFO ⏳ Exporting to ONNX (opset 17) → models/bert.onnx\n",
      "/tmp/ipykernel_33902/177478273.py:44: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.8, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "19:32:55 INFO ✅ Exported ONNX in 0.4s\n",
      "19:32:55 INFO 🏁 Total embed export time: 0.5s\n"
     ]
    }
   ],
   "source": [
    "export_embed_model(\n",
    "    model_dir=\"/home/pb/projects/course/sem2/mlops/project/mlops/models/artifacts/model/model.sentence_transformer\",\n",
    "    onnx_output_path=\"models/bert.onnx\",\n",
    ")\n",
    "export_summarization_model(\n",
    "    hf_name_or_dir=\"facebook/bart-large\",\n",
    "    onnx_output_path=\"models/bart_summarize.onnx\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c94f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Register ONNX models in MLflow\n",
    "\n",
    "import os\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://129.114.27.112:8000\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"admin\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"password\"\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import mlflow.onnx\n",
    "import onnx\n",
    "\n",
    "# point MLflow at your tracking server (or rely on env vars you’ve already set)\n",
    "mlflow.set_experiment(\"onnx-model-registration\")\n",
    "\n",
    "def make_input_example(model_cls, model_dir_or_name, feature):\n",
    "    # load HF model & tokenizer to construct dummy inputs\n",
    "    if feature == \"default\":\n",
    "        model = model_cls.from_pretrained(model_dir_or_name)\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_dir_or_name)\n",
    "    else:  # \"seq2seq-lm\"\n",
    "        model = model_cls.from_pretrained(model_dir_or_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir_or_name)\n",
    "    model.eval()\n",
    "    _, config_cls = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "    onnx_config = config_cls(model.config)\n",
    "    dummy = onnx_config.generate_dummy_inputs(tokenizer, framework=\"pt\")\n",
    "    # convert to numpy for MLflow\n",
    "    return {k: v.cpu().numpy() for k, v in dummy.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09662bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'distilbert-embedding-onnx'.\n",
      "2025/05/11 19:37:53 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: distilbert-embedding-onnx, version 1\n",
      "Created version '1' of model 'distilbert-embedding-onnx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Registered distilbert ONNX with input_example\n",
      "🏃 View run distilbert-onnx-registration at: http://129.114.27.112:8000/#/experiments/8/runs/57398c53d71b435fbcd69113af0f141e\n",
      "🧪 View experiment at: http://129.114.27.112:8000/#/experiments/8\n"
     ]
    }
   ],
   "source": [
    "# 1) distilbert embedding ONNX\n",
    "distilbert_input = make_input_example(\n",
    "    DistilBertModel,\n",
    "    \"/home/pb/projects/course/sem2/mlops/project/mlops/models/artifacts/model/model.sentence_transformer\",\n",
    "    \"default\"\n",
    ")\n",
    "lf_onnx = onnx.load(\"models/distilbert.onnx\")\n",
    "with mlflow.start_run(run_name=\"distilbert-onnx-registration\"):\n",
    "    mlflow.onnx.log_model(\n",
    "        onnx_model=lf_onnx,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"distilbert-embedding-onnx\",\n",
    "        input_example=distilbert_input\n",
    "    )\n",
    "    print(\"✅ Registered distilbert ONNX with input_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) BART summarization ONNX\n",
    "bart_input = make_input_example(\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    \"facebook/bart-large\",\n",
    "    \"seq2seq-lm\"\n",
    ")\n",
    "bart_onnx = onnx.load(\"models/bart_summarize.onnx\")\n",
    "with mlflow.start_run(run_name=\"bart-summarize-onnx-registration\"):\n",
    "    mlflow.onnx.log_model(\n",
    "        onnx_model=bart_onnx,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"bart-summarize-onnx\",\n",
    "        input_example=bart_input\n",
    "    )\n",
    "    print(\"✅ Registered BART summarization ONNX with input_example\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
